[
["index.html", "Introdução à Ciência de Dados Análise de dados e algoritmos de previsão com R Prefácio", " Introdução à Ciência de Dados Análise de dados e algoritmos de previsão com R Rafael A. Irizarry 2020-06-16 Prefácio Este livro começou como as notas usadas para ensinar a classe de HarvardX [Data Science Series] (https://www.edx.org/professional-certificate/harvardx-data-science)1. O código Rmarkdown usado para gerar o livro está disponível em [GitHub] (https://github.com/rafalab/dsbook)2. Observe que o tema gráfico usado para gráficos em todo o livro pode ser recriado usando a função ds_theme_set() do pacote dslabs. Um PDF da versão em inglês deste livro está disponível em [Leanpub] (https://leanpub.com/datasciencebook)3. Uma cópia impressa da versão em inglês deste livro está disponível em [CRC Press] (https://www.crcpress.com/Introduction-to-Data-Science-Data-Analysis-and-Prediction-Algorithms-with/Irizarry/ p/ book/ 9780367357986)4. Este trabalho foi publicado sob a licença Creative Commons Atribuição-Uso Não-Comercial-Compartilhamento pela mesma Licença 4.0 International [CC BY-NC-SA 4.0] (https://creativecommons.org/licenses/by-nc-sa/4.0). Fazemos anúncios relacionados ao livro no Twitter. Para obter as informações mais recentes, siga [@rafalab] (https://twitter.com/rafalab). https://www.edx.org/professional-certificate/harvardx-data-science↩ https://github.com/rafalab/dsbook↩ https://leanpub.com/datasciencebook↩ https://www.crcpress.com/Introduction-to-Data-Science-Data-Analysis-and-Prediction-Algorithms-with/Irizarry/p/book/9780367357986↩ "],
["obrigado.html", "Obrigado Os estudos de caso O que este livro cobre? O que este livro não cobre?", " Obrigado Este livro é dedicado a todos os envolvidos na construção e manutenção de pacotes R e R que usamos no texto. Um agradecimento especial aos desenvolvedores e mantenedores da base R, ao pacote _tidyverse_ecaret. Um agradecimento especial ao meu guru tidyverse David Robinson e Amy Gill por dezenas de comentários, alterações e sugestões. Além disso, muito obrigado a Stephanie Hicks, que serviu duas vezes como co-instrutora em minhas aulas de ciência de dados, e a Yihui Xie, que pacientemente tolerou minhas múltiplas perguntas sobre livros. Agradeço também a Karl Broman, de quem pedi idéias emprestadas para as seções de visualização de dados e ferramentas de produtividade, e a Hector Corrada-Bravo, por seus conselhos sobre como melhor ensinar o aprendizado de máquinas. Agradeço a Peter Aldhous, de quem pedi idéias emprestadas para a seção sobre princípios de visualização de dados, e Jenny Bryan por escrever Happy Git_e_GitHub pelo usoR, que influenciou nossos capítulos do Git. Agradecemos a Alyssa Frazee por ajudar a criar o problema de lição de casa que se tornou o capítulo sobre sistemas de referência e a Amanda Cox por fornecer os dados do exame New York Regents. Além disso, muito obrigado a Jeff Leek, Roger Peng e Brian Caffo, cuja turma inspirou a forma como este livro é dividido, e Garrett Grolemund e Hadley Wickham por abrirem o código de reserva para seu livro R for Data Science. Finalmente, obrigado a Alex Nones por corrigir o manuscrito durante suas várias etapas. Este livro foi concebido durante o ensino de vários cursos de estatística aplicada, iniciados há mais de quinze anos. Professores assistentes que trabalharam comigo ao longo dos anos fizeram importantes contribuições indiretas para este livro. A versão mais recente deste curso é uma série HarvardX coordenada por Heather Sternshein e Zofia Gajdos. Agradecemos suas contribuições. Também agradecemos a todos os alunos cujas perguntas e comentários nos ajudaram a melhorar o livro. Os cursos foram parcialmente financiados pelo subsídio NIH R25GM114818. Agradecemos aos Institutos Nacionais de Saúde por seu apoio. Um agradecimento especial a todos aqueles que editaram o livro através de pull request_do GitHub ou fizeram sugestões criando um issue ou enviando um email: nickyfoto (Huang Qiang) desautm (Marc-André Désautels), michaschwab (Michail Schwab) alvarolarreategui (Alvaro Larreategui), jakevc (Jake VanCampen), omerta (Guillermo Lengemann), espinielli (Enrico Spinielli), asimumba(Aaron Simumba) braunschweig (Maldewar), gwierzchowski (Grzegorz Wierzchowski), technocrat (Richard Careaga) atzakas, defeit (David Emerson Feit), shiraamitchell (Shira Mitchell) Nathalie-S, andreashandel (Andreas Handel) berkowitze (Elias Berkowitz) Dean-Webb (Dean Webber), mohayusuf, jimrothstein, mPloenzke (Matthew Ploenzke), NicholasDowand (Nicholas Dow) kant (Darío Hereñú), debbieyuster (Debbie Yuster), tuanchauict (Tuan Chau), phzeller, David D. Kane, El Mustapha El Abbassi e Vadim Zipunnikov. O livro foi traduzido para o português por . Agradecemos a todos que contribuíram para esta tradução. Ilia Ushkin e Dustin Tingley geraram um primeiro rascunho usando um programa de tradução automática. #Introdução {-} A demanda por profissionais qualificados em ciência de dados na indústria, academia e governo está crescendo rapidamente. Este livro apresenta conceitos e habilidades que podem ajudá-lo a enfrentar os desafios da análise de dados em situações reais. O texto aborda os conceitos de probabilidade, inferência estatística, regressão linear e aprendizado de máquina. Isso também os ajudará a desenvolver habilidades como programação R, wrangling data, dplyr, visualização de dados com ggplot2, criação de algoritmo com caret, organização de arquivos com shell UNIX/ Linux, controle de versão com Git e GitHub e preparando documentos reproduzíveis com as marcações knitr e R. O livro está dividido em seis partes: R, Visualização de dados, _Wrangling_of data, Estatísticas com R, ___Machine Learning___eProductivity tools. Cada parte possui vários capítulos que devem ser apresentados em uma única aula e inclui dezenas de exercícios distribuídos pelos capítulos. Os estudos de caso Ao longo do livro, usamos estudos de caso motivadores. Em cada estudo de caso, tentamos imitar realisticamente a experiência dos cientistas de dados. Para cada um dos conceitos que discutimos, começamos fazendo perguntas específicas que depois respondemos através da análise de dados. Aprendemos conceitos como um meio de responder perguntas. Exemplos dos estudos de caso que incluímos neste livro são: Estudo de caso Conceito Taxas de homicídio nos Estados Unidos por estado Conceitos básicos de R Alturas dos estudantes Resumos estatísticos Tendências em saúde e economia global Visualização de dados O impacto das vacinas nas taxas de doenças infecciosas Visualização de dados A crise financeira de 2007-2008 Probabilidade Previsão de eleições Inferência estatística Alturas de estudantes autorreferidas Arranjo de dados Money Ball: Construindo um time de beisebol Regressão linear MNIST: Processamento de imagem com dígitos manuscritos Máquina de Aprendizagem Sistemas de recomendação de filmes Máquina de Aprendizagem Quem acha este livro útil? {-} O objetivo deste livro é servir como texto para um primeiro curso de ciência de dados. Nenhum conhecimento prévio de R é necessário, embora alguma experiência em programação possa ser útil. Os conceitos estatísticos usados para responder às perguntas do estudo de caso são apresentados apenas brevemente e, portanto, recomendamos um livro de probabilidade e estatística para quem deseja entender completamente esses conceitos. Ao ler e entender todos os capítulos e concluir todos os exercícios, os alunos estarão bem posicionados para concluir tarefas básicas de análise de dados e aprender os conceitos e habilidades mais avançados necessários para se tornarem especialistas. O que este livro cobre? Começamos analisando o R básico e o tidyverse. Eles aprenderão R ao longo do livro, mas na primeira parte nos dedicamos a revisar os componentes básicos necessários para continuar aprendendo. A crescente disponibilidade de conjuntos de dados informativos e ferramentas de software levou a uma maior dependência da visualização de dados em muitos campos. Na segunda parte, demonstramos como usar o ggplot2 para gerar gráficos e descrever princípios importantes da visualização de dados. Na terceira parte, demonstramos a importância da estatística na análise dos dados, respondendo a perguntas de estudos de caso usando probabilidade, inferência e regressão com R. A quarta parte usa vários exemplos para familiarizar os leitores com os wrangling data. As habilidades específicas que estudamos incluem raspagem da Web, usando expressões regulares e mesclando e remodelando tabelas de dados. Fazemos isso usando as ferramentas tidyverse. Na quinta parte, apresentamos vários desafios que nos levam a introduzir o machine learning. Aprendemos a usar o pacote __caret_para criar algoritmos de previsão que incluem_K-vizinhos mais próximos_e florestas aleatórias_. Na parte final, oferecemos uma breve introdução às ferramentas de produtividade que usamos diariamente em projetos de ciência de dados. Estes são RStudio, shell UNIX/ Linux, Git e GitHub e knitr e R Markdown. O que este livro não cobre? Este livro enfoca os aspectos da análise de dados da ciência de dados. Portanto, não discutimos aspectos relacionados ao gerenciamento de dados (data management em inglês) ou engenharia. Embora a programação em R seja uma parte essencial do livro, não ensinamos tópicos de computação mais avançados, como estruturas de dados, otimização e teoria de algoritmos. Da mesma forma, não discutimos tópicos como serviços da web, gráficos interativos, computação paralela e processamento de streaming de dados. Os conceitos estatísticos são apresentados principalmente como ferramentas para a solução de problemas e nenhuma descrição teórica detalhada é incluída neste livro. "],
["getting-started.html", "Capítulo 1 Começando com R e RStudio 1.1 O console do R 1.2 scripts 1.3 RStudio 1.4 Instalando pacotes R", " Capítulo 1 Começando com R e RStudio Por que R? R não é uma linguagem de programação como C ou Java. Não foi criado por software_engenheiros para_software development, mas por estatísticos como um ambiente interativo para análise de dados. Você pode ler a história completa no artigo “Uma breve história de S”5. A interatividade é um recurso indispensável na ciência de dados porque, como você aprenderá em breve, a capacidade de explorar rapidamente os dados é necessária para o sucesso neste campo. No entanto, assim como em outras linguagens de programação, no R eles podem salvar seu trabalho como scripts, ou scripts, que podem ser facilmente executados a qualquer momento. Esses scripts servem como um registro da análise que eles realizaram, um recurso importante que facilita o trabalho reproduzível. Programadores especialistas não devem esperar que o R siga as convenções às quais estão acostumados, pois ficarão desapontados. Se você é paciente, apreciará a grande vantagem do R quando se trata de análise de dados e especificamente visualização de dados. Outras características atraentes do R são: R é gratuito e de código aberto6. Ele roda em todas as principais plataformas: Windows, Mac Os, UNIX/ Linux. Scripts e objetos de dados podem ser compartilhados diretamente entre plataformas. Existe uma comunidade grande, crescente e ativa de usuários de R e, como resultado, existem inúmeros recursos para aprender e fazer perguntas[https://stats.stackexchange.com/questions/138/free-resources-for-learning -r][https://www.r-project.org/help.html]7. É fácil para outros contribuírem com plug-ins (add-ons_ em inglês) que permitem aos desenvolvedores compartilhar implementações de software de novas metodologias de ciência de dados. Isso dá aos usuários do R acesso antecipado aos métodos e ferramentas mais recentes desenvolvidos para uma ampla variedade de disciplinas, incluindo ecologia, biologia molecular, ciências sociais e geografia, entre outros campos. 1.1 O console do R A análise de dados interativa geralmente ocorre no console do R que executa comandos à medida que são digitados. Existem várias maneiras de obter acesso a um console R. Um é simplesmente iniciar o R no seu computador. O console fica assim: Como um exemplo rápido, tente usar o console para calcular uma gorjeta de 15% em uma refeição que custa US $ 19,71: 0.15 * 19.71 #&gt; [1] 2.96 ** Nota: neste livro, as caixas cinza são usadas para mostrar o código R escrito no console R. O símbolo #&gt; é usado para denotar a output do console R. ** 1.2 scripts Uma das grandes vantagens de R sobre a análise de apontar e clicar software_é que eles podem salvar seu trabalho como_scripts, que podem ser editados e salvos com um editor de texto. O material deste livro foi desenvolvido usando o Integrated Development Environment (IDE) do RStudio8. O RStudio inclui um editor com muitos recursos específicos de R, um console para executar seu código e outros painéis úteis, incluindo um para exibir figuras. A maioria dos consoles R disponíveis na Web também inclui um painel para editar scripts, mas nem todos permitem salvar scripts para uso posterior. Todos os R scripts usados para gerar este livro podem ser encontrados no GitHub9. 1.3 RStudio O RStudio será nossa plataforma de lançamento para projetos de ciência de dados. Ele não apenas nos fornece um editor para criar e editar nossos scripts, mas também oferece muitas outras ferramentas úteis. Nesta seção, abordaremos alguns dos princípios básicos. 1.3.1 Painéis Quando iniciarem o RStudio pela primeira vez, eles verão três painéis. O painel esquerdo mostra o console R. À direita, o painel superior inclui guias como Environment_e_History, enquanto o painel inferior mostra cinco guias: Arquivo, Plots, Packages, Help_e_Viewer (essas guias podem ser diferentes nas novas versões do RStudio). Eles podem clicar em cada guia para percorrer as diferentes opções. Para iniciar um novo script, clique em File, depois em New File e, em seguida, em R Script. Isso inicia um novo painel à esquerda e é aqui que você pode começar a digitar seu script. 1.3.2 Combinações de teclas Muitas das tarefas que realizamos com o mouse podem ser realizadas com uma combinação de teclas (key bindings em inglês). Por exemplo, acabamos de mostrar como usar o mouse para iniciar um novo script, mas você também pode usar a seguinte combinação de teclas: Ctrl + Shift + N no Windows e Command + Shift + N no Mac. Embora muitas vezes demonstremos como usar o mouse neste tutorial, recomendamos vivamente que você memorize combinações de teclas para as operações que usa com mais frequência. O RStudio inclui uma folha de referência útil (cheat sheet em inglês) com os comandos mais usados. Você pode obtê-lo diretamente do RStudio assim: Recomendamos que seja útil para que você possa procurar combinações de teclas ao apontar e clicar repetidamente. 1.3.3 Como executar comandos enquanto edita scripts Existem muitos editores projetados especificamente para codificação. Isso é útil porque a cor e o recuo são adicionados automaticamente para tornar o código mais legível. O RStudio é um desses editores e foi desenvolvido especificamente para R. Uma das principais vantagens do RStudio sobre outros editores é que podemos testar facilmente nosso código enquanto editamos nossos scripts. Aqui está um exemplo. Vamos começar abrindo um novo script como fizemos antes. Então, vamos nomear o script. Podemos fazer isso através do editor salvando o novo script atual sem nome. Para começar, clique no ícone Salvar ou use a combinação de teclas Ctrl + S no Windows e Command + S no Mac. Quando você tenta salvar o documento pela primeira vez, o RStudio solicita um nome. Uma boa convenção é usar um nome descritivo, com letras minúsculas, sem espaços, apenas hífens para separar as palavras e, em seguida, seguido pelo sufixo .R. Vamos chamar esse script: my-first-script.R. Agora estamos prontos para começar a editar nosso primeiro script. As primeiras linhas de código em um R script são dedicadas ao carregamento dos pacotes que iremos usar. Outro recurso útil do RStudio é que, quando escrevemos library(), O RStudio começa a concluir automaticamente o que estamos escrevendo com os pacotes que instalamos. Veja o que acontece quando escrevemos library(ti): Outra característica que você deve ter notado é que, quando escrevem library( o segundo parêntese é adicionado automaticamente. Isso ajudará a evitar um dos erros de codificação mais comuns: esquecer de fechar um parêntese. Agora podemos continuar escrevendo código. Como exemplo, criaremos um gráfico mostrando os totais de assassinatos versus totais da população por estado dos EUA.Uma vez que eles terminem de escrever o código necessário para fazer esse gráfico, eles podem testá-lo executando o código. Para fazer isso, clique no botão Run no canto superior direito do painel de edição. Eles também podem usar a combinação de teclas: Ctrl + Shift + Enter no Windows ou Command + Shift + Return no Mac. Assim que executar o código, eles verão que ele aparece no console do R e, nesse caso, o gráfico resultante aparece no console de gráficos. Lembre-se de que o console gráfico possui uma interface útil que permite clicar para frente ou para trás em diferentes gráficos, ampliar o gráfico ou salvar os gráficos como arquivos. Para executar uma linha de cada vez, em vez de todo o script, você pode usar Control-Enter no Windows e Command-Return no Mac. 1.3.4 Como alterar opções globais Eles podem mudar bastante a aparência e a funcionalidade do RStudio. Para alterar as opções globais, clique em Ferramentas e, em seguida, em Global Options …. Como exemplo, mostramos como fazer uma alteração que recomendamos altamente: altere o espaço de trabalho Save para .RData na saída_para_Never_e desmarque_Restore .RData no espaço de trabalho no início. Por padrão, quando alguém sai do R, o programa salva todos os objetos que ele criou em um arquivo chamado .RData. Isso ocorre para que, ao reiniciar a sessão no mesmo arquivo, o programa carregue esses objetos. No entanto, descobrimos que isso causa confusão, principalmente quando compartilhamos código com colegas e assumimos que eles têm esse arquivo .RData. Para alterar essas opções, faça com que sua configuração General fique assim: 1.4 Instalando pacotes R A funcionalidade que uma nova instalação do R oferece é apenas uma pequena fração do que é possível. De fato, nos referimos ao que você obtém após sua primeira instalação como base R. Funcionalidades adicionais vêm de plugins disponíveis pelos desenvolvedores. Atualmente, existem centenas deles disponíveis no CRAN e muitos outros compartilhados em outros repositórios como o GitHub. No entanto, como nem todo mundo precisa de todas as funções disponíveis, o R disponibiliza diferentes componentes através de pacotes (packages em inglês). R facilita a instalação de pacotes a partir de R. Por exemplo, para instalar o pacote dslabs, que usamos para compartilhar os conjuntos de dados e códigos relacionados a este livro, eles devem escrever: install.packages(&quot;dslabs&quot;) No RStudio, eles podem navegar para a guia Ferramentas_e selecionar_Instalar pacotes. Em seguida, podemos carregar o pacote em nossas sessões R usando a função library: library(dslabs) Ao ler este livro, você verá que carregamos pacotes sem instalá-los. Isso ocorre porque, quando um pacote é instalado, ele permanece instalado e só precisa ser carregado com library. O pacote permanece carregado até terminarmos a sessão R. Se eles tentarem carregar um pacote e receber um erro, provavelmente significa que não o instalaram. Podemos instalar mais de um pacote por vez, fornecendo um vetor de caracteres para esta função: install.packages(c(&quot;tidyverse&quot;, &quot;dslabs&quot;)) Observe que a instalação tidyverse realmente instala vários pacotes. Isso geralmente ocorre quando um pacote possui * dependências * ou usa funções de outros pacotes. Quando eles carregam um pacote usando library, eles também carregam suas dependências. Após a instalação dos pacotes, você pode carregá-los no R e não precisa instalá-los novamente, a menos que instale uma nova versão do R. Lembre-se de que os pacotes estão instalados no R e não no RStudio. É útil manter uma lista de todos os pacotes necessários para o trabalho em um script porque, se eles tiverem que fazer uma nova instalação do R, eles poderão reinstalar todos os pacotes simplesmente executando um script. Eles podem ver todos os pacotes que instalaram usando a seguinte função: installed.packages() https://pdfs.semanticscholar.org/9b48/46f192aa37ca122cfabb1ed1b59866d8bfda.pdf↩ https://opensource.org/history↩ https://stackoverflow.com/documentation/r/topics↩ https://www.rstudio.com/↩ https://github.com/rafalab/dsbook↩ "],
["parte-r.html", "(PARTE) R", " (PARTE) R "],
["r-basics.html", "Capítulo 2 R básico 2.1 O básico 2.2 Exercícios 2.3 Tipo de dados 2.4 Exercícios 2.5 Vetores 2.6 Conversão forçada 2.7 Exercícios 2.8 Ordenação 2.9 Exercícios 2.10 Aritmética vetorial 2.11 Exercícios 2.12 Indexação 2.13 Exercícios 2.14 Gráficos básicos 2.15 Exercícios", " Capítulo 2 R básico Neste livro, usaremos o ambiente _software_R para todas as nossas análises. Eles aprenderão técnicas de análise de dados e R simultaneamente. Portanto, para continuar, eles terão acesso ao R. Também recomendamos o uso de um Integrated Development Environment (IDE), como o RStudio, para salvar seu trabalho. Lembre-se de que é comum que um curso ou oficina ofereça acesso a um ambiente R e um IDE por meio do navegador da web, como faz o RStudio cloud10. Se eles tiverem acesso a esse recurso, eles não precisarão instalar o R ou o RStudio. No entanto, se você quiser se tornar um analista de dados especializado, recomendamos instalar essas ferramentas no seu computador11. O R e o RStudio são gratuitos e estão disponíveis online. Estudo de caso: assassinatos por armas nos EUA EUA Imagine que você mora na Europa e recebe um emprego em uma empresa americana com muitos locais nos Estados Unidos. É um ótimo trabalho, mas manchetes como ** Taxa de homicídio por arma de fogo dos EUA. EUA mais alto do que em outros países desenvolvidos **12 . Preocupa-se? Gráficos como o seguinte podem preocupá-lo ainda mais: Porém, lembra-se que os Estados Unidos são um país grande e diversificado, com 50 estados muito diferentes, além do Distrito de Columbia (DC). A Califórnia, por exemplo, tem uma população maior que o Canadá e 20 estados dos EUA. EUA eles têm populações maiores que a Noruega. Em alguns aspectos, a variabilidade entre os estados dos EUA. EUA é semelhante à variabilidade entre os países da Europa. Além disso, embora não esteja incluído nas tabelas acima, as taxas de homicídio na Lituânia, Ucrânia e Rússia são superiores a quatro por 100.000. Portanto, as notícias pelas quais você está preocupado podem ser muito superficiais. Você tem opções de onde pode morar e deseja determinar a segurança de cada estado individual. Teremos algumas idéias examinando dados relacionados a homicídios por armas de fogo nos EUA. EUA em 2010 usando R. Antes de começarmos o nosso exemplo, precisamos discutir a logística e alguns dos componentes necessários para obter habilidades mais avançadas de R. Lembre-se de que a utilidade de alguns desses componentes nem sempre é imediatamente óbvia, mas mais adiante no livro, você apreciará dominam essas habilidades. 2.1 O básico Antes de começarmos com o conjunto de dados motivadores, precisamos revisar o básico de R. Objetos ### Suponha que alguns alunos do ensino médio nos solicitem ajuda para resolver várias equações quadráticas da forma \\(ax^2+bx+c = 0\\). A fórmula quadrática nos oferece as soluções: \\[ \\frac{-b - \\sqrt{b^2 - 4ac}}{2a}\\,\\, \\mbox{ and } \\frac{-b + \\sqrt{b^2 - 4ac}}{2a} \\] que, é claro, mudam dependendo dos valores de \\(a\\), \\(b\\) e \\(c\\). Uma vantagem das linguagens de programação é poder definir variáveis e escrever expressões com elas, como é feito na matemática, para obter uma solução numérica. Escreveremos um código geral para a equação quadrática abaixo, mas se formos solicitados a resolver \\(x^2 + x -1 = 0\\), então definimos: a &lt;- 1 b &lt;- 1 c &lt;- -1 que armazena os valores para uso posterior. Nós usamos &lt;- para atribuir valores a variáveis. Também podemos atribuir valores usando = ao invés de &lt;-, mas recomendamos não usar = evitar confusão. Copie e cole o código acima no seu console para definir as três variáveis. Observe que R não imprime nada quando fazemos essa atribuição. Isso significa que os objetos foram definidos com sucesso. Se eles tivessem cometido um erro, uma mensagem de erro teria aparecido. Para ver o valor armazenado em uma variável, simplesmente pedimos que R avalie a e isso mostra o valor armazenado: a #&gt; [1] 1 Uma maneira mais explícita de pedir que R mostre o valor armazenado em a é usar print assim: print(a) #&gt; [1] 1 Usamos o termo object para descrever coisas armazenadas em R. Variáveis são exemplos, mas objetos também podem ser entidades mais complicadas, como funções, descritas mais adiante. 2.1.1 O espaço de trabalho Conforme definimos objetos no console, estamos alterando o espaço de trabalho (workspace em inglês). Você pode ver todas as variáveis salvas no seu espaço de trabalho digitando: ls() #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;dat&quot; &quot;img_path&quot; &quot;murders&quot; No RStudio, a guia Environment mostra os valores: Devemos ver a, b e c. Se eles tentarem recuperar o valor de uma variável que não está em seus espaços de trabalho, eles receberão uma mensagem de erro. Por exemplo, se eles escrevem x, você verá o seguinte: Error: object 'x' not found. Agora, como esses valores são armazenados em variáveis, para resolver nossa equação, usamos a fórmula quadrática: (-b + sqrt(b^2 - 4*a*c) )/ ( 2*a ) #&gt; [1] 0.618 (-b - sqrt(b^2 - 4*a*c) )/ ( 2*a ) #&gt; [1] -1.62 2.1.2 Funções Uma vez definidas as variáveis, o processo de análise de dados geralmente pode ser descrito como uma série de funções aplicadas aos dados. R inclui várias funções predefinidas e a maioria das linhas de análise que construímos fazem uso extensivo delas. Nós já usamos as funções install.packages, library e ls. Também usamos a função sqrt para resolver a equação quadrática acima. Existem muitas funções predefinidas e ainda mais podem ser adicionadas através de pacotes. Eles não aparecem nos seus espaços de trabalho porque não foram definidos, mas estão disponíveis para uso imediato. Em geral, precisamos usar parênteses para avaliar uma função. Se eles escreverem ls, a função não é avaliada e, em vez disso, R mostra o código que a define. Se eles escreverem ls(), a função é avaliada e, como já mostrado, vemos objetos no espaço de trabalho. A diferença de ls, a maioria das funções requer um ou mais argumentos. Abaixo está um exemplo de como atribuímos um objeto ao argumento da função log. Lembre-se de que anteriormente definimos a como 1: log(8) #&gt; [1] 2.08 log(a) #&gt; [1] 0 Eles podem descobrir o que a função espera e o que faz, revendo alguns manuais muito úteis incluídos no R. Eles podem obter ajuda para usar a função. help assim: help(&quot;log&quot;) Para a maioria das funções, também podemos usar esta abreviação: ?log A página de ajuda mostrará quais argumentos a função espera. Por exemplo, log precisa de x e base para correr. No entanto, alguns argumentos são obrigatórios e outros são opcionais. Você pode determinar quais são opcionais anotando no documento de ajuda quais padrões são atribuídos com =. Definir isso é opcional. Por exemplo, a base da função log o padrão é base = exp(1) fazendo log o logaritmo natural padrão. Para dar uma rápida olhada nos argumentos sem abrir o sistema de ajuda, você pode escrever: args(log) #&gt; function (x, base = exp(1)) #&gt; NULL Eles podem alterar os valores padrão simplesmente atribuindo outro objeto: log(8, base = 2) #&gt; [1] 3 Lembre-se de que não especificamos o argumento x assim sendo: log(x = 8, base = 2) #&gt; [1] 3 O código acima funciona, mas também podemos economizar um pouco de digitação: se eles não usarem um nome de argumento, R assume que eles estão inserindo argumentos na ordem em que são exibidos na página de ajuda ou por args. Portanto, ao não usar os nomes, R assume que os argumentos são x seguido por base: log(8,2) #&gt; [1] 3 Se eles usarem os nomes dos argumentos, podemos incluí-los na ordem que desejamos: log(base = 2, x = 8) #&gt; [1] 3 Para especificar argumentos, devemos usar = e não &lt;-. Existem algumas exceções à regra de que funções precisam que parênteses sejam avaliados. Entre estes, os mais utilizados são os operadores aritméticos e relacionais. Por exemplo: 2^3 #&gt; [1] 8 Você pode ver os operadores aritméticos digitando: help(&quot;+&quot;) ou ?&quot;+&quot; e os operadores relacionais ao escrever: help(&quot;&gt;&quot;) ou ?&quot;&gt;&quot; 2.1.3 Outros objetos predefinidos Existem vários conjuntos de dados incluídos para os usuários praticarem e testarem as funções. Você pode ver todos os conjuntos de dados disponíveis digitando: data() Isso mostra o nome do objeto para esses conjuntos de dados. Esses conjuntos de dados são objetos que podem ser usados simplesmente digitando o nome. Por exemplo, se eles escreverem: co2 R mostrará os dados da concentração atmosférica de CO2 de Mauna Loa. Outros objetos predefinidos são quantidades matemáticas, como a constante \\(\\pi\\) e \\(\\infty\\): pi #&gt; [1] 3.14 Inf+1 #&gt; [1] Inf 2.1.4 Nomes de variáveis Nós usamos as letras a, b e c como nomes de variáveis, mas estes podem ser quase qualquer coisa. Algumas regras básicas em R são que os nomes de variáveis devem começar com uma letra, não podem conter espaços e não devem ser variáveis predefinidas em R. Por exemplo, não nomeie uma de suas variáveis. install.packages escrevendo algo como: install.packages &lt;- 2. Uma boa convenção a seguir é usar palavras significativas que descrevam o que é armazenado, use apenas letras minúsculas e sublinhados como substitutos de espaços. Para equações quadráticas, podemos usar algo como: solution_1 &lt;- (-b + sqrt(b^2 - 4*a*c))/ (2*a) solution_2 &lt;- (-b - sqrt(b^2 - 4*a*c))/ (2*a) Para obter mais dicas, recomendamos estudar o guia de estilo de Hadley Wickham13. 2.1.5 Como salvar seu espaço de trabalho Os valores permanecem no espaço de trabalho até você terminar suas sessões ou excluí-los com a função rm. Mas os espaços de trabalho também podem ser salvos para uso posterior. De fato, ao sair do R, o programa pergunta se eles desejam salvar seu espaço de trabalho. Se eles salvarem, na próxima vez que iniciarem o R, o programa restaurará a área de trabalho. Recomendamos não salvar a área de trabalho dessa maneira, porque, à medida que você começa a trabalhar em projetos diferentes, será mais difícil acompanhar o que está salvo. Em vez disso, recomendamos que você atribua à área de trabalho um nome específico. Eles podem fazer isso usando as funções save ou save.image. Para carregar, use a função load. Ao salvar um espaço de trabalho, recomendamos o sufixo rda ou RData. No RStudio, eles também podem fazer isso navegando para a guia Session_e escolhendo_Save Workspace as. Eles podem carregá-lo usando as opções Load Workspace na mesma guia. Para saber mais, leia as páginas de ajuda (help pages em inglês) em save, save.image e load. 2.1.6 Motivando scripts Para resolver outra equação como \\(3x^2 + 2x -1\\), podemos copiar e colar o código acima, redefinir as variáveis e recalcular a solução: a &lt;- 3 b &lt;- 2 c &lt;- -1 (-b + sqrt(b^2 - 4*a*c))/ (2*a) (-b - sqrt(b^2 - 4*a*c))/ (2*a) Ao criar e salvar um script com o código acima, você não precisa mais redigitar tudo a cada vez, mas simplesmente alterar os nomes das variáveis. Tente escrever o script acima em um editor e observe como é fácil alterar variáveis e obter uma resposta. Como comentar seu código Se uma linha de código R começar com o símbolo #, não é avaliado. Podemos usar isso para escrever lembretes sobre por que escrevemos um código específico. Por exemplo, no script acima, podemos adicionar: ## Código para calcular la solución a la ecuación cuadrática de la forma ax^2 + bx + c ## definir las variables a &lt;- 3 b &lt;- 2 c &lt;- -1 ## ahora calcule la solución (-b + sqrt(b^2 - 4*a*c))/ (2*a) (-b - sqrt(b^2 - 4*a*c))/ (2*a) 2.2 Exercícios 1. Qual é a soma dos 100 primeiros números inteiros positivos? A fórmula para a soma dos números inteiros \\(1\\) para \\(n\\) é \\(n(n+1)/2\\). Definir \\(n=100\\) e então use R para calcular a soma de \\(1\\) para \\(100\\) usando a fórmula. Qual é a soma? 2. Agora use a mesma fórmula para calcular a soma dos números inteiros de 1 a 1000. 3. Veja a saída de escrever o seguinte código em R: n &lt;- 1000 x &lt;- seq(1, n) sum(x) Com base no resultado, o que você acha que as funções fazem seq e sum? Você pode usar help. para. sum crie uma lista de números e seq adiciona-os. b. seq crie uma lista de números e sum adiciona-os. c. seq crie uma lista aleatória e sum calcula a soma de 1 a 1000. d. sum sempre retorna o mesmo número. 4. Em matemática e programação, dizemos que avaliamos uma função quando substituímos o argumento por um determinado número. Então, se escrevermos sqrt(4), avaliamos a função sqrt. Em R, você pode avaliar uma função dentro de outra função. As avaliações acontecem de dentro para fora. Use uma linha de código para calcular o logaritmo, base 10, da raiz quadrada de 100. 5. Qual das seguintes opções sempre retornará o valor numérico armazenado em x? Você pode tentar os exemplos e usar o sistema de ajuda, se desejar. para. log(10^x) b. log10(x^10) c. log(exp(x)) d. exp(log(x, base = 2)) 2.3 Tipo de dados Variáveis em R podem ser de tipos diferentes. Por exemplo, precisamos distinguir números de seqüências de caracteres e tabelas de listas de números simples. A função class isso nos ajuda a determinar que tipo de objeto temos: a &lt;- 2 class(a) #&gt; [1] &quot;numeric&quot; Para trabalhar com eficiência no R, é importante aprender os diferentes tipos de variáveis e o que podemos fazer com elas. 2.3.1 data frames Até agora, as variáveis que definimos são apenas um número. Isso não é muito útil para armazenar dados. A maneira mais comum de armazenar um conjunto de dados no R é usar um data frame. Conceitualmente, podemos pensar em um data frame como uma tabela com linhas que representam observações e com colunas que representam as diferentes variáveis coletadas para cada observação. Frames de dados são particularmente úteis para conjuntos de dados porque podemos combinar diferentes tipos de dados em um único objeto. Uma grande proporção dos desafios da análise de dados começa com os dados armazenados em um data frame. Por exemplo, armazenamos os dados do nosso exemplo motivador em um data frame. Eles podem acessar esse conjunto de dados carregando o pacote dslabs e, em seguida, usando a função data carregar o conjunto de dados murders : library(dslabs) data(murders) Para verificar se este é um data frame, escrevemos: class(murders) #&gt; [1] &quot;data.frame&quot; 2.3.2 Como examinar um objeto A função str é útil obter mais informações sobre a estrutura de um objeto: str(murders) #&gt; &#39;data.frame&#39;: 51 obs. of 5 variables: #&gt; $ state : chr &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... #&gt; $ abb : chr &quot;AL&quot; &quot;AK&quot; &quot;AZ&quot; &quot;AR&quot; ... #&gt; $ region : Factor w/ 4 levels &quot;Northeast&quot;,&quot;South&quot;,..: 2 4 4 2 4 4 1 2 2 #&gt; 2 ... #&gt; $ population: num 4779736 710231 6392017 2915918 37253956 ... #&gt; $ total : num 135 19 232 93 1257 ... Isso nos diz muito mais sobre o objeto. Vemos que a tabela possui 51 linhas (50 estados mais DC) e cinco variáveis. Podemos exibir as seis primeiras linhas usando a função head: head(murders) #&gt; state abb region population total #&gt; 1 Alabama AL South 4779736 135 #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 #&gt; 4 Arkansas AR South 2915918 93 #&gt; 5 California CA West 37253956 1257 #&gt; 6 Colorado CO West 5029196 65 Nesse conjunto de dados, cada estado é considerado uma observação e cinco variáveis são incluídas para cada estado. Antes de continuar com a resposta à nossa pergunta original sobre os diferentes estados, vamos revisar mais sobre os componentes deste objeto. 2.3.3 O operador de acesso: $ Para nossa análise, precisaremos acessar as diferentes variáveis representadas pelas colunas incluídas neste data frame. Para fazer isso, usamos o operador de acesso $ da seguinte maneira: murders$population #&gt; [1] 4779736 710231 6392017 2915918 37253956 5029196 3574097 #&gt; [8] 897934 601723 19687653 9920000 1360301 1567582 12830632 #&gt; [15] 6483802 3046355 2853118 4339367 4533372 1328361 5773552 #&gt; [22] 6547629 9883640 5303925 2967297 5988927 989415 1826341 #&gt; [29] 2700551 1316470 8791894 2059179 19378102 9535483 672591 #&gt; [36] 11536504 3751351 3831074 12702379 1052567 4625364 814180 #&gt; [43] 6346105 25145561 2763885 625741 8001024 6724540 1852994 #&gt; [50] 5686986 563626 Mas como sabíamos usar population? Anteriormente, aplicando a função str para o objeto murders nós revelamos os nomes de cada uma das cinco variáveis armazenadas nesta tabela. Podemos acessar rapidamente nomes de variáveis usando: names(murders) #&gt; [1] &quot;state&quot; &quot;abb&quot; &quot;region&quot; &quot;population&quot; &quot;total&quot; É importante saber que a ordem das entradas no murders$population ele preserva a ordem das linhas em nossa tabela de dados. Isso nos permitirá manipular uma variável com base nos resultados de outra. Por exemplo, podemos classificar os nomes dos estados de acordo com o número de assassinatos. Dica: R vem com uma funcionalidade de preenchimento automático muito boa, que evita o trabalho de digitar todos os nomes. Escreva murders$p e pressione a tecla tab no teclado. Essa funcionalidade e muitos outros recursos úteis de preenchimento automático estão disponíveis no RStudio. Vetores: numérico, caractere e lógico O objeto murders$population não é um número, mas vários. Chamamos esse tipo de objeto de vectors. Um número único é tecnicamente um vetor de comprimento 1, mas em geral usamos o termo vetores para se referir a objetos com várias entradas. A função length informa quantas entradas há no vetor: pop &lt;- murders$population length(pop) #&gt; [1] 51 Esse vetor específico é numeric, pois os tamanhos da população são números: class(pop) #&gt; [1] &quot;numeric&quot; Em um vetor numérico, cada entrada deve ser um número. Para armazenar uma cadeia de caracteres, os vetores também podem ser da classe character. Por exemplo, os nomes dos estados são caracteres: class(murders$state) #&gt; [1] &quot;character&quot; Assim como nos vetores numéricos, todas as entradas em um vetor de caracteres devem ter um caractere. Outro tipo importante de vetores são os vetores lógicos. Estes devem ser TRUE ou FALSE. z &lt;- 3 == 2 z #&gt; [1] FALSE class(z) #&gt; [1] &quot;logical&quot; Aqui o == é um operador relacional que pergunta se 3 é igual a 2. Em R, use apenas um = atribuir uma variável, mas se eles usarem duas == então avalia se os objetos são os mesmos. Você pode ver isso escrevendo: ?Comparison Nas próximas seções, você verá como os operadores relacionais podem ser úteis. Discutimos as características mais importantes dos vetores após os exercícios a seguir. Avançado: matematicamente, os valores em pop eles são números inteiros e há uma classe inteira em R. No entanto, por padrão, os números recebem uma classe numérica mesmo quando são números inteiros redondos. Por exemplo, class(1) retorna numérico. Eles podem convertê-lo em um número inteiro de classe com a função as.integer() ou adicionando um L assim: 1L. Considere a classe escrevendo: class(1L) Fatores ### {#factors} No conjunto de dados murders, você também pode esperar que a região seja um vetor de caracteres. No entanto, não é: class(murders$region) #&gt; [1] &quot;factor&quot; É um fator. Fatores são úteis para armazenar dados categóricos. Podemos ver que existem apenas quatro regiões ao usar a função levels: levels(murders$region) #&gt; [1] &quot;Northeast&quot; &quot;South&quot; &quot;North Central&quot; &quot;West&quot; Em segundo plano, R armazena esses níveis como números inteiros e mantém um mapa para acompanhar os rótulos. Isso é mais eficiente em termos de memória do que armazenar todos os caracteres. Observe que os níveis têm uma ordem diferente da ordem de aparência no fator. Em R, por padrão, os níveis são organizados em ordem alfabética. No entanto, geralmente queremos que os níveis sigam uma ordem diferente. Eles podem especificar uma ordem através do argumento levels quando eles criam o fator com a função factor. Por exemplo, no conjunto de dados de assassinatos, as regiões são ordenadas de leste a oeste. A função ‘reordenar’ permite alterar a ordem dos níveis de um fator de acordo com um resumo calculado em um vetor numérico. Vamos demonstrar isso com um exemplo simples e ver os mais avançados na parte de visualização de dados do livro. Suponha que desejemos que os níveis da região sejam ordenados de acordo com o número total de assassinatos, e não por ordem alfabética. Se houver valores associados a cada level, podemos usar reorder e especifique um resumo dos dados para determinar o pedido. O código a seguir pega a soma do total de assassinatos em cada região e reordena o fator de acordo com essas somas. region &lt;- murders$region value &lt;- murders$total region &lt;- reorder(region, value, FUN = sum) levels(region) #&gt; [1] &quot;Northeast&quot; &quot;North Central&quot; &quot;West&quot; &quot;South&quot; A nova ordem é consistente com o fato de que há menos assassinatos no Nordeste e mais no Sul. Aviso: os fatores podem causar confusão, pois às vezes eles se comportam como personagens e às vezes não. Como resultado, essas são uma fonte comum de erros. 2.3.4 Listas Dados frames_são um caso especial de_lists. Abordaremos as listas com mais detalhes posteriormente, mas sabemos que elas são úteis porque podem armazenar qualquer combinação de diferentes tipos de dados. Aqui, criamos uma lista para servir como exemplo: record #&gt; $name #&gt; [1] &quot;John Doe&quot; #&gt; #&gt; $student_id #&gt; [1] 1234 #&gt; #&gt; $grades #&gt; [1] 95 82 91 97 93 #&gt; #&gt; $final_grade #&gt; [1] &quot;A&quot; class(record) #&gt; [1] &quot;list&quot; Como em data frames, eles podem extrair os componentes de uma lista com o operador de acesso: $. De fato, data frames são um tipo de lista. record$student_id #&gt; [1] 1234 Também podemos usar colchetes duplos ( [[) assim: record[[&quot;student_id&quot;]] #&gt; [1] 1234 Eles devem se acostumar com o fato de que geralmente existem várias maneiras no R de fazer a mesma coisa, como acessar entradas. Você também pode encontrar listas sem nomes de variáveis: record2 #&gt; [[1]] #&gt; [1] &quot;John Doe&quot; #&gt; #&gt; [[2]] #&gt; [1] 1234 Se uma lista não tiver nomes, eles não poderão extrair os elementos com $, mas eles podem usar o método de colchete. Em vez de fornecer o nome da variável, você pode fornecer o índice da lista da seguinte maneira: record2[[1]] #&gt; [1] &quot;John Doe&quot; Não usaremos listas até mais tarde, mas você pode encontrar uma em suas próprias explorações de R. É por isso que mostramos alguns conceitos básicos aqui. 2.3.5 Arrays Matrizes são outro tipo comum de objeto em R. Matrizes são semelhantes a data frames, pois são bidimensionais: possuem linhas e colunas. No entanto, como vetores numéricos, caracteres e lógicos, as entradas nas matrizes devem ser do mesmo tipo. Por esse motivo, data frames são muito mais úteis para armazenar dados, pois podemos conter caracteres, fatores e números. Entretanto, as matrizes têm uma grande vantagem sobre os data frames: podemos executar operações de álgebra matricial, uma poderosa técnica matemática. Não descrevemos essas operações neste livro, mas muito do que acontece em segundo plano quando você executa a análise de dados envolve matrizes. Abordamos matrizes em mais detalhes no Capítulo ?? mas também os discutimos brevemente aqui, como algumas das funções que aprenderemos matrizes de retorno. Podemos definir uma matriz usando a função matrix. Precisamos especificar o número de linhas e colunas: mat &lt;- matrix(1:12, 4, 3) mat #&gt; [,1] [,2] [,3] #&gt; [1,] 1 5 9 #&gt; [2,] 2 6 10 #&gt; [3,] 3 7 11 #&gt; [4,] 4 8 12 Eles podem acessar entradas específicas em uma matriz usando colchetes ( [) Se você deseja a segunda linha, terceira coluna, use: mat[2, 3] #&gt; [1] 10 Se você deseja a segunda linha inteira, deixe o lugar da coluna vazio: mat[2, ] #&gt; [1] 2 6 10 Observe que isso retorna um vetor, não uma matriz. Da mesma forma, se você quiser a terceira coluna inteira, deixe o lugar da linha vazio: mat[, 3] #&gt; [1] 9 10 11 12 Este também é um vetor, não uma matriz. Eles podem acessar mais de uma coluna ou mais de uma linha, se desejarem. Isso lhe dará uma nova matriz. mat[, 2:3] #&gt; [,1] [,2] #&gt; [1,] 5 9 #&gt; [2,] 6 10 #&gt; [3,] 7 11 #&gt; [4,] 8 12 Eles podem criar subconjuntos com base em linhas e colunas: mat[1:2, 2:3] #&gt; [,1] [,2] #&gt; [1,] 5 9 #&gt; [2,] 6 10 Podemos converter as matrizes em data frames usando a função as.data.frame: as.data.frame(mat) #&gt; V1 V2 V3 #&gt; 1 1 5 9 #&gt; 2 2 6 10 #&gt; 3 3 7 11 #&gt; 4 4 8 12 Também podemos usar colchetes individuais ( [) para acessar as linhas e colunas de um data frame: data(&quot;murders&quot;) murders[25, 1] #&gt; [1] &quot;Mississippi&quot; murders[2:3, ] #&gt; state abb region population total #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 2.4 Exercícios 1. Carregue o conjunto de dados de assassinatos nos EUA. EUA library(dslabs) data(murders) Usar função str para examinar a estrutura do objeto murders. Qual das opções a seguir descreve melhor as variáveis representadas neste data frame? para. Os 51 estados. b. Taxas de assassinato para todos os 50 estados e DC. c. O nome do estado, a abreviação do nome do estado, região e população do estado e o número total de assassinatos em 2010 no estado. d. str não exibe informações relevantes. 2. Quais são os nomes das colunas usadas pelo data frame para essas cinco variáveis? 3. Use o operador de acesso $ extrair abreviações de estado e atribuí-las ao objeto a. Qual é a classe deste objeto? 4. Agora use os colchetes para extrair as abreviações de estado e atribuí-las ao objeto b. Use a função identical para determinar se a e b são iguais. 5. Vimos que a coluna region armazena um fator. Você pode verificar isso digitando: class(murders$region) Com uma linha de código, use as funções levels e length para determinar o número de regiões definidas por este conjunto de dados. 6. A função table pega um vetor e retorna a frequência de cada elemento. Você pode ver rapidamente quantos estados existem em cada região aplicando esta função. Use esta função em uma linha de código para criar uma tabela de estados por região. 2.5 Vetores No R, os objetos mais básicos disponíveis para armazenar dados são vectors. Como vimos, conjuntos de dados complexos geralmente podem ser divididos em componentes que são vetores. Por exemplo, em um data frame, cada coluna é um vetor. Aqui nós aprendemos mais sobre esta importante classe. 2.5.1 Como criar vetores Podemos criar vetores usando a função c, o que significa concatenate. Nós usamos c concatenar entradas da seguinte maneira: codes &lt;- c(380, 124, 818) codes #&gt; [1] 380 124 818 Também podemos criar vetores de caracteres. Usamos aspas para indicar que as entradas são caracteres e não nomes de variáveis. country &lt;- c(&quot;italy&quot;, &quot;canada&quot;, &quot;egypt&quot;) Em R, eles também podem usar aspas simples: country &lt;- c(&#39;italy&#39;, &#39;canada&#39;, &#39;egypt&#39;) Mas tome cuidado para não confundir as aspas simples 'com back quote `. Até agora você deve saber que se você escrever: country &lt;- c(italy, canada, egypt) recebe um erro porque as variáveis italy, canada e egypt não estão definidos. Se não usarmos as aspas, R procurará variáveis com esses nomes e retornará um erro. 2.5.2 Nomes Às vezes, é útil nomear as entradas para um vetor. Por exemplo, ao definir um vetor de códigos de países, podemos usar os nomes para conectar os dois: codes &lt;- c(italy = 380, canada = 124, egypt = 818) codes #&gt; italy canada egypt #&gt; 380 124 818 O objeto codes ainda é um vetor numérico: class(codes) #&gt; [1] &quot;numeric&quot; mas com nomes: names(codes) #&gt; [1] &quot;italy&quot; &quot;canada&quot; &quot;egypt&quot; Se o uso de strings sem aspas parecer confuso, saiba que você também pode usar aspas: codes &lt;- c(&quot;italy&quot; = 380, &quot;canada&quot; = 124, &quot;egypt&quot; = 818) codes #&gt; italy canada egypt #&gt; 380 124 818 Não há diferença entre esta chamada de função (função chamada em inglês) e a anterior. Essa é uma das muitas maneiras pelas quais R é peculiar em comparação com outros idiomas. Também podemos atribuir nomes usando a função names: codes &lt;- c(380, 124, 818) country &lt;- c(&quot;italy&quot;,&quot;canada&quot;,&quot;egypt&quot;) names(codes) &lt;- country codes #&gt; italy canada egypt #&gt; 380 124 818 2.5.3 Sequências Outra função útil para criar vetores gera sequências: seq(1, 10) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 O primeiro argumento define o começo e o segundo define o fim que incluímos. O padrão é subir em incrementos de 1, mas um terceiro argumento nos permite determinar quanto saltar: seq(1, 10, 2) #&gt; [1] 1 3 5 7 9 Se queremos números inteiros consecutivos, podemos usar a seguinte abreviação: 1:10 #&gt; [1] 1 2 3 4 5 6 7 8 9 10 Quando usamos essas funções, R produz números inteiros, não números, porque geralmente são usados para indexar algo: class(1:10) #&gt; [1] &quot;integer&quot; No entanto, se criarmos uma sequência que não inclua números inteiros, a classe mudará: class(seq(1, 10, 0.5)) #&gt; [1] &quot;numeric&quot; 2.5.4 Como criar um subconjunto Usamos colchetes para acessar elementos específicos de um vetor. Para vetor codes como definido anteriormente, podemos acessar o segundo elemento usando: codes[2] #&gt; canada #&gt; 124 Eles podem obter mais de uma entrada usando um vetor de múltiplas entradas como índice: codes[c(1,3)] #&gt; italy egypt #&gt; 380 818 As seqüências definidas acima são particularmente úteis se precisarmos acessar, digamos, os dois primeiros elementos: codes[1:2] #&gt; italy canada #&gt; 380 124 Se os elementos tiverem nomes, também podemos acessar as entradas usando esses nomes. Aqui estão dois exemplos: codes[&quot;canada&quot;] #&gt; canada #&gt; 124 codes[c(&quot;egypt&quot;,&quot;italy&quot;)] #&gt; egypt italy #&gt; 818 380 2.6 Conversão forçada Em geral, forced_conversion é uma tentativa do R de ser flexível com os tipos de dados. Quando uma entrada não corresponde ao esperado, algumas das funções predefinidas de R tentam adivinhar o que se estava tentando fazer antes de retornar uma mensagem de erro. Isso também pode causar confusão. Não entendendo a conversão forçada, os programadores podem enlouquecer quando codificam em R, pois o R se comporta de maneira bem diferente da maioria das outras linguagens nesse sentido. Vamos aprender mais com alguns exemplos. Dissemos que os vetores devem ser todos do mesmo tipo. Portanto, se tentarmos combinar, por exemplo, números e caracteres, eles podem esperar um erro: x &lt;- c(1, &quot;canada&quot;, 3) Mas isso não nos dá um, nem mesmo um aviso! O que aconteceu? Veja x e sua classe: x #&gt; [1] &quot;1&quot; &quot;canada&quot; &quot;3&quot; class(x) #&gt; [1] &quot;character&quot; R forçou uma conversão dos dados em caracteres. Como inserimos uma sequência de caracteres no vetor, R adivinhou que nossa intenção era que 1 e 3 fossem as cadeias de caracteres &quot;1&quot; e &quot; 3 O fato de nem sequer emitir um aviso é um exemplo de como a conversão forçada pode causar muitos erros inadvertidos no R. R também oferece funções para mudar de um tipo para outro. Por exemplo, eles podem converter números em caracteres com: x &lt;- 1:5 y &lt;- as.character(x) y #&gt; [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; Eles podem reverter para o acima com as.numeric: as.numeric(y) #&gt; [1] 1 2 3 4 5 Essa função é muito útil, pois conjuntos de dados que incluem números, como cadeias de caracteres, são comuns. 2.6.1 Não disponível (NA) Quando uma função tenta forçar uma conversão de um tipo para outro e encontra um caso impossível, geralmente nos dá um aviso e converte a entrada em um valor especial chamado NA o que significa não disponível (não disponível em inglês). Por exemplo: x &lt;- c(&quot;1&quot;, &quot;b&quot;, &quot;3&quot;) as.numeric(x) #&gt; Warning: NAs introduced by coercion #&gt; [1] 1 NA 3 R não sabe o número que eles queriam quando escreveram b e não tente adivinhar. Como cientistas de dados, você encontrará os NA freqüentemente, como geralmente é usado para dados ausentes (missing data in English), um problema comum em conjuntos de dados do mundo real. 2.7 Exercícios 1. Usar função c criar um vetor com as temperaturas médias altas em janeiro para Pequim, Lagos, Paris, Rio de Janeiro, San Juan e Toronto, com 35, 88, 42, 84, 81 e 30 graus Fahrenheit. Chame o objeto temp. 2. Agora crie um vetor com os nomes das cidades e chame o objeto city. 3. Use a função names e os objetos definidos nos exercícios anteriores para associar os dados de temperatura à cidade correspondente. 4. Use operadores [ e : para acessar a temperatura das três primeiras cidades da lista. 5. Use o operador [ para acessar a temperatura de Paris e San Juan. 6. Use o operador : para criar uma sequência de números \\(12,13,14,\\dots,73\\). 7. Crie um vetor contendo todos os números ímpares positivos menores que 100. 8. Crie um vetor de números que comece em 6, não ultrapasse 55 e adicione números em incrementos de 4/7: 6, 6 + 4/7, 6 + 8/7 e assim por diante. Quantos números a lista possui? Dica: use seq e length. 9. Qual é a classe do seguinte objeto? a &lt;- seq(1, 10, 0.5)? 10. Qual é a classe do seguinte objeto? a &lt;- seq(1, 10)? Onze. A aula de class(a&lt;-1) é numérico, não inteiro. R por padrão é numérico e para forçar um número inteiro, você deve adicionar a letra L. Confirme se a classe de 1L é inteiro. 12. Defina o seguinte vetor: x &lt;- c(&quot;1&quot;, &quot;3&quot;, &quot;5&quot;) e forçá-lo a obter números inteiros. 2.8 Ordenação Agora que dominamos alguns conhecimentos básicos de R, vamos tentar obter algumas dicas sobre a segurança de diferentes estados no contexto de matança de armas. 2.8.1 sort Digamos que queremos classificar os estados do mais baixo para o mais alto, com base no assassinato de armas. A função sort classifica um vetor em ordem crescente. Portanto, podemos ver o maior número de assassinatos por arma, escrevendo: library(dslabs) data(murders) sort(murders$total) #&gt; [1] 2 4 5 5 7 8 11 12 12 16 19 21 22 #&gt; [14] 27 32 36 38 53 63 65 67 84 93 93 97 97 #&gt; [27] 99 111 116 118 120 135 142 207 219 232 246 250 286 #&gt; [40] 293 310 321 351 364 376 413 457 517 669 805 1257 No entanto, isso não nos fornece informações sobre quais estados têm o total de assassinatos. Por exemplo, não sabemos em que estado ele estava 1257. 2.8.2 order A função order é mais apropriado para o que queremos fazer. order pega um vetor como entrada e retorna o vetor de índices que classificam o vetor de entrada. Isso pode ser um pouco confuso, então vamos estudar um exemplo simples. Podemos criar um vetor e ordená-lo (sort em inglês): x &lt;- c(31, 4, 15, 92, 65) sort(x) #&gt; [1] 4 15 31 65 92 Em vez de ordenar o vetor de entrada, a função order retorna o índice que classifica o vetor de entrada: index &lt;- order(x) x[index] #&gt; [1] 4 15 31 65 92 Este é o mesmo resultado que retorna você sort(x). Se olharmos para este índice, veremos por que ele funciona: x #&gt; [1] 31 4 15 92 65 order(x) #&gt; [1] 2 3 1 5 4 A segunda entrada de x é o menor, então order(x) começa com 2. O próximo menor é a terceira entrada, portanto, a segunda entrada é 3 e assim por diante. Como isso nos ajuda a classificar os estados por assassinato? Primeiro, lembre-se de que as entradas de vetor que você acessa com $ eles seguem a mesma ordem que as linhas da tabela. Por exemplo, esses dois vetores que contêm os nomes dos estados e suas abreviações, respectivamente, seguem a mesma ordem: murders$state[1:6] #&gt; [1] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; &quot;California&quot; #&gt; [6] &quot;Colorado&quot; murders$abb[1:6] #&gt; [1] &quot;AL&quot; &quot;AK&quot; &quot;AZ&quot; &quot;AR&quot; &quot;CA&quot; &quot;CO&quot; Isso significa que podemos classificar os nomes dos estados com base no total de assassinatos. Primeiro, obtemos o índice que ordena os vetores pelo número total de assassinatos e depois colocamos o vetor de nomes de estados em um índice: ind &lt;- order(murders$total) murders$abb[ind] #&gt; [1] &quot;VT&quot; &quot;ND&quot; &quot;NH&quot; &quot;WY&quot; &quot;HI&quot; &quot;SD&quot; &quot;ME&quot; &quot;ID&quot; &quot;MT&quot; &quot;RI&quot; &quot;AK&quot; &quot;IA&quot; &quot;UT&quot; #&gt; [14] &quot;WV&quot; &quot;NE&quot; &quot;OR&quot; &quot;DE&quot; &quot;MN&quot; &quot;KS&quot; &quot;CO&quot; &quot;NM&quot; &quot;NV&quot; &quot;AR&quot; &quot;WA&quot; &quot;CT&quot; &quot;WI&quot; #&gt; [27] &quot;DC&quot; &quot;OK&quot; &quot;KY&quot; &quot;MA&quot; &quot;MS&quot; &quot;AL&quot; &quot;IN&quot; &quot;SC&quot; &quot;TN&quot; &quot;AZ&quot; &quot;NJ&quot; &quot;VA&quot; &quot;NC&quot; #&gt; [40] &quot;MD&quot; &quot;OH&quot; &quot;MO&quot; &quot;LA&quot; &quot;IL&quot; &quot;GA&quot; &quot;MI&quot; &quot;PA&quot; &quot;NY&quot; &quot;FL&quot; &quot;TX&quot; &quot;CA&quot; De acordo com o exposto, a Califórnia teve o maior número de assassinatos. 2.8.3 max e which.max Se estivermos interessados apenas na entrada com o valor mais alto, podemos usar max para esse valor: max(murders$total) #&gt; [1] 1257 e which.max para o índice de valor mais alto: i_max &lt;- which.max(murders$total) murders$state[i_max] #&gt; [1] &quot;California&quot; No mínimo, podemos usar min e which.min do mesmo modo. Isso significa que a Califórnia é o estado mais perigoso? Em uma próxima seção, argumentamos que devemos considerar taxas e não totais. Antes de fazer isso, apresentamos uma última função relacionada à ordem: rank. 2.8.4 rank Embora não seja usado com tanta frequência order e sort, a função rank também está relacionado à ordem e pode ser útil. Para qualquer vetor, rank retorna um vetor com o intervalo da primeira entrada, segunda entrada etc. do vetor de entrada. Aqui está um exemplo simples: x &lt;- c(31, 4, 15, 92, 65) rank(x) #&gt; [1] 3 1 2 5 4 Para resumir, vejamos os resultados das três funções que discutimos: original sort order rank 31 4 2 3 4 15 3 1 15 31 1 2 92 65 5 5 65 92 4 4 2.8.5 Cuidado com a reciclagem Outra fonte comum de erros inadvertidos no R é o uso de recycling. Vimos como os vetores são adicionados por elementos. Portanto, se os vetores não coincidirem em comprimento, é natural supor que recebemos um erro. Mas esse não é o caso. Veja o que acontece: x &lt;- c(1,2,3) y &lt;- c(10, 20, 30, 40, 50, 60, 70) x+y #&gt; Warning in x + y: longer object length is not a multiple of shorter #&gt; object length #&gt; [1] 11 22 33 41 52 63 71 Recebemos um aviso, mas não há erro. Para output, R reciclou os números em x. Veja o último dígito dos números na saída. 2.9 Exercícios Para esses exercícios, usaremos o conjunto de dados de assassinatos nos EUA. Certifique-se de carregá-lo antes de começar. library(dslabs) data(&quot;murders&quot;) 1. Use o operador $ para acessar dados de tamanho da população e armazená-los como objeto pop. Então use a função sort redefinir pop então está em ordem alfabética. Por fim, use o operador [ para indicar o menor tamanho populacional. 2. Agora, em vez do menor tamanho populacional, encontre o índice da entrada com o menor tamanho populacional. Dica: use order ao invés de sort. 3. Podemos realizar a mesma operação que no exercício anterior usando a função which.min. Escreva uma linha de código que faça isso. 4. Agora sabemos o quão pequeno é o menor estado e qual linha o representa. Que estado é esse? Definir uma variável states para ser os nomes dos estados do data frame murders. Digite o nome do estado com a menor população. 5. Você pode criar um data frame usando a função data.frame. Aqui está um exemplo: temp &lt;- c(35, 88, 42, 84, 81, 30) city &lt;- c(&quot;Beijing&quot;, &quot;Lagos&quot;, &quot;Paris&quot;, &quot;Rio de Janeiro&quot;, &quot;San Juan&quot;, &quot;Toronto&quot;) city_temps &lt;- data.frame(name = city, temperature = temp) Use a função rank para determinar a faixa populacional de cada estado, do menos populoso ao mais populoso. Armazene esses intervalos em um objeto chamado ranks, crie um data frame com o nome do estado e seu intervalo. Nomeie o data frame my_df. 6. Repita o exercício anterior, mas desta vez solicite my_df para que os estados estejam na ordem de menos populosos para os mais populosos. Dica: crie um objeto ind que armazena os índices necessários para ordenar os valores da população. Em seguida, use o operador de suporte [ para reordenar cada coluna no data frame. 7. O vetor na_example representa uma série de contagens. Você pode navegar rapidamente pelo objeto usando: data(&quot;na_example&quot;) str(na_example) #&gt; int [1:1000] 2 1 3 2 1 3 1 4 3 2 ... No entanto, quando calculamos a média com a função mean, temos um NA: mean(na_example) #&gt; [1] NA A função is.na retorna um vetor lógico que nos diz quais entradas são NA. Atribua esse vetor lógico a um objeto chamado ind e determine quantas NA barbear na_example. 8. Agora calcule a média novamente, mas apenas para as entradas que não são NA. Dica: lembre-se do operador !. 2.10 Aritmética vetorial A Califórnia teve mais assassinatos, mas isso significa que é o estado mais perigoso? E se você tiver muito mais pessoas do que qualquer outro estado? Podemos confirmar rapidamente que a Califórnia tem a maior população: library(dslabs) data(&quot;murders&quot;) murders$state[which.max(murders$population)] #&gt; [1] &quot;California&quot; mais que 37 milhões de habitantes. Portanto, é injusto comparar os totais se estivermos interessados em saber quão seguro é o estado. O que realmente devemos calcular são os assassinatos per capita. Os relatórios que descrevemos na seção motivadora usam assassinatos por 100.000 como unidade. Para calcular essa quantidade, usamos os poderosos recursos aritméticos vetoriais de R. 2.10.1 Rescalar um vetor Em R, operações aritméticas em vetores ocorrem elemento a elemento. Como exemplo, suponha que tenhamos a altura em polegadas (inches em inglês): inches &lt;- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70) e queremos convertê-lo em centímetros. Observe o que acontece quando multiplicamos inches por 2.54: inches * 2.54 #&gt; [1] 175 157 168 178 178 185 170 185 170 178 Acima, multiplicamos cada elemento por 2,54. Da mesma forma, se para cada entrada queremos calcular quantas polegadas mais altas, ou quantas menores, que 69 polegadas (a altura média para homens), podemos subtraí-la de cada entrada desta maneira: inches - 69 #&gt; [1] 0 -7 -3 1 1 4 -2 4 -2 1 2.10.2 Dois vetores Se tivermos dois vetores do mesmo comprimento e os adicionarmos em R, eles serão adicionados entrada por entrada da seguinte maneira: \\[ \\begin{pmatrix} a\\\\ b\\\\ c\\\\ d \\end{pmatrix} + \\begin{pmatrix} e\\\\ f\\\\ g\\\\ h \\end{pmatrix} = \\begin{pmatrix} a +e\\\\ b + f\\\\ c + g\\\\ d + h \\end{pmatrix} \\] O mesmo se aplica a outras operações matemáticas, como -, * e /. Isso implica que, para calcular as taxas de homicídio (murder rates em inglês), podemos simplesmente escrever: murder_rate &lt;- murders$total/ murders$population * 100000 Ao fazer isso, percebemos que a Califórnia não está mais perto do topo da lista. De fato, podemos usar o que aprendemos para colocar os estados em ordem pela taxa de homicídios: murders$abb[order(murder_rate)] #&gt; [1] &quot;VT&quot; &quot;NH&quot; &quot;HI&quot; &quot;ND&quot; &quot;IA&quot; &quot;ID&quot; &quot;UT&quot; &quot;ME&quot; &quot;WY&quot; &quot;OR&quot; &quot;SD&quot; &quot;MN&quot; &quot;MT&quot; #&gt; [14] &quot;CO&quot; &quot;WA&quot; &quot;WV&quot; &quot;RI&quot; &quot;WI&quot; &quot;NE&quot; &quot;MA&quot; &quot;IN&quot; &quot;KS&quot; &quot;NY&quot; &quot;KY&quot; &quot;AK&quot; &quot;OH&quot; #&gt; [27] &quot;CT&quot; &quot;NJ&quot; &quot;AL&quot; &quot;IL&quot; &quot;OK&quot; &quot;NC&quot; &quot;NV&quot; &quot;VA&quot; &quot;AR&quot; &quot;TX&quot; &quot;NM&quot; &quot;CA&quot; &quot;FL&quot; #&gt; [40] &quot;TN&quot; &quot;PA&quot; &quot;AZ&quot; &quot;GA&quot; &quot;MS&quot; &quot;MI&quot; &quot;DE&quot; &quot;SC&quot; &quot;MD&quot; &quot;MO&quot; &quot;LA&quot; &quot;DC&quot; 2.11 Exercícios 1. Criamos anteriormente este data frame: temp &lt;- c(35, 88, 42, 84, 81, 30) city &lt;- c(&quot;Beijing&quot;, &quot;Lagos&quot;, &quot;Paris&quot;, &quot;Rio de Janeiro&quot;, &quot;San Juan&quot;, &quot;Toronto&quot;) city_temps &lt;- data.frame(name = city, temperature = temp) Recrie o data frame usando o código acima, mas adicione uma linha que converta a temperatura de Fahrenheit em Celsius. A conversão é \\(C = \\frac{5}{9} \\times (F - 32)\\). 2. Qual é a seguinte soma? \\(1+1/2^2 + 1/3^2 + \\dots 1/100^2\\)? Dica: Graças a Euler, sabemos que você deve estar perto de \\(\\pi^2/6\\). 3. Calcule a taxa de homicídios por 100.000 para cada estado e armazene-a no objeto murder_rate. Em seguida, encontre a taxa média de homicídios nos EUA. EUA com função mean. Qual é a média? 2.12 Indexação O R fornece uma maneira poderosa e conveniente de indexar vetores. Podemos, por exemplo, criar um subconjunto de um vetor com base nas propriedades de outro vetor. Nesta seção, continuamos a trabalhar em nosso exemplo de assassinato nos EUA. EUA, que podemos carregar assim: library(dslabs) data(&quot;murders&quot;) 2.12.1 Criar subconjuntos com lógicas Agora calculamos a taxa de homicídios usando: murder_rate &lt;- murders$total/ murders$population * 100000 Imagine se mudar da Itália, onde, segundo uma reportagem, a taxa de homicídios é de apenas 0,71 por 100.000. Você prefere mudar para um estado com uma taxa de homicídios semelhante. Outro recurso poderoso do R é que podemos usar lógicas para indexar vetores. Se compararmos um vetor com um único número, R realiza o teste para cada entrada. Aqui está um exemplo relacionado à pergunta anterior: ind &lt;- murder_rate &lt; 0.71 Se, em vez disso, queremos saber se um valor é menor ou igual, podemos usar: ind &lt;- murder_rate &lt;= 0.71 Lembre-se de que recuperamos um vetor lógico com TRUE para cada entrada menor ou igual a 0,71. Para ver quais são esses estados, podemos tirar proveito do fato de que vetores podem ser indexados com lógicas. murders$state[ind] #&gt; [1] &quot;Hawaii&quot; &quot;Iowa&quot; &quot;New Hampshire&quot; &quot;North Dakota&quot; #&gt; [5] &quot;Vermont&quot; Para contar quantos TRUE, a função sum retorna a soma das entradas de um vetor e força a conversão de vetores lógicos em números com TRUE codificado como 1 e FALSE como 0. Portanto, podemos contar os estados usando: sum(ind) #&gt; [1] 5 2.12.2 Operadores lógicos Suponha que gostemos de montanhas e que desejemos mudar para um estado seguro na região oeste do país. Queremos que a taxa de homicídios seja no máximo 1. Nesse caso, queremos que duas coisas diferentes sejam verdadeiras. Aqui podemos usar o operador lógico and, que em R é representado por &amp;. Esta operação resulta em TRUE somente quando ambas as lógicas são TRUE, isto é, certo. Para ver isso, considere este exemplo: TRUE &amp; TRUE #&gt; [1] TRUE TRUE &amp; FALSE #&gt; [1] FALSE FALSE &amp; FALSE #&gt; [1] FALSE Para o nosso exemplo, podemos formar duas lógicas: west &lt;- murders$region == &quot;West&quot; safe &lt;- murder_rate &lt;= 1 e podemos usar o &amp; para obter um vetor lógico que nos diz quais estados satisfazem ambas as condições: ind &lt;- safe &amp; west murders$state[ind] #&gt; [1] &quot;Hawaii&quot; &quot;Idaho&quot; &quot;Oregon&quot; &quot;Utah&quot; &quot;Wyoming&quot; 2.12.3 which Suponha que queremos ver a taxa de homicídios da Califórnia. Para esse tipo de operação, é conveniente converter vetores lógicos em índices em vez de manter vetores lógicos longos. A função which nos diz quais entradas para um vetor lógico são VERDADEIRAS. Então podemos escrever: ind &lt;- which(murders$state == &quot;California&quot;) murder_rate[ind] #&gt; [1] 3.37 2.12.4 match Se, em vez de um único estado, queremos descobrir as taxas de assassinatos de vários estados, digamos Nova York, Flórida e Texas, podemos usar a função match. Essa função nos diz quais índices de um segundo vetor correspondem a cada uma das entradas de um primeiro vetor: ind &lt;- match(c(&quot;New York&quot;, &quot;Florida&quot;, &quot;Texas&quot;), murders$state) ind #&gt; [1] 33 10 44 Agora podemos ver as taxas de homicídio: murder_rate[ind] #&gt; [1] 2.67 3.40 3.20 2.12.5 %in% Se, em vez de um índice, queremos que um lógico nos diga se cada elemento de um primeiro vetor está em um segundo vetor, podemos usar a função %in%. Vamos imaginar que você não tem certeza se Boston, Dakota e Washington são estados. Você pode descobrir assim: c(&quot;Boston&quot;, &quot;Dakota&quot;, &quot;Washington&quot;) %in% murders$state #&gt; [1] FALSE FALSE TRUE Lembre-se de que usaremos %in% frequentemente ao longo do livro. Avançado: existe uma conexão entre match e %in% através which. Para ver isso, observe que as duas linhas a seguir produzem o mesmo índice (embora em ordem diferente): match(c(&quot;New York&quot;, &quot;Florida&quot;, &quot;Texas&quot;), murders$state) #&gt; [1] 33 10 44 which(murders$state%in%c(&quot;New York&quot;, &quot;Florida&quot;, &quot;Texas&quot;)) #&gt; [1] 10 33 44 2.13 Exercícios Comece carregando o pacote e os dados. library(dslabs) data(murders) 1. Calcule a taxa de homicídios por 100.000 para cada estado e armazene-a em um objeto chamado murder_rate. Em seguida, use operadores lógicos para criar um vetor lógico chamado low que nos diz quais entradas de murder_rate são menores que 1. 2. Agora use os resultados do exercício anterior e a função which para determinar os índices de murder_rate associado a valores menores que 1. 3. Use os resultados do exercício anterior para indicar os nomes dos estados com taxas de homicídio menores que 1. 4. Agora estenda o código nos Exercícios 2 e 3 para indicar os estados do nordeste com taxas de homicídios menores que 1. Dica: Use o vetor lógico predefinido low e o operador lógico &amp;. 5. Em um exercício anterior, calculamos a taxa de homicídios de cada estado e a média desses números. Quantos estados estão abaixo da média? 6. Use a função match para identificar estados com as abreviações AK, MI e IA. Dica: Comece definindo um índice das entradas em murders$abb que correspondem às três abreviações. Então use o operador [ para extrair os estados. 7. Use o operador %in% para criar um vetor lógico que responda à pergunta: quais das seguintes são abreviações reais: MA, ME, MI, MO, MU? 8. Estenda o código usado no Exercício 7 para descobrir a única entrada que não é uma abreviação real. Dica: use o operador !, que converte FALSE para TRUE e vice-versa, e depois which para obter um índice. 2.14 Gráficos básicos No capítulo 7 descrevemos um pacote complementar que oferece uma abordagem poderosa para a produção de gráficos (plots em inglês) em R. Em seguida, temos uma parte completa, &amp;Quot;Visualização de dados&quot;, na qual oferecemos muitos exemplos. Aqui, descrevemos brevemente algumas das funções disponíveis em uma instalação básica do R. 2.14.1 plot A função plot pode ser usado para criar diagramas de dispersão (scatterplots em inglês). Aqui está um gráfico do total de assassinatos versus população. x &lt;- murders$population/ 10^6 y &lt;- murders$total plot(x, y) Para criar um gráfico rápido que não acessa variáveis duas vezes, podemos usar a função with: with(murders, plot(population, total)) A função with nos permite usar os nomes na coluna murders na função plot. Também funciona com qualquer data frame e qualquer função. 2.14.2 hist x &lt;- with(murders, total/ population * 100000) hist(x) Podemos ver que há uma grande variedade de valores, com a maioria entre 2 e 3 e um caso muito extremo com uma taxa de homicídios acima de 15: murders$state[which.max(x)] #&gt; [1] &quot;District of Columbia&quot; 2.14.3 boxplot Os diagramas de caixa (boxplots em inglês) também serão descritos na parte “Visualização de dados” do livro. Eles fornecem um resumo mais conciso do que os histogramas, mas são mais fáceis de empilhar com outros gráficos de caixa. Por exemplo, aqui podemos usá-los para comparar diferentes regiões: murders$rate &lt;- with(murders, total/ population * 100000) boxplot(rate~region, data = murders) Podemos ver que o Sul tem taxas mais altas de assassinatos do que as outras três regiões. 2.14.4 image A função image exibe os valores em uma matriz usando cores. Aqui está um exemplo rápido: x &lt;- matrix(1:120, 12, 10) image(x) 2.15 Exercícios 1. Fizemos um gráfico do total de assassinatos versus população e notamos um forte relacionamento. Não é de surpreender que os estados com populações maiores tenham sofrido mais assassinatos. library(dslabs) data(murders) population_in_millions &lt;- murders$population/10^6 total_gun_murders &lt;- murders$total plot(population_in_millions, total_gun_murders) Lembre-se de que muitos estados têm populações inferiores a 5 milhões e estão agrupados. Podemos obter mais informações criando esse gráfico na escala logarítmica. Transformar variáveis usando transformação log10 e depois crie um gráfico dos resultados. 2. Crie um histograma das populações do estado. 3. Gere boxplots de populações estaduais por região. https://rstudio.cloud↩ https://rafalab.github.io/dsbook/installing-r-rstudio.html↩ http://abcnews.go.com/blogs/headlines/2012/12/us-gun-ownership-homicide-rate-higher-than-other-developed-countries/↩ http://adv-r.had.co.nz/Style.html↩ "],
["conceitos-básicos-de-programação.html", "Capítulo 3 Conceitos básicos de programação 3.1 Expressões condicionais 3.2 Como definir funções 3.3 Namespaces 3.4 Loops para 3.5 Vectorização e funcional 3.6 Exercícios", " Capítulo 3 Conceitos básicos de programação Ensinamos R porque facilita muito a análise de dados, o tema principal deste livro. Ao codificar em R, podemos executar com eficiência análise exploratória de dados, criar canais de análise de dados e preparar a visualização de dados para comunicar resultados. No entanto, R não é apenas um ambiente de análise de dados, mas uma linguagem de programação. Programadores avançados de R podem desenvolver pacotes complexos e até melhorar o R, embora não abordemos a programação avançada neste livro. No entanto, nesta seção, apresentamos três conceitos-chave de programação: expressões condicionais, for-loops e funções. Estes não são apenas os principais componentes da programação avançada, mas às vezes são úteis durante a análise de dados. Também observamos que existem várias funções amplamente usadas para programação em R, mas que não discutiremos neste livro. Esses incluem split, cut, do.call e Reduce, bem como o pacote data.table. Vale a pena aprender como usá-los, se você quiser se tornar programador especialista em R. 3.1 Expressões condicionais Expressões condicionais são uma das características básicas da programação. Eles são usados para o que é chamado de flow control. A expressão condicional mais comum é a instrução if-else. Em R, podemos fazer muitas análises de dados sem condicionais. No entanto, eles aparecem ocasionalmente e você precisará deles assim que começar a escrever suas próprias funções e pacotes. Aqui está um exemplo muito simples que mostra a estrutura geral de uma instrução if-else. A idéia básica é imprimir o recíproco de a a menos que a deixe 0: a &lt;- 0 if(a!=0){ print(1/a) } else{ print(&quot;No reciprocal for 0.&quot;) } #&gt; [1] &quot;No reciprocal for 0.&quot; Vejamos outro exemplo usando o conjunto de dados de assassinatos nos EUA. EUA: library(dslabs) data(murders) murder_rate &lt;- murders$total/ murders$population*100000 Aqui está um exemplo muito simples que nos diz quais estados, se houver, têm uma taxa de homicídios inferior a 0,5 por 100.000. As declarações if eles nos protegem do caso em que nenhum estado satisfaz a condição. ind &lt;- which.min(murder_rate) if(murder_rate[ind] &lt; 0.5){ print(murders$state[ind]) } else{ print(&quot;No state has murder rate that low&quot;) } #&gt; [1] &quot;Vermont&quot; Se tentarmos novamente com uma taxa de 0,25, obteremos uma resposta diferente: if(murder_rate[ind] &lt; 0.25){ print(murders$state[ind]) } else{ print(&quot;No state has a murder rate that low.&quot;) } #&gt; [1] &quot;No state has a murder rate that low.&quot; Uma função relacionada que é muito útil é ifelse. Essa função usa três argumentos: um lógico e duas respostas possíveis. Se o lógico for TRUE, retorna o valor no segundo argumento e, se for FALSE, retorna o valor no terceiro argumento. Aqui está um exemplo: a &lt;- 0 ifelse(a &gt; 0, 1/a, NA) #&gt; [1] NA Essa função é particularmente útil porque é útil para vetores. Isso examina cada entrada do vetor lógico e retorna elementos do vetor fornecido no segundo argumento, se a entrada for TRUE, ou elementos do vetor fornecido no terceiro argumento, se a entrada for FALSE. a &lt;- c(0, 1, 2, -4, 5) result &lt;- ifelse(a &gt; 0, 1/a, NA) Esta tabela nos ajuda a ver o que aconteceu: a is_a_positive answer1 answer2 result 0 FALSE Inf NA NA 1 TRUE 1.00 NA 1.0 2 TRUE 0.50 NA 0.5 -4 FALSE -0.25 NA NA 5 TRUE 0.20 NA 0.2 Aqui está um exemplo de como essa função pode ser facilmente usada para substituir todos os valores ausentes em um vetor por zeros: data(na_example) no_nas &lt;- ifelse(is.na(na_example), 0, na_example) sum(is.na(no_nas)) #&gt; [1] 0 Duas outras funções úteis são any e all. A função any pega um vetor de lógicas e retorna TRUE se alguma das entradas for TRUE. A função all pega um vetor de lógicas e retorna TRUE se todas as entradas forem TRUE. Aqui está um exemplo: z &lt;- c(TRUE, TRUE, FALSE) any(z) #&gt; [1] TRUE all(z) #&gt; [1] FALSE 3.2 Como definir funções À medida que ganham mais experiência, eles precisam executar as mesmas operações repetidamente. Um exemplo simples é o cálculo de médias. Podemos calcular a média de um vetor x usando funções sum e length: sum(x)/length(x). Como fazemos isso repetidamente, é muito mais eficiente escrever uma função que execute essa operação. Essa operação específica é tão comum que alguém já escreveu a função mean e está incluído na base R. No entanto, você encontrará situações em que a função ainda não existe, portanto, R permite que você escreva uma. Você pode definir uma versão simples de uma função que calcula a média da seguinte maneira: avg &lt;- function(x){ s &lt;- sum(x) n &lt;- length(x) s/n } Agora avg é uma função que calcula a média: x &lt;- 1:100 identical(mean(x), avg(x)) #&gt; [1] TRUE Observe que as variáveis definidas em uma função não são salvas no espaço de trabalho. Então, enquanto usamos s e n quando chamamos (call em inglês) avg, os valores são criados e alterados apenas durante a chamada. Aqui podemos ver um exemplo ilustrativo: s &lt;- 3 avg(1:10) #&gt; [1] 5.5 s #&gt; [1] 3 Note como s ainda é 3 depois que ligamos avg. Em geral, funções são objetos, portanto, atribuímos nomes de variáveis a eles com &lt;-. A função function diz a R que ele está prestes a definir uma função. A forma geral da definição de uma função é assim: my_function &lt;- function(VARIABLE_NAME){ perform operations on VARIABLE_NAME and calculate VALUE VALUE } As funções que eles definem podem ter vários argumentos, bem como valores padrão. Por exemplo, podemos definir uma função que calcula a média aritmética ou geométrica, dependendo de uma variável definida pelo usuário como esta: avg &lt;- function(x, arithmetic = TRUE){ n &lt;- length(x) ifelse(arithmetic, sum(x)/n, prod(x)^(1/n)) } Aprenderemos mais sobre como criar funções através da experiência à medida que enfrentamos tarefas mais complexas. 3.3 Namespaces Quando eles começarem a se tornar usuários experientes do R, provavelmente precisarão carregar vários plug-ins de pacotes (add-ons) para algumas de suas varreduras. Assim que fazem isso, é provável que descubram que dois pacotes usam o mesmo nome para duas funções diferentes. E muitas vezes essas funções fazem coisas completamente diferentes. De fato, já vimos isso porque os pacotes base R __dplyr__estats definem uma função filter. Existem outros cinco exemplos em dplyr. Sabemos disso porque, quando carregamos dplyr pela primeira vez, vemos a seguinte mensagem: The following objects are masked from ‘package:stats’: filter, lag The following objects are masked from ‘package:base’: intersect, setdiff, setequal, union Então, o que R faz quando escrevemos filter? Você usa a função __dplyr__ou a funçãostats? Do nosso trabalho anterior, sabemos que ele usa dplyr. Mas e se quisermos usar stats? Essas funções vivem em diferentes namespaces. R seguirá uma determinada ordem ao procurar uma função nesses namespaces. Você pode ver o pedido escrevendo: search() A primeira entrada nesta lista é o ambiente global que inclui todos os objetos que eles definem. E daí se quisermos usar o filter stats em vez de filter __dplyr__masdplyr aparece primeiro na lista de pesquisa? Eles podem forçar o uso de um namespace específico usando dois pontos duplos ( ::) assim: stats::filter Se queremos ter certeza absoluta de que usamos o filter dplyr, podemos usar: dplyr::filter Lembre-se de que, se queremos usar uma função em um pacote sem carregar o pacote inteiro, também podemos usar dois pontos duplos. Para obter mais informações sobre esse tópico mais avançado, recomendamos o livro de pacotes R14. 3.4 Loops para A fórmula para a soma da série \\(1+2+\\dots+n\\) é \\(n(n+1)/2\\). E se não tivéssemos certeza de que essa era a função correta? Como poderíamos verificar? Usando o que aprendemos sobre funções, podemos criar um que calcule \\(S_n\\): compute_s_n &lt;- function(n){ x &lt;- 1:n sum(x) } Como podemos calcular \\(S_n\\) para vários valores de \\(n\\), digamos \\(n=1,\\dots,25\\)? Escrevemos 25 linhas de chamada de código compute_s_n? Não. É para isso que servem os loops de programação. Nesse caso, estamos realizando a mesma tarefa repetidamente, e a única coisa que está mudando é o valor de \\(n\\). Os loops for permitem definir o intervalo que nossa variável leva (no nosso exemplo \\(n=1,\\dots,10\\)), altere o valor e avalie a expressão enquanto você faz o loop _. Talvez o exemplo mais simples de um loop for seja esse código inútil: for(i in 1:5){ print(i) } #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 #&gt; [1] 4 #&gt; [1] 5 Aqui está o loop for que escreveríamos para o nosso exemplo \\(S_n\\): m &lt;- 25 s_n &lt;- vector(length = m) # create an empty vector for(n in 1:m){ s_n[n] &lt;- compute_s_n(n) } Em cada iteração \\(n=1\\), \\(n=2\\), etc …, calculamos \\(S_n\\) e mantemos na entrada \\(n\\) do s_n. Agora podemos criar um gráfico para procurar um padrão: n &lt;- 1:m plot(n, s_n) Se você percebeu que parece quadrático, está no caminho certo porque a fórmula é \\(n(n+1)/2\\). 3.5 Vectorização e funcional Embora os for-loops sejam um conceito importante para entender, eles não são muito usados em R. À medida que aprendem mais R, eles descobrirão que vectoring é preferível aos for-loops, pois resulta em um código mais curto e claro. . Já vimos exemplos na seção aritmética vetorial. Uma função vectorized é uma função que aplicará a mesma operação a cada um dos vetores. x &lt;- 1:10 sqrt(x) #&gt; [1] 1.00 1.41 1.73 2.00 2.24 2.45 2.65 2.83 3.00 3.16 y &lt;- 1:10 x*y #&gt; [1] 1 4 9 16 25 36 49 64 81 100 Para fazer esse cálculo, não precisamos de loops de for. No entanto, nem todas as funções funcionam dessa maneira. Por exemplo, a função que acabamos de escrever, compute_s_n, ele não funciona elemento a elemento, pois espera um escalar. Esse trecho de código não executa a função em todas as entradas de n: n &lt;- 1:25 compute_s_n(n) As functionals são funções que nos ajudam a aplicar a mesma função a cada entrada em um vetor, matriz, data frame ou lista. Aqui abordamos o funcional que opera em vetores numéricos, lógicos e de caracteres: sapply. A função sapply nos permite executar operações baseadas em elementos (element-wise em inglês) em qualquer função. Aqui podemos ver como funciona: x &lt;- 1:10 sapply(x, sqrt) #&gt; [1] 1.00 1.41 1.73 2.00 2.24 2.45 2.65 2.83 3.00 3.16 Cada elemento de x é passado para a função sqrt e retorna o resultado. Esses resultados são concatenados. Nesse caso, o resultado é um vetor do mesmo comprimento que o original, x. Isso implica que o loop for acima pode ser escrito da seguinte maneira: n &lt;- 1:25 s_n &lt;- sapply(n, compute_s_n) Outros funcionais são apply, lapply, tapply, mapply, vapply e replicate. Nós usamos principalmente sapply, apply e replicate neste livro, mas recomendamos que se conheçam, pois podem ser muito úteis. 3.6 Exercícios 1. O que essa expressão condicional retornará? x &lt;- c(1,2,-3,4) if(all(x&gt;0)){ print(&quot;All Postives&quot;) } else{ print(&quot;Not all positives&quot;) } 2. Qual das seguintes expressões é sempre FALSE quando pelo menos uma entrada de um vetor lógico x é verdade? para. all(x) b. any(x) c. any(!x) d. all(!x) 3. A função nchar informa quantos caracteres um vetor de caracteres possui. Escreva uma linha de código que atribua o objeto new_names a abreviação de estado quando o nome do estado tiver mais de 8 caracteres. 4. Crie uma função sum_n que, por qualquer valor, digamos \\(n\\), calcule a soma dos números inteiros de 1 a n (inclusive). Use a função para determinar a soma dos números inteiros de 1 a 5.000. 5. Crie uma função altman_plot que leva dois argumentos, x e y e faça um gráfico da diferença em relação à soma. 6. Depois de executar o código a seguir, qual é o valor de x? x &lt;- 3 my_func &lt;- function(y){ x &lt;- 5 y+5 } 7. Escreva uma função compute_s_n isso para qualquer \\(n\\) calcular a soma \\(S_n = 1^2 + 2^2 + 3^2 + \\dots n^2\\). Indique o valor da soma quando \\(n=10\\). 8. Definir um vetor de número vazio s_n tamanho 25 usando s_n &lt;- vector(&quot;numeric&quot;, 25) e armazene os resultados de \\(S_1, S_2, \\dots S_{25}\\) usando um loop for. 9. Repita o exercício 8, mas desta vez use sapply. 10. Repita o exercício 8, mas desta vez use map_dbl. Onze. Gráfico \\(S_n\\) versus \\(n\\). Use pontos definidos por \\(n=1,\\dots,25\\). 12. Confirme se a fórmula para esta soma é \\(S_n= n(n+1)(2n+1)/6\\). http://r-pkgs.had.co.nz/namespace.html↩ "],
["tidyverse.html", "Capítulo 4 tidyverse 4.1 Data tidy 4.2 Exercícios 4.3 Como manipular data frames 4.4 Exercícios 4.5 O cano_: %&gt;% 4.6 Exercícios 4.7 Como resumir dados 4.8 Como encomendar os data frames 4.9 Exercícios 4.10 Tibbles 4.11 O operador de ponto 4.12 do 4.13 O pacote purrr 4.14 Os condicionais tidyverse 4.15 Exercícios", " Capítulo 4 tidyverse Até agora, manipulamos vetores reorganizando-os e criando subconjuntos indexando. No entanto, quando iniciamos as análises mais avançadas, a unidade preferida para armazenamento de dados não é o vetor, mas o data frame. Neste capítulo, aprenderemos a trabalhar diretamente com data frames, o que facilita muito a organização da informação. Usaremos data frames para a maior parte deste livro. Vamos nos concentrar em um formato de dados específico chamado tidy_e uma coleção específica de pacotes que são particularmente úteis para trabalhar com dados_tidy_chamados_tidyverse. Podemos carregar todos os pacotes _tidyverse_de uma só vez instalando e carregando o pacotetidyverse: library(tidyverse) Aprenderemos como implementar a abordagem tidyverse ao longo do livro, mas antes de nos aprofundarmos nos detalhes, neste capítulo, apresentaremos alguns dos aspectos mais comuns do tidyverse, começando com o pacote __dplyr__para manipular os quadros de dados e o pacotepurrr para trabalhar com As funções. Observe que o tidyverse também inclui um pacote gráfico, ggplot2, que apresentaremos mais adiante neste capítulo. 7 na parte “Visualização de dados” do livro; o pacote readr discutido no capítulo 5; e muitos outros. Neste capítulo, primeiro apresentamos o conceito de dados tidy_e depois demonstramos como usamos_tidyverse_para trabalhar com_data frames neste formato. 4.1 Data tidy Dizemos que uma tabela de dados está no formato tidy se cada linha representa uma observação e as colunas representam as diferentes variáveis disponíveis para cada uma dessas observações. O conjunto de dados murders é um exemplo de um tidy data frame. #&gt; state abb region population total #&gt; 1 Alabama AL South 4779736 135 #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 #&gt; 4 Arkansas AR South 2915918 93 #&gt; 5 California CA West 37253956 1257 #&gt; 6 Colorado CO West 5029196 65 Cada linha representa um estado com cada uma das cinco colunas fornecendo uma variável diferente relacionada a esses estados: nome, abreviação, região, população e número total de assassinatos. Para ver como as mesmas informações podem ser fornecidas em diferentes formatos, considere o seguinte exemplo: #&gt; country year fertility #&gt; 1 Germany 1960 2.41 #&gt; 2 South Korea 1960 6.16 #&gt; 3 Germany 1961 2.44 #&gt; 4 South Korea 1961 5.99 #&gt; 5 Germany 1962 2.47 #&gt; 6 South Korea 1962 5.79 Este conjunto de dados tidy oferece taxas de fertilidade para dois países ao longo dos anos. Um conjunto de dados arrumado é considerado porque cada linha apresenta uma observação com as três variáveis: país, ano e taxa de fertilidade. No entanto, esse conjunto de dados originalmente veio em um formato diferente e o remodelamos para distribuição através do pacote dslabs. Originalmente, os dados estavam no seguinte formato: #&gt; country 1960 1961 1962 #&gt; 1 Germany 2.41 2.44 2.47 #&gt; 2 South Korea 6.16 5.99 5.79 A mesma informação é fornecida, mas há duas diferenças importantes no formato: 1) cada linha inclui várias observações e 2) uma das variáveis, ano, é armazenada no cabeçalho. Para que os pacotes tidyverse sejam utilizados da melhor maneira, precisamos alterar a forma dos dados para que estejam no formato tidy, que eles aprenderão na seção “Wrangling data” do livro. Até lá, usaremos exemplos de conjuntos de dados que já estão no formato tidy. Embora não seja imediatamente óbvio, ao longo do livro, você começará a apreciar os benefícios de trabalhar em uma estrutura em que as funções usam os formatos tidy_para_inputs_e_outputs. Você verá como isso permite que os analistas de dados se concentrem nos aspectos mais importantes da análise, e não no formato dos dados. 4.2 Exercícios 1. Examine o conjunto de dados incluído na base R co2. Qual dos seguintes é verdadeiro? para. co2 tidy data: possui um ano para cada linha. b. co2 not tidy: precisamos de pelo menos uma coluna com um vetor de caracteres. c. co2 não é tidy: é uma matriz em vez de um data frame. d. co2 não é arrumado: para arrumar, teríamos que mudar a forma (em inglês) para ter três colunas (ano, mês e valor), e então cada observação de CO2 teria uma linha. 2. Examine o conjunto de dados incluído na base R ChickWeight. Qual dos seguintes é verdadeiro? para. ChickWeight não tidy: cada pintinho tem mais de uma linha. b. ChickWeight is_tidy_: cada observação (um peso) é representada por uma linha. O pintinho de onde essa medição veio é uma das variáveis. c. ChickWeight não é arrumado: estamos perdendo a coluna do ano. d. ChickWeight é tidy: é armazenado em um data frame. 3. Examine o conjunto de dados predefinido BOD. Qual dos seguintes é verdadeiro? para. BOD não é arrumado: tem apenas seis linhas. b. BOD not tidy: a primeira coluna é apenas um índice. c. BOD é tidy: cada linha é uma observação com dois valores (tempo e demanda) d. BOD é tidy: todos os conjuntos de dados pequenos são tidy por definição. 4. Qual dos seguintes conjuntos de dados internos é tidy? Você pode escolher mais de um. para. BJsales b. EuStockMarkets c. DNase d. Formaldehyde e. Orange f. UCBAdmissions 4.3 Como manipular data frames O pacote __dplyr_do_tidyverse_oferece funções que executam algumas das operações mais comuns ao trabalhar com_data frames e usa nomes para essas funções que são relativamente fáceis de lembrar. Por exemplo, para alterar a tabela de dados adicionando uma nova coluna, usamos mutate. Para filtrar a tabela de dados para um subconjunto de linhas, usamos filter. Por fim, para subdividir os dados selecionando colunas específicas, usamos select. 4.3.1 Como adicionar uma coluna com mutate Queremos que todas as informações necessárias para nossa análise sejam incluídas na tabela de dados. Portanto, a primeira tarefa é adicionar as taxas de assassinato ao nosso quadro de dados de assassinatos. A função mutate pegue data frame como primeiro argumento e nome da variável e valores como segundo argumento usando a convenção name = values. Então, para adicionar taxas de assassinatos, usamos: library(dslabs) data(&quot;murders&quot;) murders &lt;- mutate(murders, rate = total/ population * 100000) Lembre-se que aqui usamos total e population dentro da função, que são objetos não definidos em nossa área de trabalho. Mas por que não recebemos um erro? Este é um dos principais recursos do dplyr. As funções neste pacote, como mutate eles sabem como procurar variáveis no data frame que o primeiro argumento fornece. Na chamada para mutate que vemos acima, total terá os valores de murders$total. Essa abordagem torna o código muito mais legível. Podemos ver que a nova coluna foi adicionada: head(murders) #&gt; state abb region population total rate #&gt; 1 Alabama AL South 4779736 135 2.82 #&gt; 2 Alaska AK West 710231 19 2.68 #&gt; 3 Arizona AZ West 6392017 232 3.63 #&gt; 4 Arkansas AR South 2915918 93 3.19 #&gt; 5 California CA West 37253956 1257 3.37 #&gt; 6 Colorado CO West 5029196 65 1.29 Embora tenhamos substituído o objeto original murders, isso não altera o objeto que foi carregado com data(murders). Se carregarmos os dados murders novamente, o original substituirá nossa versão mutada. 4.3.2 Como criar subconjuntos com filter Agora, suponha que desejamos filtrar a tabela de dados para mostrar apenas as entradas para as quais a taxa de homicídios é menor que 0,71. Para fazer isso, usamos a função filter, que usa a tabela de dados como o primeiro argumento e, em seguida, a instrução condicional como o segundo. O mesmo que com mutate, podemos usar nomes de variáveis sem aspas de murders dentro da função e ele saberá que estamos nos referindo às colunas e não aos objetos na área de trabalho. filter(murders, rate &lt;= 0.71) #&gt; state abb region population total rate #&gt; 1 Hawaii HI West 1360301 7 0.515 #&gt; 2 Iowa IA North Central 3046355 21 0.689 #&gt; 3 New Hampshire NH Northeast 1316470 5 0.380 #&gt; 4 North Dakota ND North Central 672591 4 0.595 #&gt; 5 Vermont VT Northeast 625741 2 0.320 4.3.3 Como selecionar colunas com select Embora nossa tabela de dados tenha apenas seis colunas, algumas tabelas de dados incluem centenas. Se queremos ver apenas algumas colunas, podemos usar a função select de dplyr. No código a seguir, selecionamos três, atribuímos o resultado a um novo objeto e filtramos esse novo objeto: new_table &lt;- select(murders, state, region, rate) filter(new_table, rate &lt;= 0.71) #&gt; state region rate #&gt; 1 Hawaii West 0.515 #&gt; 2 Iowa North Central 0.689 #&gt; 3 New Hampshire Northeast 0.380 #&gt; 4 North Dakota North Central 0.595 #&gt; 5 Vermont Northeast 0.320 Na chamada para select, o primeiro argumento murders é um objeto mas state, region e rate eles são nomes de variáveis. 4.4 Exercícios 1. Carregue o pacote dplyr e o conjunto de dados de assassinato nos EUA. library(dplyr) library(dslabs) data(murders) Você pode adicionar colunas usando a função mutate de dplyr. Esta função reconhece os nomes das colunas e, dentro da função, você pode chamá-los sem aspas: murders &lt;- mutate(murders, population_in_millions = population/ 10^6) Nós podemos escrever population em vez de murders$population. A função mutate sabemos que estamos agarrando colunas de murders. Usar função mutate para adicionar uma coluna de assassinato chamada rate com a taxa de homicídios por 100.000, como no código do exemplo acima. Certifique-se de redefinir murders como foi feito no código do exemplo anterior (assassinatos &lt;- [seu código]) para que possamos continuar usando essa variável. 2. Sim rank(x) fornece o alcance das entradas x do menor para o maior, rank(-x) fornece os intervalos do maior para o menor. Usar função mutate adicionar uma coluna rank a contendo a faixa de taxa de homicídios da maior para a menor. Certifique-se de redefinir murders para continuar usando esta variável. 3. Com dplyr, podemos usar select para mostrar apenas determinadas colunas. Por exemplo, com este código, mostraríamos apenas os estados e tamanhos da população: select(murders, state, population) %&gt;% head() Usar select para exibir nomes e abreviações de estado em murders. Não redefina murders, apenas mostre os resultados. 4. A função filter __dplyr_é usado para escolher linhas específicas do_data frame para salvar. A diferença de select que é para colunas, filter é para linhas. Por exemplo, você pode exibir apenas a linha de Nova York da seguinte maneira: filter(murders, state == &quot;New York&quot;) Você pode usar outros vetores lógicos para filtrar linhas. Usar filter para mostrar os cinco estados com as maiores taxas de homicídio. Depois de adicionar a taxa e o intervalo de assassinatos, não altere o conjunto de dados de assassinatos nos EUA. EUA, apenas mostre o resultado. Lembre-se de que você pode filtrar com base na coluna rank. 5. Podemos excluir linhas usando o operador !=. Por exemplo, para remover a Flórida, faríamos o seguinte: no_florida &lt;- filter(murders, state != &quot;Florida&quot;) Crie um novo data frame com o nome no_south isso elimina os estados da região sul. Quantos estados há nesta categoria? Você pode usar a função nrow para isto. 6. Também podemos usar %in% para filtrar com dplyr. Portanto, você pode visualizar os dados de Nova York e Texas como este: filter(murders, state %in% c(&quot;New York&quot;, &quot;Texas&quot;)) Crie um novo data frame chamado murders_nw apenas com os estados nordeste e oeste. Quantos estados há nesta categoria? 7. Suponha que você queira morar no nordeste ou oeste e que a taxa de homicídios seja menor que 1. Queremos ver os dados dos estados que satisfazem essas opções. Observe que você pode usar operadores lógicos com filter. Aqui está um exemplo em que filtramos para manter apenas pequenos estados na região nordeste. filter(murders, population &lt; 5000000 &amp; region == &quot;Northeast&quot;) Certifique-se de que murders foi definido com rate e rank e ainda tem todos os estados. Crie uma tabela chamada my_states contendo linhas para estados que satisfazem ambas as condições: é no nordeste ou oeste e a taxa de homicídios é menor que 1. select para exibir apenas o nome, a taxa e o intervalo do estado. 4.5 O cano_: %&gt;% Com dplyr, podemos executar uma série de operações, por exemplo select e então filter, enviando os resultados de uma função para outra usando o que é chamado de pipe operator: %&gt;%. Alguns detalhes estão incluídos abaixo. Escrevemos o código acima para mostrar três variáveis (estado, região, taxa) para estados com taxas de homicídio abaixo de 0,71. Para isso, definimos o objeto intermediário new_table. Em dplyr, podemos escrever um código mais parecido com uma descrição do que queremos fazer sem objetos intermediários: \\[ \\mbox {original data } \\rightarrow \\mbox { select } \\rightarrow \\mbox { filter } \\] Para essa operação, podemos usar o pipe %&gt;%. O código fica assim: murders %&gt;% select(state, region, rate) %&gt;% filter(rate &lt;= 0.71) #&gt; state region rate #&gt; 1 Hawaii West 0.515 #&gt; 2 Iowa North Central 0.689 #&gt; 3 New Hampshire Northeast 0.380 #&gt; 4 North Dakota North Central 0.595 #&gt; 5 Vermont Northeast 0.320 Essa linha de código é equivalente às duas linhas de código anteriores. O que está acontecendo aqui? Em geral, o pipe_envia o resultado que está no lado esquerdo do_pipe_para ser o primeiro argumento da função no lado direito do_pipe. Aqui está um exemplo simples: 16 %&gt;% sqrt() #&gt; [1] 4 Podemos continuar canalizando valores (piping em inglês) ao longo de: 16 %&gt;% sqrt() %&gt;% log2() #&gt; [1] 2 A declaração acima é equivalente a log2(sqrt(16)). Lembre-se de que o pipe envia valores para o primeiro argumento, para que possamos definir outros argumentos como se o primeiro argumento já estivesse definido: 16 %&gt;% sqrt() %&gt;% log(base = 2) #&gt; [1] 2 Portanto, ao usar _pipe_com_data frames_edplyr, não precisamos mais especificar o primeiro argumento necessário, pois as funções dplyr que descrevemos pegam todos os dados como o primeiro argumento. No código que escrevemos: murders %&gt;% select(state, region, rate) %&gt;% filter(rate &lt;= 0.71) murders é o primeiro argumento da função select e o novo data frame (anteriormente new_table) é o primeiro argumento da função filter. Observe que o pipe funciona bem com funções em que o primeiro argumento são os dados de entrada. As funções nos pacotes __tidyverse__e__dplyr_têm esse formato e podem ser facilmente usadas com o_pipe. 4.6 Exercícios 1. O cano_ %&gt;% ele pode ser usado para executar operações sequencialmente sem precisar definir objetos intermediários. Comece redefinindo murders para incluir a taxa e o intervalo. murders &lt;- mutate(murders, rate = total/ population * 100000, rank = rank(-rate)) Na solução do exercício anterior, fizemos o seguinte: my_states &lt;- filter(murders, region %in% c(&quot;Northeast&quot;, &quot;West&quot;) &amp; rate &lt; 1) select(my_states, state, rate, rank) O cano_ %&gt;% nos permite executar as duas operações sequencialmente sem precisar definir uma variável intermediária my_states. Então, poderíamos ter mudado e selecionado na mesma linha assim: mutate(murders, rate = total/ population * 100000, rank = rank(-rate)) %&gt;% select(state, rate, rank) Eu sinto isso select ele não possui mais um data frame como seu primeiro argumento. O primeiro argumento é assumido como o resultado da operação realizada imediatamente antes %&gt;%. Repita o exercício anterior, mas agora, em vez de criar um novo objeto, mostre o resultado e inclua apenas as colunas de status, velocidade e intervalo. Use um pipe %&gt;% para fazer isso em uma linha. 2. Reiniciar murders para a tabela original usando data(murders). Use um pipe_para criar um novo_data frame chamado my_states ele considera apenas os estados do nordeste ou oeste que têm uma taxa de homicídios menor que 1 e contém apenas as colunas de estado, taxa e faixa. O pipe também deve ter quatro componentes separados por três %&gt;%. O código deve se parecer com o seguinte: my_states &lt;- murders %&gt;% mutate SOMETHING %&gt;% filter SOMETHING %&gt;% select SOMETHING 4.7 Como resumir dados Uma parte importante da análise exploratória de dados é resumir os dados. A média e o desvio padrão são dois exemplos de estatísticas resumidas amplamente usadas. É possível obter resumos mais informativos, dividindo primeiro os dados em grupos. Nesta seção, abordamos dois novos verbos dplyr que facilitam esses cálculos: summarize e group_by. Aprendemos a acessar os valores resultantes usando a função pull. 4.7.1 summarize A função summarize o de dplyr oferece uma maneira de calcular estatísticas resumidas com código intuitivo e legível. Começamos com um exemplo simples baseado em alturas. O conjunto de dados heights inclui as alturas e o sexo relatados pelos alunos em uma pesquisa de classe. library(dplyr) library(dslabs) data(heights) O código a seguir calcula a média e o desvio padrão para mulheres: s &lt;- heights %&gt;% filter(sex == &quot;Female&quot;) %&gt;% summarize(average = mean(height), standard_deviation = sd(height)) s #&gt; average standard_deviation #&gt; 1 64.9 3.76 Isso leva nossa tabela de dados original como entrada, a filtra para incluir apenas as linhas que representam as fêmeas e, em seguida, produz uma nova tabela de resumo com apenas a média e o desvio padrão das alturas. Podemos escolher os nomes das colunas da tabela resultante. Por exemplo, acima, decidimos usar average e standard_deviation, mas poderíamos ter usado outros nomes da mesma maneira. Como a tabela resultante armazenada em s é um data frame, podemos acessar os componentes com o operador de acesso $: s$average #&gt; [1] 64.9 s$standard_deviation #&gt; [1] 3.76 Como na maioria das outras funções dplyr, summarize conhece os nomes das variáveis e podemos usá-las diretamente. Então, quando escrevemos mean(height) dentro da chamada de função summarize, a função acessa a coluna denominada “height” ou height e calcula a média do vetor numérico resultante. Podemos calcular qualquer outro resumo que opere em vetores e retorne um único valor. Por exemplo, podemos adicionar as medianas, alturas mínima e máxima desta maneira: heights %&gt;% filter(sex == &quot;Female&quot;) %&gt;% summarize(median = median(height), minimum = min(height), maximum = max(height)) #&gt; median minimum maximum #&gt; 1 65 51 79 Podemos obter esses três valores com apenas uma linha usando a função quantile: por exemplo, quantile(x, c(0,0.5,1)) retorna o mínimo (percentil 0), mediana (percentil 50) e máximo (percentil 100) do vetor x. No entanto, se tentarmos usar uma função como essa que retorne dois ou mais valores dentro summarize: heights %&gt;% filter(sex == &quot;Female&quot;) %&gt;% summarize(range = quantile(height, c(0, 0.5, 1))) receberemos um erro: Error: expecting result of length one, got : 2. Com função summarize, podemos chamar apenas funções que retornam um único valor. Na seção 4.12 vamos aprender a lidar com funções que retornam mais de um valor. Para outro exemplo de como podemos usar a função summarize vamos calcular a taxa média de homicídios nos Estados Unidos. Lembre-se de que nossa tabela de dados inclui o total de assassinatos e o tamanho da população de cada estado e já usamos dplyr para adicionar uma coluna de taxa de assassinatos: murders &lt;- murders %&gt;% mutate(rate = total/population*100000) Lembre-se que a taxa de homicídios nos EUA EUA não é a média das taxas de homicídio do estado: summarize(murders, mean(rate)) #&gt; mean(rate) #&gt; 1 2.78 Isso ocorre porque no cálculo anterior, estados pequenos têm o mesmo peso que estados grandes. A taxa de assassinatos nos Estados Unidos é o número total de assassinatos nos Estados Unidos dividido pela população total. Portanto, o cálculo correto é: us_murder_rate &lt;- murders %&gt;% summarize(rate = sum(total)/ sum(population) * 100000) us_murder_rate #&gt; rate #&gt; 1 3.03 Este cálculo conta estados maiores proporcionalmente ao seu tamanho, resultando em um valor maior. 4.7.2 pull O objeto us_murder_rate definido acima representa apenas um número. No entanto, estamos armazenando-o em um data frame: class(us_murder_rate) #&gt; [1] &quot;data.frame&quot; desde que, como a maioria das funções dplyr, summarize sempre retorna um data frame. Isso pode ser problemático se quisermos usar esse resultado com funções que requerem um valor numérico. Aqui está um truque útil para acessar os valores armazenados nos dados quando usamos pipes: quando um objeto de dados é canalizado (_ é canalizado em inglês), esse objeto e suas colunas podem ser acessados usando a função pull. Para entender o que queremos dizer, considere esta linha de código: us_murder_rate %&gt;% pull(rate) #&gt; [1] 3.03 Isso retorna o valor na coluna rate do us_murder_rate tornando-o equivalente a us_murder_rate$rate. Para obter um número da tabela de dados original com uma linha de código, podemos escrever: us_murder_rate &lt;- murders %&gt;% summarize(rate = sum(total)/ sum(population) * 100000) %&gt;% pull(rate) us_murder_rate #&gt; [1] 3.03 que agora é numérico: class(us_murder_rate) #&gt; [1] &quot;numeric&quot; 4.7.3 Como agrupar e resumir com group_by Uma operação comum na exploração de dados é primeiro dividir os dados em grupos e depois calcular resumos para cada grupo. Por exemplo, podemos querer calcular a média e o desvio padrão para as alturas de homens e mulheres separadamente. A função group_by nos ajuda a fazer isso. Se escrevermos isso: heights %&gt;% group_by(sex) #&gt; # A tibble: 1,050 x 2 #&gt; # Groups: sex [2] #&gt; sex height #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Male 75 #&gt; 2 Male 70 #&gt; 3 Male 68 #&gt; 4 Male 74 #&gt; 5 Male 61 #&gt; # … with 1,045 more rows O resultado não parece muito diferente de heights exceto que vemos Groups: sex [2] quando imprimimos o objeto. Embora não seja imediatamente óbvio a partir de sua aparência, agora é um data frame_especial chamado data de dados agrupados, e as funções de dplyr, em particular summarize, eles se comportarão de maneira diferente quando agirem sobre esse objeto. Conceitualmente, eles podem pensar nessa tabela como muitas tabelas, com as mesmas colunas, mas não necessariamente com o mesmo número de linhas, empilhadas em um objeto. Quando resumimos os dados após o agrupamento, é o que acontece: heights %&gt;% group_by(sex) %&gt;% summarize(average = mean(height), standard_deviation = sd(height)) #&gt; # A tibble: 2 x 3 #&gt; sex average standard_deviation #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Female 64.9 3.76 #&gt; 2 Male 69.3 3.61 A função summarize aplique o resumo a cada grupo separadamente. Para ver outro exemplo, vamos calcular a taxa média de homicídios nas quatro regiões do país: murders %&gt;% group_by(region) %&gt;% summarize(median_rate = median(rate)) #&gt; # A tibble: 4 x 2 #&gt; region median_rate #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Northeast 1.80 #&gt; 2 South 3.40 #&gt; 3 North Central 1.97 #&gt; 4 West 1.29 4.8 Como encomendar os data frames Ao examinar um conjunto de dados, geralmente é conveniente classificar numericamente ou alfabeticamente, com base em uma ou mais das colunas da tabela. Conhecemos as funções order e sort, mas para classificar tabelas inteiras, a função arrange dplyr é útil. Por exemplo, aqui ordenamos os estados de acordo com o tamanho da população: murders %&gt;% arrange(population) %&gt;% head() #&gt; state abb region population total rate #&gt; 1 Wyoming WY West 563626 5 0.887 #&gt; 2 District of Columbia DC South 601723 99 16.453 #&gt; 3 Vermont VT Northeast 625741 2 0.320 #&gt; 4 North Dakota ND North Central 672591 4 0.595 #&gt; 5 Alaska AK West 710231 19 2.675 #&gt; 6 South Dakota SD North Central 814180 8 0.983 Com arrange podemos decidir qual coluna usar para solicitar. Para ver os estados por população, do menor para o maior, organizamos pela rate : murders %&gt;% arrange(rate) %&gt;% head() #&gt; state abb region population total rate #&gt; 1 Vermont VT Northeast 625741 2 0.320 #&gt; 2 New Hampshire NH Northeast 1316470 5 0.380 #&gt; 3 Hawaii HI West 1360301 7 0.515 #&gt; 4 North Dakota ND North Central 672591 4 0.595 #&gt; 5 Iowa IA North Central 3046355 21 0.689 #&gt; 6 Idaho ID West 1567582 12 0.766 Observe que o comportamento padrão é classificar em ordem crescente. Em dplyr, a função desc transformar um vetor para que fique em ordem decrescente. Para classificar a tabela em ordem decrescente, podemos escrever: murders %&gt;% arrange(desc(rate)) 4.8.1 Como encomendar aninhado Se estivermos ordenando uma coluna quando houver empates, podemos usar uma segunda coluna para quebrar o empate. Da mesma forma, uma terceira coluna pode ser usada para romper os laços entre a primeira e a segunda, e assim por diante. Aqui nós ordenamos por region, na região, ordenamos por taxa de homicídio: murders %&gt;% arrange(region, rate) %&gt;% head() #&gt; state abb region population total rate #&gt; 1 Vermont VT Northeast 625741 2 0.320 #&gt; 2 New Hampshire NH Northeast 1316470 5 0.380 #&gt; 3 Maine ME Northeast 1328361 11 0.828 #&gt; 4 Rhode Island RI Northeast 1052567 16 1.520 #&gt; 5 Massachusetts MA Northeast 6547629 118 1.802 #&gt; 6 New York NY Northeast 19378102 517 2.668 4.8.2 Os primeiros \\(n\\) No código acima, usamos a função head para impedir que a página seja preenchida com todo o conjunto de dados. Se queremos ver uma proporção maior, podemos usar a função top_n. Essa função usa um data frame como o primeiro argumento, o número de linhas a serem exibidas no segundo e a variável a ser filtrada no terceiro. Aqui está um exemplo de como visualizar as 5 principais linhas: murders %&gt;% top_n(5, rate) #&gt; state abb region population total rate #&gt; 1 District of Columbia DC South 601723 99 16.45 #&gt; 2 Louisiana LA South 4533372 351 7.74 #&gt; 3 Maryland MD South 5773552 293 5.07 #&gt; 4 Missouri MO North Central 5988927 321 5.36 #&gt; 5 South Carolina SC South 4625364 207 4.48 Observe que as linhas não são ordenadas por rate, apenas filtrado. Se quisermos pedir, precisamos usar arrange. Lembre-se de que, se o terceiro argumento for deixado em branco, top_n filtrar pela última coluna. 4.9 Exercícios Para esses exercícios, usaremos os dados da pesquisa coletados pelo Centro Nacional de Estatísticas da Saúde dos Estados Unidos (NCHS). Este centro realiza uma série de pesquisas em saúde e nutrição desde a década de 1960. Desde 1999, cerca de 5.000 indivíduos de todas as idades foram entrevistadas a cada ano e concluíram o componente de triagem de saúde da pesquisa. Alguns dos dados estão disponíveis no pacote NHANES. Depois de instalar o pacote NHANES, eles podem carregar os dados da seguinte maneira: library(NHANES) data(NHANES) Os dados NHANES têm muitos valores ausentes. As funções mean e sd retornará NA se alguma das entradas do vetor de entrada for uma NA. Aqui está um exemplo: library(dslabs) data(na_example) mean(na_example) #&gt; [1] NA sd(na_example) #&gt; [1] NA Para ignorar o NA s podemos usar o argumento na.rm: mean(na_example, na.rm = TRUE) #&gt; [1] 2.3 sd(na_example, na.rm = TRUE) #&gt; [1] 1.22 Vamos agora explorar os dados NHANES. 1. Oferecemos algumas informações básicas sobre pressão arterial. Primeiro, vamos selecionar um grupo para definir o padrão. Usaremos mulheres de 20 a 29 anos. AgeDecade é uma variável categórica com essas idades. Observe que a categoria está codificada “20-29”, com um espaço na frente! Qual é a média e desvio padrão da pressão arterial sistólica, conforme armazenado na variável BPSysAve? Salve-o em uma variável chamada ref. Dica: use filter e summarize e use o argumento na.rm = TRUE ao calcular a média e o desvio padrão. Você também pode filtrar valores de NA usando filter. 2. Usando um pipe, atribua a média a uma variável numérica ref_avg. Dica: use o código semelhante ao acima e depois pull. 3. Agora insira os valores mínimo e máximo para o mesmo grupo. 4. Calcule a média e o desvio padrão para as mulheres, mas para cada faixa etária separadamente, em vez de uma década selecionada, como na pergunta 1. Observe que as faixas etárias são definidas por AgeDecade. Dica: em vez de filtrar por idade e sexo, filtre por Gender e depois use group_by. 5. Repita o exercício 4 para os meninos. 6. Podemos combinar os dois resumos dos exercícios 4 e 5 em uma linha de código. Isto é porque group_by permite agrupar por mais de uma variável. Obtenha uma excelente tabela de resumo usando group_by(AgeDecade, Gender). 7. Para homens entre 40 e 49 anos, compare a pressão arterial sistólica por raça, conforme aparece na variável Race1. Encomende a tabela resultante com base na pressão arterial sistólica média mais baixa para a mais alta. 4.10 Tibbles Os dados tidy_devem ser armazenados em_data frames. Discutimos o data frame na Seção 2.3.1 e estamos usando o data frame murders ao longo do livro. Na seção 4.7.3 nós introduzimos a função group_by, que permite estratificar os dados antes de calcular as estatísticas de resumo. Mas onde estão as informações do grupo armazenadas no data frame? murders %&gt;% group_by(region) #&gt; # A tibble: 51 x 6 #&gt; # Groups: region [4] #&gt; state abb region population total rate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Alabama AL South 4779736 135 2.82 #&gt; 2 Alaska AK West 710231 19 2.68 #&gt; 3 Arizona AZ West 6392017 232 3.63 #&gt; 4 Arkansas AR South 2915918 93 3.19 #&gt; 5 California CA West 37253956 1257 3.37 #&gt; # … with 46 more rows Observe que não há colunas com essas informações. Mas se você olhar para a saída acima, verá a linha A tibble seguido por dimensões. Podemos aprender a classe do objeto retornado usando: murders %&gt;% group_by(region) %&gt;% class() #&gt; [1] &quot;grouped_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; O tbl é um tipo especial de data frame. As funções group_by e summarize sempre retorne esse tipo de data frame. A função group_by retorna um tipo especial de tbl, a grouped_df. Discutiremos isso mais tarde. Para consistência, os verbos de manipulação dplyr ( select, filter, mutate e arrange) preserva a classe input: se eles recebem um data frame regular, eles retornam um data frame regular, enquanto que se eles recebem um tibble, eles retornam um tibble. Mas tibbles_é o formato preferido_tidyverse e, como resultado, funções tidyverse_que produzem um_data frame_do zero retornam uma_tibble. Por exemplo, no capítulo 5 veremos que as funções tidyverse_usadas para importar dados criam_tibbles. Tibbles_são muito semelhantes aos_data frames. De fato, eles podem pensar neles como uma versão moderna de data frames. No entanto, existem três diferenças importantes que descreveremos abaixo. 4.10.1 Tibbles parece melhor O método de impressão para tibbles_é mais legível que o de um_data frame. Para ver isso, compare a saída da digitação murders e a saída dos assassinatos, se os tornarmos uma tagarelice. Podemos fazer isso usando as_tibble(murders). Se você usa o RStudio, a saída_de_tabela_ ajusta-se ao tamanho da sua janela. Para ver isso, altere a largura do seu console R e observe como mais/ menos colunas são exibidas. 4.10.2 Tibbles_subconjuntos são_tibbles Se criarmos subconjuntos das colunas de um data frame, eles poderão retornar um objeto que não seja data frame, como um vetor ou escalar. Por exemplo: class(murders[,4]) #&gt; [1] &quot;numeric&quot; não é um data frame. Com tibbles isso não acontece: class(as_tibble(murders)[,4]) #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Isso é útil em tidyverse, pois as funções requerem data frames_como_input. Com tibbles, se você deseja acessar o vetor que define uma coluna e não recuperar um data frame, deve usar o operador de acesso $: class(as_tibble(murders)$population) #&gt; [1] &quot;numeric&quot; Um recurso relacionado é que tibbles avisará se eles tentarem acessar uma coluna que não existe. Por exemplo, se escrevermos acidentalmente Population ao invés de population nós vemos que: murders$Population #&gt; NULL retorna um NULL sem aviso, o que pode dificultar a depuração. Por outro lado, se tentarmos isso com uma tibble, obteremos um aviso informativo: as_tibble(murders)$Population #&gt; Warning: Unknown or uninitialised column: `Population`. #&gt; NULL 4.10.3 Tibbles pode ter entradas complexas Embora as colunas do data frame devam ser vetores de números, cadeias ou valores lógicos, tibbles pode ter objetos mais complexos, como listas ou funções. Além disso, podemos criar tibbles com funções: tibble(id = c(1, 2, 3), func = c(mean, median, sd)) #&gt; # A tibble: 3 x 2 #&gt; id func #&gt; &lt;dbl&gt; &lt;list&gt; #&gt; 1 1 &lt;fn&gt; #&gt; 2 2 &lt;fn&gt; #&gt; 3 3 &lt;fn&gt; 4.10.4 Tibbles podem ser agrupados A função group_by retorna um tipo especial de tibble: um tibble agrupado. Essa classe armazena informações que permitem saber quais linhas estão em quais grupos. Funções Tidyverse, em particular a função summarize, estão cientes das informações do grupo. 4.10.5 Como criar um tibble usando tibble ao invés de data.frame Às vezes, é útil criarmos nossos próprios data frames. Para criar um data frame_no formato_tibble, você pode usar a função tibble. grades &lt;- tibble(names = c(&quot;John&quot;, &quot;Juan&quot;, &quot;Jean&quot;, &quot;Yao&quot;), exam_1 = c(95, 80, 90, 85), exam_2 = c(90, 85, 85, 90)) Observe que a base R (nenhum pacote carregado) tem uma função com um nome muito semelhante, data.frame, que pode ser usado para criar um data frame_regular em vez de um tibble. Outra diferença importante é que, por padrão data.frame forçar a conversão de caracteres em fatores sem fornecer um aviso ou mensagem: grades &lt;- data.frame(names = c(&quot;John&quot;, &quot;Juan&quot;, &quot;Jean&quot;, &quot;Yao&quot;), exam_1 = c(95, 80, 90, 85), exam_2 = c(90, 85, 85, 90)) class(grades$names) #&gt; [1] &quot;factor&quot; Para evitar isso, usamos o argumento bastante complicado stringsAsFactors: grades &lt;- data.frame(names = c(&quot;John&quot;, &quot;Juan&quot;, &quot;Jean&quot;, &quot;Yao&quot;), exam_1 = c(95, 80, 90, 85), exam_2 = c(90, 85, 85, 90), stringsAsFactors = FALSE) class(grades$names) #&gt; [1] &quot;character&quot; Para converter um data frame_normal em um_tibble, você pode usar a função as_tibble. as_tibble(grades) %&gt;% class() #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; 4.11 O operador de ponto Uma das vantagens de usar o pipe %&gt;% é que não precisamos continuar nomeando novos objetos enquanto manipulamos o data frame. Lembre-se de que, se quisermos calcular a taxa média de homicídios nos estados do sul, em vez de escrever: tab_1 &lt;- filter(murders, region == &quot;South&quot;) tab_2 &lt;- mutate(tab_1, rate = total/ population * 10^5) rates &lt;- tab_2$rate median(rates) #&gt; [1] 3.4 podemos evitar definir novos objetos intermediários escrevendo: filter(murders, region == &quot;South&quot;) %&gt;% mutate(rate = total/ population * 10^5) %&gt;% summarize(median = median(rate)) %&gt;% pull(median) #&gt; [1] 3.4 Podemos fazer isso porque cada uma dessas funções usa um data frame como o primeiro argumento. Mas e se quisermos acessar um componente do data frame? Por exemplo, e se a função pull não está disponível e queremos acessar tab_2$rate? Que nome de quadro de dados usamos? A resposta é o operador de ponto (dot operator em inglês). Por exemplo, para acessar o vetor de velocidade sem a função pull, poderíamos usar: rates &lt;-filter(murders, region == &quot;South&quot;) %&gt;% mutate(rate = total/ population * 10^5) %&gt;% .$rate median(rates) #&gt; [1] 3.4 Na próxima seção, veremos outras instâncias nas quais usar o . é útil. 4.12 do As funções tidyverse_sabem como interpretar_tibbles agrupados. Além disso, para facilitar o script através do pipe %&gt;% as funções tidyverse_retornam constantemente_data frames, pois isso garante que a saída de uma função seja aceita como a entrada de outra. Mas a maioria das funções R não reconhece tibbles_agrupados nem retorna_data frames. A função quantile é um exemplo que descrevemos na seção 4.7.1. A função do serve como uma ponte entre as funções de R, como quantile e o tidyverse. A função do entende tibbles_agrupados e sempre retorna um_data frame. Na seção 4.7.1, percebemos que, se tentarmos usar quantile para obter o mínimo, a mediana e o máximo de uma chamada, receberemos um erro: Error: expecting result of length one, got : 2. data(heights) heights %&gt;% filter(sex == &quot;Female&quot;) %&gt;% summarize(range = quantile(height, c(0, 0.5, 1))) Nós podemos usar a função do para corrigir isso. Primeiro, precisamos escrever uma função que se ajuste ao foco do tidyverse: isto é, ele recebe um data frame_e retorna um_data frame. my_summary &lt;- function(dat){ x &lt;- quantile(dat$height, c(0, 0.5, 1)) tibble(min = x[1], median = x[2], max = x[3]) } Agora podemos aplicar a função ao conjunto de dados de altura para obter os resumos: heights %&gt;% group_by(sex) %&gt;% my_summary #&gt; # A tibble: 1 x 3 #&gt; min median max #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 50 68.5 82.7 Mas não é isso que queremos. Queremos um resumo para cada gênero e o código retornou apenas um resumo. Isto é porque my_summary não faz parte do tidyverse_e não sabe como lidar com os_tibbles agrupados. do faz esta conexão: heights %&gt;% group_by(sex) %&gt;% do(my_summary(.)) #&gt; # A tibble: 2 x 4 #&gt; # Groups: sex [2] #&gt; sex min median max #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Female 51 65.0 79 #&gt; 2 Male 50 69 82.7 Lembre-se de que aqui precisamos usar o operador de ponto. O tibble criado por group_by é canalizado para do. Dentro da chamada para do, o nome dessa tibble é . e queremos enviá-lo para my_summary. Se eles não usarem o ponto, então my_summary ele não tem argumento e retorna um erro nos dizendo que o argument &quot;dat&quot; . Você pode ver o erro digitando: heights %&gt;% group_by(sex) %&gt;% do(my_summary()) Se eles não usarem parênteses, a função não será executada e, em vez disso, do tente retornar a função. Isso dá um erro porque do você sempre deve retornar um data frame. Você pode ver o erro digitando: heights %&gt;% group_by(sex) %&gt;% do(my_summary) 4.13 O pacote purrr Na seção 3.5 nós aprendemos sobre função sapply, o que nos permitiu aplicar a mesma função a cada elemento de um vetor. Criamos uma função e usamos sapply para calcular a soma do primeiro n números inteiros para vários valores de n assim: compute_s_n &lt;- function(n){ x &lt;- 1:n sum(x) } n &lt;- 1:25 s_n &lt;- sapply(n, compute_s_n) Esse tipo de operação, que aplica a mesma função ou procedimento aos elementos de um objeto, é bastante comum na análise de dados. O pacote purrr inclui funções semelhantes a sapply mas eles interagem melhor com outras funções tidyverse. A principal vantagem é que podemos controlar melhor o tipo de resultado das funções. Por contraste, sapply você pode retornar vários tipos diferentes de objetos, convertendo-os quando conveniente. As funções purrr nunca farão isso: elas retornarão objetos de um tipo específico ou retornarão um erro se isso não for possível. A primeira função de purrr que aprenderemos é map, que funciona muito semelhante a sapply mas sempre, sem exceção, ele retorna uma lista: library(purrr) s_n &lt;- map(n, compute_s_n) class(s_n) #&gt; [1] &quot;list&quot; Se queremos um vetor numérico, podemos usar map_dbl que sempre retorna um vetor de valores numéricos. s_n &lt;- map_dbl(n, compute_s_n) class(s_n) #&gt; [1] &quot;numeric&quot; Isso produz os mesmos resultados que a chamada sapply que vemos acima. Uma função __purrr_particularmente útil para interagir com o resto do_tidyverse é map_df, que sempre retorna um tibble data frame. No entanto, a função chamada deve retornar um vetor ou uma lista com nomes. Por esse motivo, o código a seguir resultaria em um erro Argument 1 must have names: s_n &lt;- map_df(n, compute_s_n) Precisamos alterar a função para corrigir isso: compute_s_n &lt;- function(n){ x &lt;- 1:n tibble(sum = sum(x)) } s_n &lt;- map_df(n, compute_s_n) O pacote purrr oferece muito mais funcionalidades não discutidas aqui. Para mais detalhes, você pode consultar [este recurso online] (https://jennybc.github.io/purrr-tutorial/). 4.14 Os condicionais tidyverse Uma análise de dados típica geralmente envolve uma ou mais operações condicionais. Na seção 3.1 nós descrevemos a função ifelse, que usaremos extensivamente neste livro. Nesta seção, apresentamos duas funções dplyr que oferecem funcionalidade adicional para executar operações condicionais. 4.14.1 case_when A função case_when é útil para vetorizar instruções condicionais. Isso é semelhante a ifelse mas pode gerar qualquer número de valores, em vez de apenas TRUE ou FALSE. Aqui está um exemplo que divide os números em negativo, positivo e 0: x &lt;- c(-2, -1, 0, 1, 2) case_when(x &lt; 0 ~ &quot;Negative&quot;, x &gt; 0 ~ &quot;Positive&quot;, TRUE ~ &quot;Zero&quot;) #&gt; [1] &quot;Negative&quot; &quot;Negative&quot; &quot;Zero&quot; &quot;Positive&quot; &quot;Positive&quot; Um uso comum dessa função é definir variáveis categóricas com base nas variáveis existentes. Por exemplo, suponha que desejamos comparar as taxas de homicídios em quatro grupos de estados: Nova Inglaterra, Costa Oeste, Sul e Outro. Para cada estado, primeiro perguntamos se está na Nova Inglaterra. Se a resposta for não, perguntamos se está na Costa Oeste e, se não, perguntamos se está no Sul e, se não, atribuímos uma das opções acima (Outro). Aqui vemos como usamos case_when para fazer isso: murders %&gt;% mutate(group = case_when( abb %in% c(&quot;ME&quot;, &quot;NH&quot;, &quot;VT&quot;, &quot;MA&quot;, &quot;RI&quot;, &quot;CT&quot;) ~ &quot;New England&quot;, abb %in% c(&quot;WA&quot;, &quot;OR&quot;, &quot;CA&quot;) ~ &quot;West Coast&quot;, region == &quot;South&quot; ~ &quot;South&quot;, TRUE ~ &quot;Other&quot;)) %&gt;% group_by(group) %&gt;% summarize(rate = sum(total)/ sum(population) * 10^5) #&gt; # A tibble: 4 x 2 #&gt; group rate #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 New England 1.72 #&gt; 2 Other 2.71 #&gt; 3 South 3.63 #&gt; 4 West Coast 2.90 4.14.2 between Uma operação comum na análise de dados é determinar se um valor cai dentro de um intervalo. Podemos verificar isso usando condicionais. Por exemplo, para verificar se os elementos de um vetor x estão entre a e b nós podemos escrever: x &gt;= a &amp; x &lt;= b No entanto, isso pode ficar complicado, especialmente dentro da abordagem tidyverse. A função between execute a mesma operação: between(x, a, b) 4.15 Exercícios 1. Carregar o conjunto de dados murders. Qual dos seguintes é verdadeiro? para. murders está no formato tidy_e é armazenado em uma_tibble. b. murders está no formato tidy_e é armazenado em um_data frame. c. murders não está no formato tidy_e é armazenado em um_tibble. d. murders não está no formato tidy_e é armazenado em um_data frame. 2. Usar as_tibble converter a tabela de dados murders em uma tibble e salve-a em um objeto chamado murders_tibble. 3. Use a função group_by converter murders em uma tibble que é agrupada por região. 4. Escreva o código tidyverse que é equivalente a este código: exp(mean(log(murders$population))) Escreva usando o pipe para que cada função seja chamada sem argumentos. Use o operador de ponto para acessar a população. Dica: o código deve começar com murders %&gt;%. 5. Use o map_df para criar um data frame com três colunas denominadas n, s_n e s_n_2. A primeira coluna deve conter os números de 1 a 100. A segunda e a terceira coluna devem conter a soma de 1 a 100. \\(n\\) com \\(n\\) representando o número da linha. "],
["importing-data.html", "Capítulo 5 Importando dados 5.1 Caminhos e diretório de trabalho 5.2 Os pacotes readr e readxl 5.3 Exercícios 5.4 Como baixar arquivos 5.5 Funções básicas de importação R 5.6 Arquivos de texto versus arquivos binários 5.7 Unicode versus ASCII 5.8 Como organizar dados com planilhas 5.9 Exercícios", " Capítulo 5 Importando dados Temos usado conjuntos de dados já armazenados como objetos R. Os cientistas de dados raramente têm a mesma sorte e geralmente precisam importar dados para o R de um arquivo, banco de dados ou outras fontes. Atualmente, uma das maneiras mais comuns de armazenar e compartilhar dados para análise é por meio de planilhas eletrônicas. Uma planilha armazena dados em linhas e colunas. Basicamente, é uma versão de arquivo de um data frame. Ao salvar essa tabela em um arquivo de computador, é necessário definir como uma nova linha ou coluna termina e quando a outra começa. Por sua vez, define as células nas quais os valores individuais são armazenados. Ao criar planilhas com arquivos de texto, como aqueles criados com um editor de texto simples, uma nova linha é definida com um retorno e as colunas são separadas com um caractere especial predefinido. Os caracteres mais comuns são vírgulas ( ,), ponto e vírgula ( ;), space () e o tab (um número predeterminado de espaços ou \\t) Aqui está um exemplo da aparência de um arquivo separado por vírgula se o abrirmos com um editor de texto básico: A primeira linha contém nomes de colunas em vez de dados. Nós nos referimos a isso como um header (header em inglês) e, quando lemos (read-in) dados de uma planilha, é importante saber se o arquivo tem um cabeçalho ou não. A maioria das funções de leitura assume que existe um cabeçalho. Para descobrir se o arquivo tem um cabeçalho, verifique o arquivo antes de tentar lê-lo. Isso pode ser feito com um editor de texto ou com o RStudio. No RStudio, podemos fazer isso abrindo o arquivo no editor ou navegando até o local do arquivo, clicando duas vezes no arquivo e pressionando View File. No entanto, nem todos os arquivos de planilha estão no formato de texto. O Planilhas Google (_ Planilhas Google em inglês), por exemplo, é acessado com um navegador. Outro exemplo é o formato proprietário usado pelo Microsoft Excel, que não pode ser exibido com um editor de texto. Apesar disso e devido à popularidade do Microsoft Excel software, esse formato é amplamente utilizado. Começamos este capítulo descrevendo as diferenças entre arquivos de texto (ASCII), Unicode e binários e como isso afeta a maneira como os importamos. Em seguida, explicamos os conceitos de caminhos de arquivos e diretórios de trabalho, essenciais para entender como importar dados de maneira eficaz. Então, apresentamos os pacotes __readr__ereadxl e as funções disponíveis para importar planilhas para o R. Por fim, oferecemos algumas recomendações sobre como armazenar e organizar dados em arquivos. Desafios mais complexos, no entanto, como extrair dados de páginas da Web ou documentos em PDF, serão discutidos na parte “Wrangling data” do livro. 5.1 Caminhos e diretório de trabalho A primeira etapa na importação de dados de uma planilha é localizar o arquivo que contém os dados. Embora não o recomendamos, você pode usar uma abordagem semelhante à usada para abrir arquivos no Microsoft Excel, clicando no menu “Arquivo” no RStudio, clicando em “Importar conjunto de dados” e depois clicando em pastas até encontrar o arquivo. Queremos escrever código em vez de apontar e clicar. As chaves e os conceitos que precisamos aprender como fazer isso são descritos em detalhes na parte “Ferramentas de produtividade” do livro. Aqui, fornecemos uma visão geral do básico. O principal desafio desta primeira etapa é permitir que as funções R de importação saibam onde procurar o arquivo que contém os dados. A maneira mais fácil de fazer isso é ter uma cópia do arquivo na pasta em que as funções de importação pesquisam por padrão. Depois disso, basta fornecer o nome do arquivo para a função de importação. O pacote dslabs inclui uma planilha contendo os dados dos assassinatos nos EUA. EUA A localização desse arquivo não é óbvia, mas as seguintes linhas de código copiam o arquivo para a pasta que R procura por padrão. Abaixo, explicamos como essas linhas funcionam. filename &lt;- &quot;murders.csv&quot; dir &lt;- system.file(&quot;extdata&quot;, package = &quot;dslabs&quot;) fullpath &lt;- file.path(dir, filename) file.copy(fullpath, &quot;murders.csv&quot;) Este código não lê os dados em R, apenas copia um arquivo. Mas uma vez que o arquivo é copiado, podemos importar os dados com apenas uma linha de código. Aqui usamos a função read_csv do pacote readr, que faz parte do tidyverse. library(tidyverse) dat &lt;- read_csv(filename) Os dados são importados e armazenados em dat. O restante desta seção define alguns conceitos importantes e fornece uma visão geral de como escrevemos código para que o R possa encontrar os arquivos que queremos importar. Capítulo ?? oferece mais detalhes sobre este tópico. 5.1.1 O sistema de arquivos Eles podem pensar no sistema de arquivos do seu computador (file system) como uma série de pastas aninhadas, cada uma contendo outras pastas e arquivos. Os cientistas de dados se referem às pastas como diretórios_e à pasta que contém todas as outras pastas como o_root directory. O diretório em que estamos localizados é chamado de working directory. Portanto, o diretório de trabalho muda à medida que você percorre as pastas - considere-o como seu local atual. 5.1.2 Caminhos relativos e completos O path (path em inglês) de um arquivo é uma lista de nomes de diretórios que podem ser considerados instruções em quais pastas clicar e em que ordem para localizar o arquivo. Se essas instruções forem para encontrar o arquivo no diretório raiz, vamos nos referir a elas como o caminho completo (full path em inglês). Se as instruções forem para encontrar o arquivo no diretório ativo, nós os referimos como um caminho relativo (relative path em inglês). Seção ?? oferece mais detalhes sobre este tópico. Para ver um exemplo de caminho completo em seus sistemas, digite o seguinte: system.file(package = &quot;dslabs&quot;) As cadeias separadas por barras são os nomes dos diretórios. A primeira barra representa o diretório raiz e sabemos que esse é um caminho completo porque começa com uma barra. Se o primeiro nome do diretório aparecer sem uma barra, R assume que o caminho é relativo. Nós podemos usar a função list.files para ver exemplos de rotas relativas: dir &lt;- system.file(package = &quot;dslabs&quot;) list.files(path = dir) #&gt; [1] &quot;data&quot; &quot;DESCRIPTION&quot; &quot;extdata&quot; &quot;help&quot; #&gt; [5] &quot;html&quot; &quot;INDEX&quot; &quot;Meta&quot; &quot;NAMESPACE&quot; #&gt; [9] &quot;R&quot; &quot;script&quot; Esses caminhos relativos nos fornecem a localização dos arquivos ou diretórios se iniciarmos no diretório com o caminho completo. Por exemplo, o caminho completo para o diretório help no exemplo acima, é: /Library/Frameworks/R.framework/Versions/3.5/Resources/library/dslabs/help. Nota: eles provavelmente não farão muito uso do recurso system.file no seu trabalho diário de análise de dados. Nós o apresentamos nesta seção, pois facilita a troca de planilhas, incluindo-as no pacote dslabs. Raramente eles terão o luxo de incluir dados nos pacotes que já instalaram. No entanto, eles geralmente precisam navegar por caminhos completos e relativos e importar dados no formato de planilha. 5.1.3 O diretório de trabalho Recomendamos escrever apenas caminhos relativos no seu código, pois os caminhos completos são exclusivos para seus computadores e você deseja que seu código seja portátil. Eles podem obter o caminho completo do diretório de trabalho sem gravá-lo explicitamente usando a função getwd: wd &lt;- getwd() Se você precisar alterar seu diretório de trabalho, poderá usar a função setwd ou eles podem alterá-lo através do RStudio, clicando em “Sessão”. 5.1.4 Como gerar nomes de caminhos Outro exemplo de como obter um caminho completo sem escrever explicitamente foi oferecido acima quando criamos o objeto fullpath desta maneira: filename &lt;- &quot;murders.csv&quot; dir &lt;- system.file(&quot;extdata&quot;, package = &quot;dslabs&quot;) fullpath &lt;- file.path(dir, filename) A função system.file fornece o caminho completo da pasta que contém todos os arquivos e diretórios relevantes para o pacote especificado pelo argumento package. Ao procurar diretórios em dir, descobrimos que extdata contém o arquivo que queremos: dir &lt;- system.file(package = &quot;dslabs&quot;) filename %in% list.files(file.path(dir, &quot;extdata&quot;)) #&gt; [1] TRUE A função system.file nos permite fornecer um subdiretório como o primeiro argumento, para que possamos obter o caminho completo do diretório extdata assim: dir &lt;- system.file(&quot;extdata&quot;, package = &quot;dslabs&quot;) A função file.path é usado para combinar os nomes dos diretórios para produzir o caminho completo do arquivo que queremos importar. fullpath &lt;- file.path(dir, filename) 5.1.5 Como copiar arquivos usando caminhos A última linha de código que usamos para copiar o arquivo em nosso diretório inicial usou a função file.copy. Isso requer dois argumentos: o nome do arquivo a ser copiado e o nome a ser usado no novo diretório. file.copy(fullpath, &quot;murders.csv&quot;) #&gt; [1] TRUE Se um arquivo for copiado com sucesso, a função file.copy trazer de volta TRUE. Lembre-se de que estamos dando o mesmo nome ao arquivo, murders.csv, mas poderíamos ter dado qualquer nome a ele. Lembre-se também de que, ao não iniciar a sequência com uma barra, R assume que este é um caminho relativo e copia o arquivo no diretório de trabalho. Eles devem poder visualizar o arquivo em seu diretório de trabalho usando: list.files() 5.2 Os pacotes readr e readxl Nesta seção, apresentamos as principais funções de importação do tidyverse. Nós vamos usar o arquivo murders.csv do pacote dslabs como um exemplo. Para simplificar a ilustração, copiaremos o arquivo para nosso diretório de trabalho usando o seguinte código: filename &lt;- &quot;murders.csv&quot; dir &lt;- system.file(&quot;extdata&quot;, package = &quot;dslabs&quot;) fullpath &lt;- file.path(dir, filename) file.copy(fullpath, &quot;murders.csv&quot;) 5.2.1 readr O pacote readr inclui funções para ler dados armazenados em planilhas. __readr__faz parte do pacotetidyverse, ou você pode carregá-lo diretamente assim: library(readr) As seguintes funções estão disponíveis para leitura de planilhas: Função Formato Sufixo típico read_table valores separados por espaços em branco txt read_csv valores separados por vírgula CSV read_csv2 valores separados por ponto e vírgula CSV read_tsv valores separados delimitados por tabulação tsv read_delim formato de arquivo de texto geral, você deve definir delimitador txt Embora o sufixo geralmente nos diga que tipo de arquivo é, não há garantia de que eles sempre correspondam. Podemos abrir o arquivo para dar uma olhada ou usar a função read_lines para ver algumas linhas: read_lines(&quot;murders.csv&quot;, n_max = 3) #&gt; [1] &quot;state,abb,region,population,total&quot; #&gt; [2] &quot;Alabama,AL,South,4779736,135&quot; #&gt; [3] &quot;Alaska,AK,West,710231,19&quot; Isso também mostra que existe um cabeçalho. Agora estamos prontos para ler os dados em R. A partir do sufixo .csv e do arquivo, sabemos que precisamos usar read_csv: dat &lt;- read_csv(filename) #&gt; Parsed with column specification: #&gt; cols( #&gt; state = col_character(), #&gt; abb = col_character(), #&gt; region = col_character(), #&gt; population = col_double(), #&gt; total = col_double() #&gt; ) Observe que recebemos uma mensagem informando que tipos de dados foram usados para cada coluna. Observe também que dat é um tibble, não apenas um data frame. Isto é porque read_csv é um leitor (parser em inglês) de tidyverse. Podemos confirmar que os dados foram lidos da seguinte forma: View(dat) Por fim, lembre-se de que também podemos usar o caminho completo para o arquivo: dat &lt;- read_csv(fullpath) 5.2.2 readxl Eles podem carregar o pacote readxl usando: library(readxl) O pacote oferece funções para a leitura dos formatos do Microsoft Excel: Função Formato Sufixo típico read_excel detectar automaticamente o formato xls, xlsx read_xls formato original xls read_xlsx novo formato xlsx Os formatos do Microsoft Excel permitem que você tenha mais de uma planilha em um arquivo. Estes são conhecidos como sheets. As funções listadas acima leem a primeira folha por padrão, mas também podemos ler as outras. A função excel_sheets nos fornece os nomes de todas as planilhas em um arquivo excel. Esses nomes podem ser passados para o argumento sheet nas três funções anteriores para ler folhas diferentes da primeira. 5.3 Exercícios 1. Use a função read_csv para ler cada um dos arquivos que o código a seguir armazena no objeto files: path &lt;- system.file(&quot;extdata&quot;, package = &quot;dslabs&quot;) files &lt;- list.files(path) files 2. Observe que o último, o arquivo olive, nos dá um aviso. Isso ocorre porque a primeira linha do arquivo está ausente no cabeçalho da primeira coluna. Leia a página de ajuda para read_csv para aprender a ler o arquivo sem ler este cabeçalho. Se você omitir o cabeçalho, não deverá receber este aviso. Salve o resultado em um objeto chamado dat. 3. Um problema com a abordagem acima é que não sabemos o que as colunas representam. Escriba: names(dat) para confirmar que os nomes não são informativos. Use a função readLines para ler apenas a primeira linha (mais tarde aprenderemos como extrair valores de output). 5.4 Como baixar arquivos Outro local comum onde os dados residem é na internet. Quando esses dados estão em arquivos, podemos baixá-los e importá-los, ou mesmo lê-los diretamente da web. Por exemplo, observamos que, como o pacote dslabs está no GitHub, o arquivo que baixamos com o pacote tem uma URL: url &lt;- &quot;https://raw.githubusercontent.com/rafalab/dslabs/master/inst/ extdata/murders.csv&quot; O arquivo read_csv você pode ler esses arquivos diretamente: dat &lt;- read_csv(url) Se você deseja ter uma cópia local do arquivo, pode usar a função download.file: download.file(url, &quot;murders.csv&quot;) Isso fará o download do arquivo e o salvará no seu sistema com o nome murders.csv. Você pode usar qualquer nome aqui, não necessariamente murders.csv. Lembre-se de que ao usar download.file tenha cuidado, pois substituirá os arquivos existentes sem aviso prévio. Duas funções que às vezes são úteis ao baixar dados da Internet são tempdir e tempfile. O primeiro cria um diretório com um nome aleatório que provavelmente é único. Igualmente, tempfile crie uma cadeia de caracteres, não um arquivo, que provavelmente é um nome de arquivo exclusivo. Eles podem executar um comando, como o seguinte, que exclui o arquivo temporário depois que os dados são importados: tmp_filename &lt;- tempfile() download.file(url, tmp_filename) dat &lt;- read_csv(tmp_filename) file.remove(tmp_filename) 5.5 Funções básicas de importação R A base R também fornece funções de importação. Eles têm nomes semelhantes aos do tidyverse, por exemplo read.table, read.csv e read.delim. No entanto, existem algumas diferenças importantes. Para mostrar isso, lemos os dados com uma função base R: dat2 &lt;- read.csv(filename) Uma diferença importante é que os personagens se tornam fatores: class(dat2$abb) #&gt; [1] &quot;factor&quot; class(dat2$region) #&gt; [1] &quot;factor&quot; Isso pode ser evitado definindo o argumento stringsAsFactors como FALSE. dat &lt;- read.csv(&quot;murders.csv&quot;, stringsAsFactors = FALSE) class(dat$state) #&gt; [1] &quot;character&quot; Em nossa experiência, isso pode ser confuso, pois uma variável que foi salva como caracteres no arquivo se torna fatores, independentemente do que a variável represente. De fato, é altamente recomendável definir sempre stringsAsFactors=FALSE para ser sua abordagem padrão ao usar leitores de base R. Eles podem converter facilmente as colunas desejadas em fatores após a importação de dados. 5.5.1 scan Ao ler planilhas, muitas coisas podem dar errado. O arquivo pode ter um cabeçalho de várias linhas, as células podem estar ausentes ou pode usar uma codificação inesperada15. Recomendamos que você leia esta postagem sobre problemas comuns: [https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about -unicode -and-character-sets-no-desculuses/] (https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about) -unicode-and-character-sets-no-desculpas/). Com a experiência, eles aprenderão a lidar com diferentes desafios. Além disso, ajudará você a ler atentamente os arquivos de ajuda para as funções discutidas aqui. Com scan eles podem ler todas as células em um arquivo, como vemos aqui: path &lt;- system.file(&quot;extdata&quot;, package = &quot;dslabs&quot;) filename &lt;- &quot;murders.csv&quot; x &lt;- scan(file.path(path, filename), sep=&quot;,&quot;, what = &quot;c&quot;) x[1:10] #&gt; [1] &quot;state&quot; &quot;abb&quot; &quot;region&quot; &quot;population&quot; &quot;total&quot; #&gt; [6] &quot;Alabama&quot; &quot;AL&quot; &quot;South&quot; &quot;4779736&quot; &quot;135&quot; Observe que o tidyverse inclui read_lines, uma função igualmente útil. 5.6 Arquivos de texto versus arquivos binários Na ciência de dados, os arquivos geralmente podem ser classificados em duas categorias: arquivos de texto (também conhecidos como arquivos ASCII) e arquivos binários. Eles já trabalharam com arquivos de texto. Todos os seus R scripts são arquivos de texto, assim como os arquivos de remarcação R usados para criar este livro. As tabelas csv que você leu também são arquivos de texto. Uma grande vantagem desses arquivos é que podemos “olhá-los” com facilidade, sem precisar comprar nenhum tipo de software especial ou seguir instruções complicadas. Qualquer editor de texto pode ser usado para navegar em um arquivo de texto, incluindo editores disponíveis gratuitamente, como RStudio, Notepad, textEdit, vi, emacs, nano e pico. Para ver isso, tente abrir um arquivo csv com a ferramenta RStudio “Abrir arquivo”. Eles devem poder ver o conteúdo diretamente no editor. No entanto, se tentarem abrir, digamos, um arquivo xls, jpg ou png do Excel, não poderão ver nada imediatamente útil. Estes são arquivos binários. Os arquivos do Excel são pastas compactadas com vários arquivos de texto. Mas a principal distinção aqui é que os arquivos de texto podem ser facilmente navegados. Embora o R inclua ferramentas para ler arquivos binários amplamente utilizados, como arquivos xls, geralmente é melhor encontrar conjuntos de dados armazenados em arquivos de texto. Da mesma forma, ao compartilhar dados, é melhor disponibilizá-los como arquivos de texto, desde que o armazenamento não seja um problema (os arquivos binários são muito mais eficientes na economia de espaço em disco). Em geral, os formatos de texto facilitam a troca de dados, pois não requerem software comercial para trabalhar com os dados. Extrair dados de uma planilha armazenada como um arquivo de texto é talvez a maneira mais fácil de trazer dados de um arquivo para uma sessão R. Infelizmente, as planilhas nem sempre estão disponíveis e o fato de poderem visualizar arquivos de O texto não implica necessariamente que é fácil extrair dados dele. Na parte “Wrangling data” do livro, aprendemos como extrair dados de arquivos de texto mais complexos, como arquivos html. 5.7 Unicode versus ASCII Uma armadilha na ciência de dados é assumir que um arquivo é um arquivo de texto ASCII, quando na verdade é outra coisa que pode se parecer muito com um arquivo de texto ASCII: um arquivo de texto Unicode. Para entender a diferença entre eles, lembre-se de que tudo no computador precisa se tornar 0s e 1s. ASCII é um encoding que define uma correspondência entre caracteres e números. O ASCII usa 7 bits (0s e 1s), o que resulta em \\(2^7 = 128\\) elementos exclusivos, suficientes para codificar todos os caracteres em um teclado em inglês. No entanto, outros idiomas, como o espanhol, usam caracteres não incluídos nesta codificação. Por exemplo, os tildes não são codificados em ASCII. Por esse motivo, foi definida uma nova codificação que usa mais de 7 bits: Unicode. Ao usar o Unicode, você pode escolher entre UTF-8, UTF-16 e UTF-32 abreviado de 8, 16 e 32 bits, respectivamente. O RStudio usa a codificação UTF-8 por padrão. Embora não entremos em detalhes sobre como lidar com diferentes codificações aqui, é importante que você saiba que existem codificações diferentes para que você possa diagnosticar bem um problema, se o encontrar. Uma maneira de os problemas se manifestarem é quando personagens “de aparência estranha” surgem e você não esperava. Esta discussão do StackOverflow é um exemplo: [https://stackoverflow.com/questions/18789330/r-on-windows-character-encoding-hell&gt;(https://stackoverflow.com/questions/18789330/r-on- Windows-personagem-codificação-inferno). 5.8 Como organizar dados com planilhas Embora este livro se concentre quase exclusivamente na análise de dados, o gerenciamento de dados também é uma parte importante do ciência de dados. Como explicamos na introdução, não cobrimos isso tema. No entanto, muitas vezes analistas de dados eles precisam coletar dados ou trabalhar com outras pessoas que coletam dados, portanto, a maneira mais conveniente de armazená-los é planilha. Embora o preenchimento manual de uma planilha seja um prática que não recomendamos e preferimos que o processo é automatizado o máximo possível, às vezes não há outra opção. Para o portanto, nesta seção, oferecemos recomendações sobre como organizar os dados em uma planilha. Embora existam pacotes R projetados para lendo planilhas do Microsoft Excel, geralmente queremos evitar este formato. Recomendamos o Google Sheets como uma ferramenta para software grátis. Baixa resumimos as recomendações feitas em uma publicação de Karl Broman e Kara Woo16. Leia o artigo completo para obter detalhes mais importantes. Seja consistente - Antes de começar a inserir dados, tenha um plano. Depois de ter, seja consistente e siga-o. Escolha bons nomes para as coisas: os nomes que você escolher para objetos, arquivos e diretórios devem ser memoráveis, fáceis de soletrar e descritivos. Esse é um equilíbrio difícil de alcançar e requer tempo e reflexão. Uma regra importante a seguir é não use espaços, use sublinhados _ ou hífens -. Além disso, evite símbolos; é melhor usar letras e números. Insira as datas como AAAA-MM-DD - Para evitar confusão, recomendamos o uso do padrão global ISO 8601. Evitar células vazias - Preencha todas as células e use código comum para a falta de dados. Coloque apenas uma coisa em cada célula - É melhor adicionar colunas para armazenar informações adicionais em vez de ter mais de uma informação em uma célula. Faça um retângulo - A planilha deve ser um retângulo. Criar um dicionário de dados - Se você precisar explicar as coisas, por exemplo, quais são as colunas ou os rótulos usados para variáveis categóricas, faça isso em um arquivo separado. Não faça cálculos em arquivos de dados brutos - o Excel permite que você faça cálculos. Não faça parte da sua planilha. O código para os cálculos deve estar em um script. Não use cores ou realces como dados - A maioria das funções de importação não pode importar essas informações. Em vez disso, codifique essas informações como uma variável. Faça backup de suas informações: faça backup de seus dados com freqüência. Utilize a validação de dados para evitar erros - Aproveite as ferramentas do seu spreadsheet_software para tornar o processo o mais livre possível de erros e lesões por esforço repetitivo. Salvar dados como arquivos de texto - Salve os arquivos para compartilhar em formato delimitado por vírgula ou tabs. 5.9 Exercícios 1. Escolha uma ação que você pode executar regularmente. Por exemplo, seu peso diário ou quanto tempo leva para percorrer 8 km. Mantenha uma planilha que inclua data, hora, medida e outras variáveis informativas que considere valiosas. Faça isso por 2 semanas. Então faça um gráfico. https://en.wikipedia.org/wiki/Character_encoding↩ https://www.tandfonline.com/doi/abs/10.1080/00031305.2017.1375989↩ "],
["parte-exibição-de-dados.html", "(PARTE) Exibição de dados", " (PARTE) Exibição de dados "],
["introdução-à-visualização-de-dados.html", "Capítulo 6 Introdução à visualização de dados", " Capítulo 6 Introdução à visualização de dados Raramente é útil observar os números e as cadeias de caracteres que definem um conjunto de dados. Para confirmar isso, imprima e observe a tabela de dados de assassinato dos Estados Unidos: library(dslabs) data(murders) head(murders) #&gt; state abb region population total #&gt; 1 Alabama AL South 4779736 135 #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 #&gt; 4 Arkansas AR South 2915918 93 #&gt; 5 California CA West 37253956 1257 #&gt; 6 Colorado CO West 5029196 65 O que eles aprendem ao ver esta tabela? Com que rapidez eles podem determinar quais estados têm as maiores populações? Quais estados têm o menor? Qual o tamanho de um estado típico? Existe uma relação entre o tamanho da população e o total de mortes? Como as taxas de homicídio variam entre as regiões do país? Para a maioria dos cérebros humanos, é bastante difícil extrair essas informações simplesmente observando os números. Em vez disso, as respostas para todas as perguntas acima estão prontamente disponíveis examinando este gráfico: Isso nos lembra o ditado “uma imagem vale mais que mil palavras”. A visualização de dados oferece uma maneira poderosa de comunicar descobertas com base nos dados. Em alguns casos, a visualização é tão atraente que não requer análise de acompanhamento. A crescente disponibilidade de conjuntos de dados informativos e ferramentas de software levou a uma maior dependência da visualização de dados em muitos setores, universidades e governos. Um excelente exemplo são as organizações de notícias, que estão cada vez mais adotando data journalism_e incluindo_infographics eficaz como parte de seus relatórios. Um exemplo particularmente eficaz é um artigo do Wall Street Journal17 mostrando dados relacionados ao impacto das vacinas na luta contra a doenças infecciosas. Um dos gráficos mostra casos de sarampo por estado dos EUA. EUA ao longo dos anos com uma linha vertical indicando quando a vacina foi introduzida. Outro exemplo notável vem de um gráfico do New York Times18 que resume os resultados dos testes do Regentes da cidade de Nova York. De acordo com o artigo19, essas pontuações são coletadas por vários motivos, inclusive para determinar se um aluno está se formando no ensino médio. Na cidade de Nova York, é necessária uma pontuação mínima de 65 para passar. A distribuição dos resultados dos testes nos obriga a notar algo um pouco problemático: A pontuação mais comum no teste é a nota mínima para aprovação, com muito poucas notas logo abaixo do limite. Esse resultado inesperado é consistente com o aumento da pontuação dos alunos próximo da aprovação, mas sem a obtenção do mínimo de 65. Este é um exemplo de como a visualização de dados pode levar a descobertas que de outra forma seriam perdidas se simplesmente submetêssemos os dados a uma série de ferramentas ou procedimentos de análise de dados. A visualização de dados é a ferramenta mais eficaz do que chamamos de “análise exploratória de dados”, ou EDA. John W. Tukey20, considerado o pai da EDA, disse uma vez: “O maior valor de uma imagem é quando ela nos força a perceber o que nunca esperávamos ver”. Muitas das ferramentas de análise de dados mais usadas foram inicialmente desenvolvidas graças à EDA. Esta é talvez a parte mais importante da análise de dados, mas é frequentemente ignorada. A visualização de dados agora é onipresente também em organizações filantrópicas e educacionais. Nas conferências “Novas idéias sobre a pobreza”21 e “As melhores estatísticas que você nunca viu”^[https://www.ted. Hans Rosling nos obriga a perceber o inesperado com uma série de gráficos relacionados à saúde e à economia global. Em seus vídeos, Rosling usa gráficos animados para demonstrar como o mundo está mudando e como as antigas narrativas não são mais verdadeiras. Também é importante lembrar que erros, preconceitos, erros sistemáticos e outros problemas inesperados geralmente levam a dados que devem ser cuidadosamente analisados. A falha em descobrir esses problemas pode levar a análises falhas e descobertas falsas. Como exemplo, considere que os instrumentos de medição às vezes falham e que a maioria dos procedimentos de análise de dados não foi projetada para detectá-los. No entanto, esses procedimentos ainda lhe darão uma resposta. O fato de ser difícil, ou mesmo impossível, perceber um erro apenas nos resultados relatados torna a visualização de dados particularmente importante. Nesta parte do livro, aprenderemos os conceitos básicos de visualização de dados e análise exploratória de dados usando três exemplos motivadores. Usaremos o pacote ggplot2 para codificar. Para aprender o básico, usaremos um exemplo um tanto artificial: as alturas relatadas pelos alunos. Em seguida, discutiremos dois exemplos mencionados acima: 1) saúde e economia globais e 2) tendências em doenças infecciosas nos Estados Unidos. Obviamente, a visualização de dados é muito mais do que o que abordamos aqui. Aqui estão algumas referências para quem deseja saber mais: ER Tufte (1983) A exibição visual de informações quantitativas. Imprensa gráfica. ER Tufte (1990) Envisioning information. Imprensa gráfica. ER Tufte (1997) Explicações visuais. Imprensa gráfica. WS Cleveland (1993) Visualização de dados. Hobart Press. WS Cleveland (1994) Os elementos dos dados gráficos. CRC Pressione. A Gelman, C Pasarica, R Dodhia (2002) Vamos praticar o que pregamos: Transformar tabelas em gráficos. The American Statistician 56: 121-130. NB Robbins (2004) Criando gráficos mais eficazes. Wiley. A Cairo (2013) A arte funcional: Uma introdução à informação gráfica e visualização. Novos Cavaleiros. N Yau (2013) Pontos de dados: Visualização que significa alguma coisa. Wiley. Por fim, não discutiremos gráficos interativos, um tópico muito avançado para este livro. Abaixo estão alguns recursos úteis para aqueles interessados em aprender mais sobre isso: [https://shiny.rstudio.com] (https://shiny.rstudio.com/) [https://d3js.org] (https://d3js.org/) http://graphics.wsj.com/infectious-diseases-and-vaccines/?mc_cid=711ddeb86e↩ http://graphics8.nytimes.com/images/2011/02/19/nyregion/19schoolsch/19schoolsch-popup.gif↩ https://www.nytimes.com/2011/02/19/nyregion/19schools.html↩ https://en.wikipedia.org/wiki/John_Tukey↩ https://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en↩ "],
["ggplot2.html", "Capítulo 7 ggplot2 7.1 Os componentes de um gráfico 7.2 objetos ggplot 7.3 Geometrias 7.4 Mapeamentos estéticos 7.5 Camadas 7.6 Mapeamento estético global versus local 7.7 Escalas 7.8 Tags e títulos 7.9 Categorias como cores 7.10 Anotação, formas e ajustes 7.11 Pacotes complementares 7.12 Como combinar tudo 7.13 Gráficos rápidos com qplot 7.14 Grades de gráfico 7.15 Exercícios", " Capítulo 7 ggplot2 A visualização de dados exploratórios é talvez a maior vantagem de R. É possível passar rapidamente da ideia para os dados para representar graficamente com um equilíbrio único de flexibilidade e facilidade. Por exemplo, o Excel pode ser mais fácil que o R para alguns gráficos, mas não é tão flexível. O D3.js pode ser mais flexível e poderoso que o R, mas leva muito mais tempo para gerar um gráfico. Ao longo do livro, criaremos gráficos usando o pacote ggplot222. library(dplyr) library(ggplot2) Existem muitas opções gráficas disponíveis em R. De fato, os recursos gráficos que vêm com uma instalação básica do R já são bastante poderosos. Também existem outros pacotes para criar gráficos como __grid__elattice. Neste livro, decidimos usar ggplot2 porque ele divide gráficos em componentes de uma maneira que permite aos iniciantes criar gráficos relativamente complexos e esteticamente agradáveis usando sintaxe intuitiva e relativamente fácil de lembrar. Um dos motivos pelo qual ggplot2 é geralmente mais intuitivo para iniciantes é porque ele usa uma gramática gráfica23, o _gg_doggplot2. Isso é análogo a como a aprendizagem de gramática pode ajudar um aluno a criar centenas de frases diferentes, aprendendo apenas um pequeno número de verbos, substantivos e adjetivos, em vez de memorizar cada frase específica. Da mesma forma, aprendendo uma pequena quantidade dos componentes básicos do ggplot2 e sua gramática, você poderá criar centenas de gráficos diferentes. Outro motivo pelo qual ggplot2 é fácil para iniciantes é que seu comportamento padrão foi cuidadosamente escolhido para satisfazer a grande maioria dos casos e também é visualmente agradável. Como resultado, é possível criar gráficos informativos e elegantes com código relativamente simples e legível. Uma limitação de __ggplot2_é que ele foi projetado para funcionar exclusivamente com tabelas de dados no formato_tidy (onde linhas são observações e colunas são variáveis). No entanto, uma porcentagem substancial dos conjuntos de dados com os quais os iniciantes trabalham estão neste formato ou podem ser convertidos para ele. Uma vantagem dessa abordagem é que, desde que nossos dados sejam arrumados, ggplot2 simplifica o aprendizado de código e gramática de gráficos para uma variedade de gráficos. Para usar ggplot2, eles terão que aprender várias funções e argumentos. Como é difícil memorizar, recomendamos que você tenha a folha de referência do ggplot2 à mão. Você pode obter uma cópia aqui: [https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf…………….(https://www.rstudio.com/wp-content/uploads/ 2015/03/ ggplot2-cheatsheet.pdf) ou simplesmente faça uma pesquisa na Internet por “ggplot2 cheat sheet”. 7.1 Os componentes de um gráfico Construiremos um gráfico que resume o conjunto de dados de assassinatos por armas nos Estados Unidos da seguinte forma: Podemos ver claramente quanto os estados variam de acordo com o tamanho da população e o número total de assassinatos. Não é de surpreender que também exista uma relação clara entre o total de assassinatos e o tamanho da população. Um estado que cai na linha tracejada cinza tem a mesma taxa de homicídios que a média dos EUA. As quatro regiões geográficas são indicadas com cores, o que mostra como a maioria dos estados do sul tem taxas de homicídio acima da média. Essa visualização de dados mostra praticamente todas as informações na tabela de dados. O código necessário para criar o gráfico é relativamente simples. Vamos aprender a criá-lo parte por parte. O primeiro passo para aprender ggplot2 é ser capaz de separar um gráfico em componentes. Começaremos examinando o gráfico acima e introduzindo algumas das terminologias de ggplot2. Os três principais componentes a serem considerados são: Data: O conjunto de dados dos canhões de armas dos EUA está sendo resumido. Nós nos referimos a isso como o componente data. Geometria: O gráfico acima é um diagrama de dispersão. Isso é chamado de componente geometry. Outras geometrias possíveis são diagrama de barras, histograma, densidades suaves, gráfico QQ e diagrama de caixa. Mapeamento estético: o gráfico usa várias dicas visuais para representar as informações fornecidas pelo conjunto de dados. Os dois sinais mais importantes neste gráfico são as posições dos pontos no eixo xe no eixo y, representando o tamanho da população e o número total de assassinatos, respectivamente. Cada ponto representa uma observação diferente e mapeamos os dados dessas observações e as pistas visuais nas escalas x e y. A cor é outra dica visual que atribuímos à região. Nós nos referimos a isso como o componente mapeamento estético. Como definimos o mapeamento depende de qual geometria estamos usando. Também observamos que: Os pontos são rotulados com abreviações de estado. O intervalo do eixo xe o eixo y parecem ser definidos pelo intervalo dos dados. Ambos estão em escalas logarítmicas. Existem rótulos, um título, uma lenda e usamos o estilo da revista “The Economist”. Agora vamos construir o gráfico parte por parte. Vamos começar carregando o conjunto de dados: library(dslabs) data(murders) 7.2 objetos ggplot A primeira etapa na criação de um gráfico ggplot2 é definir um objeto ggplot. Fazemos isso com a função ggplot, que inicializa o gráfico. Se lermos a página de ajuda para esta função, veremos que o primeiro argumento é usado para especificar quais dados estão associados a este objeto: ggplot(data = murders) Também podemos pipe os dados como o primeiro argumento. Portanto, essa linha de código é equivalente à anterior: murders %&gt;% ggplot() O código cria um gráfico, neste caso, um quadro em branco, pois a geometria não foi definida. A única opção de estilo que vemos é um fundo cinza. O que aconteceu é que o objeto foi criado e, por não ter sido designado, foi automaticamente avaliado. Mas podemos atribuir nosso gráfico a um objeto, por exemplo: p &lt;- ggplot(data = murders) class(p) #&gt; [1] &quot;gg&quot; &quot;ggplot&quot; Para representar o gráfico associado a este objeto, simplesmente imprimimos o objeto p. Cada uma das duas linhas de código a seguir produz o mesmo gráfico que vemos acima: print(p) p 7.3 Geometrias No ggplot2 criamos gráficos adicionando layers (layers em inglês). As camadas podem definir geometrias, calcular estatísticas de resumo, definir quais escalas (scales em inglês) usar ou até alterar estilos. Para adicionar camadas, usamos o símbolo +. Em geral, uma linha de código ficará assim: DADOS%&gt;% ggplot() + CAMADA 1 + CAMADA 2 + … + CAMADA N Geralmente, a primeira camada que adicionamos define a geometria. Queremos fazer um diagrama de dispersão. Que geometria devemos usar? Observando rapidamente a folha de referência, vemos que a função usada para criar gráficos com essa geometria é geom_point. (Imagem cortesia de RStudio24. Licença CC-BY-4.025.) Os nomes das funções de geometria seguem o padrão: geom_X onde X é o nome da geometria. Alguns exemplos incluem geom_point, geom_bar e geom_histogram. Para que geom_point funciona bem, precisamos fornecer dados e uma correspondência. Nós já conectamos o objeto p com tabela de dados murders e se adicionarmos a camada geom_point, isso por padrão usa os dados do assassinato. Para descobrir quais correspondências são esperadas, leia a seção Estética da página de ajuda de geom_point: &gt; Aesthetics &gt; &gt; geom_point understands the following aesthetics (required aesthetics are in bold): &gt; &gt; x &gt; &gt; y &gt; &gt; alpha &gt; &gt; colour e, como esperado, vemos que pelo menos dois argumentos são necessários x e y. 7.4 Mapeamentos estéticos Os __mapeamentos estéticos_ descrevem como as propriedades dos dados estão conectadas às características do gráfico, como a distância ao longo de um eixo, o tamanho ou a cor. A função aes conecta os dados com o que vemos no gráfico, definindo atribuições estéticas e essa será uma das funções que mais serão usadas na representação gráfica. O resultado da função aes é frequentemente usado como argumento para uma função de geometria. Este exemplo produz um gráfico de dispersão do total de assassinatos versus população em milhões: murders %&gt;% ggplot() + geom_point(aes(x = population/10^6, y = total)) Nós podemos remover o x = e y = se quiséssemos, pois esse é o primeiro e o segundo argumento esperado, como visto na página de ajuda. Em vez de definir nosso gráfico do zero, também podemos adicionar uma camada ao objeto p que foi definido anteriormente como p &lt;- ggplot(data = murders): p + geom_point(aes(population/10^6, total)) A escala e os rótulos são definidos por padrão ao adicionar essa camada. Como as funções dplyr, aes ele também usa os nomes de variáveis do componente do objeto: podemos usar population e total sem ter que chamá-los como assassinatos $population and murders$ total NA a é NA população NA total NA aes, eles receberão um erro. 7.5 Camadas Uma segunda camada no gráfico que queremos criar envolve adicionar um rótulo a cada ponto para identificar o estado. As funções geom_label e geom_text nos permite adicionar texto ao gráfico com ou sem um retângulo atrás do texto, respectivamente. Como cada ponto (cada estado neste caso) possui um rótulo, precisamos de um mapeamento estético para fazer a conexão entre os pontos e os rótulos. Lendo a página de ajuda, aprendemos que o mapeamento entre o ponto e o rótulo é fornecido através do argumento label do aes. Portanto, o código fica assim: p + geom_point(aes(population/10^6, total)) + geom_text(aes(population/10^6, total, label = abb)) Adicionamos com sucesso uma segunda camada ao gráfico. Como exemplo do comportamento único de aes mencionado acima, observe que esta chamada: p_test &lt;- p + geom_text(aes(population/10^6, total, label = abb)) ok enquanto esta chamada: p_test &lt;- p + geom_text(aes(population/10^6, total), label = abb) lhe dará um erro desde abb não foi encontrado porque está fora de funcionamento aes. A capa geom_text não sei onde encontrar abb porque é um nome de coluna e não uma variável global. 7.5.1 Como testar vários argumentos p + geom_point(aes(population/10^6, total), size = 3) + geom_text(aes(population/10^6, total, label = abb)) size no é um mapeamento: enquanto os mapeamentos usam dados de observações específicas e precisam estar dentro aes(), as operações que queremos afetar todos os pontos da mesma maneira não precisam ser incluídas dentro aes. Agora, como os pontos são maiores, é difícil ver os rótulos. Se lermos a página de ajuda para geom_text, vemos que o argumento nudge_x mova o texto levemente para a direita ou esquerda: p + geom_point(aes(population/10^6, total), size = 3) + geom_text(aes(population/10^6, total, label = abb), nudge_x = 1.5) Isso é preferível, pois facilita a leitura do texto. Na seção 7.11 aprenderemos uma maneira melhor de garantir que possamos ver os pontos e os rótulos. 7.6 Mapeamento estético global versus local Na linha de código anterior, definimos o mapeamento aes(population/10^6, total) duas vezes, uma vez em cada geometria. Podemos evitar isso usando um mapeamento estético global quando definimos o quadro em branco que o objeto nos fornece ggplot. Lembre-se de que a função ggplot ele contém um argumento que nos permite definir mapeamentos estéticos: args(ggplot) #&gt; function (data = NULL, mapping = aes(), ..., environment = parent.frame()) #&gt; NULL Se definirmos um mapeamento em ggplot, todas as geometrias adicionadas como camadas serão atribuídas por padrão a esse mapeamento. Redefinimos p: p &lt;- murders %&gt;% ggplot(aes(population/10^6, total, label = abb)) e então podemos escrever o código a seguir para produzir o gráfico acima: p + geom_point(size = 3) + geom_text(nudge_x = 1.5) Mantemos os argumentos size e nudge_x no geom_point e geom_text, respectivamente, porque queremos aumentar apenas o tamanho dos pontos e ajustar a posição (nudge em inglês) dos rótulos. Se colocarmos esses argumentos em aes, eles serão aplicados aos dois gráficos. Observe também que a função geom_point não precisa de discussão label e, portanto, ignora essa estética. Se necessário, podemos substituir o mapeamento global definindo um novo mapeamento dentro de cada camada. Essas definições local_substituem o_global. Aqui está um exemplo: p + geom_point(size = 3) + geom_text(aes(x = 10, y = 800, label = &quot;Hello there!&quot;)) Claramente, a segunda chamada para geom_text não use population e total. 7.7 Escalas Primeiro, as escalas que queremos estão em uma escala logarítmica. Esse não é o padrão, portanto, essa alteração precisa ser adicionada por meio de uma camada scale. Uma rápida olhada na folha de referência revela que a função scale_x_continuous nos permite controlar o comportamento das escalas. Usamos assim: p + geom_point(size = 3) + geom_text(nudge_x = 0.05) + scale_x_continuous(trans = &quot;log10&quot;) + scale_y_continuous(trans = &quot;log10&quot;) Como agora estamos na escala logarítmica, o ajuste na posição deve ser menor. Essa transformação específica é tão comum que ggplot2 oferece duas funções especializadas scale_x_log10 e scale_y_log10, que podemos usar para reescrever o código assim: p + geom_point(size = 3) + geom_text(nudge_x = 0.05) + scale_x_log10() + scale_y_log10() 7.8 Tags e títulos Da mesma forma, a folha de referência revela que, para alterar os rótulos e adicionar um título, usamos as seguintes funções: p + geom_point(size = 3) + geom_text(nudge_x = 0.05) + scale_x_log10() + scale_y_log10() + xlab(&quot;Populations in millions (log scale)&quot;) + ylab(&quot;Total number of murders (log scale)&quot;) + ggtitle(&quot;US Gun Murders in 2010&quot;) Estamos quase terminando! A única coisa que precisamos é adicionar cor, legenda e alterações opcionais ao estilo. 7.9 Categorias como cores Podemos mudar a cor dos pontos usando o argumento col na função geom_point. Para facilitar a demonstração de novos recursos, redefiniremos p para ser tudo, exceto a camada de pontos: p &lt;- murders %&gt;% ggplot(aes(population/10^6, total, label = abb)) + geom_text(nudge_x = 0.05) + scale_x_log10() + scale_y_log10() + xlab(&quot;Populations in millions (log scale)&quot;) + ylab(&quot;Total number of murders (log scale)&quot;) + ggtitle(&quot;US Gun Murders in 2010&quot;) e depois testaremos o que acontece quando adicionamos chamadas diferentes a geom_point. Por exemplo, podemos deixar todos os pontos em azul adicionando o argumento color: p + geom_point(size = 3, color =&quot;blue&quot;) No entanto, não queremos isso. Queremos atribuir cores de acordo com a região geográfica. Um bom comportamento padrão para ggplot2 é que, se atribuirmos uma variável categórica à cor, ela atribuirá automaticamente uma cor diferente a cada categoria, além de uma legenda. Como a escolha da cor é determinada por uma característica de cada observação, esse é um mapeamento estético. Para atribuir uma cor a cada ponto, precisamos usar aes. Usamos o seguinte código: p + geom_point(aes(col=region), size = 3) Os mapeamentos x e y são herdados daqueles já definidos em p então nós não os redefinimos. Nós também nos mudamos aes para o primeiro argumento, pois é aí que os mapeamentos são esperados nesta chamada. Aqui está outro comportamento padrão útil: ggplot2 adiciona automaticamente uma legenda que atribui a cor à região. Para evitar adicionar essa legenda, definimos o argumento geom_point como show.legend = FALSE. 7.10 Anotação, formas e ajustes Muitas vezes, queremos adicionar formas ou anotações às figuras que não são derivadas diretamente do mapeamento estético; alguns exemplos incluem etiquetas, caixas, áreas sombreadas e linhas. Aqui, queremos adicionar uma linha que represente a taxa média de homicídios em todo o país. Depois de determinar a taxa por milhão a ser \\(r\\), esta linha é definida pela fórmula: \\(y = r x\\) com \\(y\\) e \\(x\\) nossos eixos: total de assassinatos e população em milhões, respectivamente. Na escala logarítmica, essa linha se torna: \\(\\log(y) = \\log(r) + \\log(x)\\). Então, no nosso gráfico, é uma linha com a inclinação 1 e intercepta \\(\\log(r)\\). Para calcular esse valor, usamos nosso conhecimento de dplyr: r &lt;- murders %&gt;% summarize(rate = sum(total)/ sum(population) * 10^6) %&gt;% pull(rate) Para adicionar uma linha, usamos a função geom_abline. ggplot2 usa ab no nome para nos lembrar de que estamos fornecendo a interceptação ( a) e o brinco ( b) A linha padrão possui a inclinação 1 e intercepta 0, portanto, apenas precisamos definir a interceptação: p + geom_point(aes(col=region), size = 3) + geom_abline(intercept = log10(r)) Aqui geom_abline não usa nenhuma informação do objeto de dados. Podemos alterar o tipo de linha e a cor das linhas usando argumentos. Além disso, primeiro o desenhamos para que não cubra nossos pontos. p &lt;- p + geom_abline(intercept = log10(r), lty = 2, color = &quot;darkgrey&quot;) + geom_point(aes(col=region), size = 3) Observe que redefinimos p e vamos usar esse novo p abaixo e na próxima seção. Os gráficos padrão criados por ggplot2 já são muito úteis. No entanto, geralmente precisamos fazer pequenos ajustes no comportamento padrão. Embora nem sempre seja óbvio como fazer isso, mesmo com a folha de referência, ggplot2 é muito flexível. Por exemplo, podemos fazer alterações na legenda através da função scale_color_discrete. Em nosso gráfico original, a palavra region está em maiúscula e podemos alterá-la assim: p &lt;- p + scale_color_discrete(name = &quot;Region&quot;) 7.11 Pacotes complementares O poder do ggplot2 aumentou ainda mais devido à disponibilidade de pacotes adicionais. As alterações restantes necessárias para dar os retoques finais em nosso gráfico exigem os pacotes __ggthemes__eggrepel. O estilo de um gráfico ggplot2 pode ser alterado usando as funções de theme. Vários temas (themes em inglês) estão incluídos como parte do pacote ggplot2. De fato, para a maioria dos gráficos deste livro, usamos uma função do pacote dslabs que define automaticamente um tema padrão: ds_theme_set() O pacote ggthemes adiciona muitos outros temas, incluindo o tema theme_economist que nós escolhemos. Após a instalação do pacote, eles podem alterar o estilo adicionando uma camada como a seguinte: library(ggthemes) p + theme_economist() Você pode ver como são os outros temas simplesmente alterando a função. Por exemplo, eles podem testar o tema theme_fivethirtyeight() em vez do anterior. A diferença final tem a ver com a posição dos rótulos. No nosso gráfico, alguns dos rótulos se sobrepõem. O pacote de plug-ins ggrepel inclui uma geometria que adiciona rótulos, garantindo que eles não se sobreponham. Para usá-lo, simplesmente mudamos geom_text para geom_text_repel. 7.12 Como combinar tudo Agora que terminamos o teste, podemos escrever um código que produza o gráfico desejado do zero. library(ggthemes) library(ggrepel) r &lt;- murders %&gt;% summarize(rate = sum(total)/ sum(population) * 10^6) %&gt;% pull(rate) murders %&gt;% ggplot(aes(population/10^6, total, label = abb)) + geom_abline(intercept = log10(r), lty = 2, color = &quot;darkgrey&quot;) + geom_point(aes(col=region), size = 3) + geom_text_repel() + scale_x_log10() + scale_y_log10() + xlab(&quot;Populations in millions (log scale)&quot;) + ylab(&quot;Total number of murders (log scale)&quot;) + ggtitle(&quot;US Gun Murders in 2010&quot;) + scale_color_discrete(name = &quot;Region&quot;) + theme_economist() 7.13 Gráficos rápidos com qplot Nós aprendemos as poderosas técnicas ggplot para gerar visualizações. No entanto, há casos em que precisamos apenas de um gráfico rápido de, por exemplo, um histograma dos valores em um vetor, um gráfico de dispersão dos valores em dois vetores ou um gráfico de caixa usando vetores categóricos e numéricos. Já demonstramos como gerar esses gráficos com hist, plot e boxplot. No entanto, se queremos ser consistentes com o estilo ggplot, podemos usar a função qplot. Se tivermos valores em dois vetores como: data(murders) x &lt;- log10(murders$population) y &lt;- murders$total e queremos fazer um diagrama de dispersão com ggplot, teríamos que escrever algo como: data.frame(x = x, y = y) %&gt;% ggplot(aes(x, y)) + geom_point() Parece ser muito código para um gráfico tão simples. A função qplot sacrifica a flexibilidade oferecida pelo ggplot, mas permite gerar rapidamente um gráfico. qplot(x, y) Vamos aprender mais sobre qplot na seção 8.16 7.14 Grades de gráfico Muitas vezes, temos que colocar gráficos próximos um do outro. O pacote gridExtra nos permite fazer isso: library(gridExtra) p1 &lt;- qplot(x) p2 &lt;- qplot(x,y) grid.arrange(p1, p2, ncol = 2) 7.15 Exercícios Comece carregando os pacotes __dplyr__eggplot2, bem como os dados murders e heights. library(dplyr) library(ggplot2) library(dslabs) data(heights) data(murders) 1. Com ggplot2, os gráficos podem ser salvos como objetos. Por exemplo, podemos associar um conjunto de dados a um objeto gráfico da seguinte maneira: p &lt;- ggplot(data = murders) Quão data é o primeiro argumento, não precisamos explicá-lo: p &lt;- ggplot(murders) e também podemos usar o pipe: p &lt;- murders %&gt;% ggplot() Qual é a classe do objeto p? 2. Lembre-se de que para imprimir um objeto, você pode usar o comando print ou apenas escreva o objeto. Imprimir o objeto p definido no exercício um e descreva o que você vê. para. Nada acontece. b. Um gráfico de quadro branco em branco. c. Um diagrama de dispersão. d. Um histograma. 3. Usando o pipe %&gt;% criar um objeto p mas desta vez associado ao conjunto de dados heights em vez do conjunto de dados murders. 4. Qual é a classe do objeto p o que você acabou de criar? 5. Agora vamos adicionar uma camada e os mapeamentos estéticos correspondentes. Para os dados de assassinatos, plotamos o total de assassinatos versus o tamanho da população. Explore o conjunto de dados murders para lembrar quais são os nomes dessas duas variáveis e escolher a resposta correta. Hint: Look ?murders. para. state e abb b. total_murders e population_size c. total e population d. murders e size 6. Para criar o gráfico de dispersão, adicionamos uma camada com geom_point. O mapeamento estético requer que definamos as variáveis do eixo xe do eixo y, respectivamente. Portanto, o código fica assim: murders %&gt;% ggplot(aes(x = , y = )) + geom_point() exceto que temos que definir as duas variáveis x e y. Preencha o espaço com os nomes corretos das variáveis. 7. Lembre-se de que, se não usarmos nomes de argumentos, podemos obter o mesmo gráfico se inserirmos os nomes das variáveis na ordem correta, assim: murders %&gt;% ggplot(aes(population, total)) + geom_point() Refaça o gráfico, mas agora com total no eixo x e população no eixo y. 8. Se, em vez de pontos, queremos adicionar texto, podemos usar as geometrias geom_text() ou geom_label(). O código a seguir: murders %&gt;% ggplot(aes(population, total)) + geom_label() nos dará a mensagem de erro: Error: geom_label requires the following missing aesthetics: label Por que isso acontece? para. Precisamos mapear um caractere para cada ponto através do argumento label em aes. b. Precisamos deixar geom_label saber qual caractere usar no gráfico. c. A geometria geom_label não requer valores do eixo xe do eixo y. d. geom_label não é um comando de ggplot2. 9. Reescreva o código acima para usar a abreviação como o rótulo via aes. 10. Mude a cor dos rótulos para azul. Como se faz? para. Adicionando uma coluna chamada blue para murders. b. Como cada etiqueta precisa de uma cor diferente, mapeamos as cores através aes. c. Usando o argumento color no ggplot. d. Como queremos que todas as cores sejam azuis, não precisamos atribuir cores, basta usar o argumento de cores em geom_label. Onze. Reescreva o código acima para que os rótulos fiquem azuis. 12. Agora, suponha que desejamos usar cores para representar as diferentes regiões. Nesse caso, qual das seguintes opções é a mais apropriada? para. Adicione uma coluna chamada color para murders com a cor que queremos usar. b. Como cada tag precisa de uma cor diferente, mapeie as cores através do argumento de cores de aes . c. Use o argumento color no ggplot. d. Como queremos que todas as cores sejam azuis, não precisamos atribuir cores, basta usar o argumento de cores em geom_label. 13. Reescreva o código acima para que a cor dos rótulos seja determinada pela região do estado. 14. Agora vamos mudar o eixo x para uma escala logarítmica para levar em conta o fato de que a distribuição da população é assimétrica. Vamos começar definindo um objeto p salvando o gráfico que criamos até agora: p &lt;- murders %&gt;% ggplot(aes(population, total, label = abb, color = region)) + geom_label() Para alterar o eixo y para uma escala logarítmica, aprendemos sobre a função scale_x_log10(). Adicione esta camada ao objeto p para alterar a escala e criar o gráfico. Quinze. Repita o exercício anterior, mas agora mude os dois eixos para que estejam na escala logarítmica. 16. Agora edite o código acima para adicionar o título “Dados de assassinatos por armas” ao argumento. Dica: use a função ggtitle. https://ggplot2.tidyverse.org/↩ http://www.springer.com/us/book/9780387245447↩ https://github.com/rstudio/cheatsheets↩ https://github.com/rstudio/cheatsheets/blob/master/LICENSE↩ "],
["distributions.html", "Capítulo 8 Como visualizar distribuições de dados 8.1 Tipos de variáveis 8.2 Estudo de caso: descrevendo a altura dos alunos 8.3 A função de distribuição 8.4 Funções de distribuição cumulativa 8.5 Histogramas 8.6 Densidade suave 8.7 Exercícios 8.8 A distribuição normal 8.9 Unidades padrão 8.10 Gráficos QQ 8.11 Percentis 8.12 Diagramas de caixas 8.13 Estratificação 8.14 Estudo de caso: descrição das alturas dos alunos (continuação) 8.15 Exercícios 8.16 Gegplot2 geometrias 8.17 Exercícios", " Capítulo 8 Como visualizar distribuições de dados Os dados numéricos são frequentemente resumidos com o valor average. Por exemplo, a qualidade de uma escola secundária às vezes é resumida em um único número: a pontuação média em um teste padronizado. Ocasionalmente, um segundo número é incluído: o desvio padrão. Por exemplo, eles podem ler um relatório de que as pontuações foram 680 mais ou menos 50 (o desvio padrão). O relatório resumiu um vetor completo de pontuações com apenas dois números. Isso é apropriado? Existe alguma informação importante que não estamos considerando ao exibir este resumo em vez da lista completa? Nosso primeiro componente básico de visualização de dados está aprendendo a resumir listas de fatores ou vetores de números. Geralmente, a melhor maneira de compartilhar ou explorar este resumo é através da visualização de dados. O resumo estatístico mais básico de uma lista de objetos ou números é sua distribuição. Depois que um vetor é resumido como uma distribuição, existem várias técnicas de visualização de dados para transmitir efetivamente essas informações. Neste capítulo, discutiremos primeiro as propriedades de uma variedade de distribuições e como visualizá-las usando um exemplo motivador de altura dos alunos. Então na Seção 8.16, discutiremos as geometrias ggplot2 para essas visualizações. 8.1 Tipos de variáveis Trabalharemos com dois tipos de variáveis: categórica e numérica. Cada um pode ser dividido em dois outros grupos: as variáveis categóricas podem ser ordinais ou não, enquanto as numéricas podem ser discretas ou contínuas. Quando cada entrada em um vetor vem de um pequeno número de grupos, nos referimos aos dados como dados categóricos. Dois exemplos simples são sexo (masculino ou feminino) e regiões (nordeste, sul, norte central, oeste). Alguns dados categóricos podem ser solicitados, mesmo que não sejam números, por exemplo, quão picante é um alimento (pouco, médio, muito). Nos livros de estatística, os dados categóricos ordenados são chamados ordinal data. Exemplos de dados numéricos são tamanho da população, taxas de homicídios e altura. Alguns dados numéricos podem ser tratados como ordenadas categóricas. Podemos dividir ainda mais os dados numéricos em contínuos e discretos. Variáveis contínuas são aquelas que podem assumir qualquer valor, como alturas, se forem medidas com precisão suficiente. Por exemplo, um par de botões de punho pode medir 68,12 e 68,11 polegadas, respectivamente. As contagens, como o tamanho da população, são discretas porque precisam ser números redondos. Observe que dados numéricos discretos podem ser considerados ordinais. Embora isso seja tecnicamente verdadeiro, geralmente reservamos o termo dados ordinais para variáveis que pertencem a um pequeno número de grupos diferentes, e cada grupo tem muitos membros. Por outro lado, quando temos muitos grupos com poucos casos em cada grupo, geralmente nos referimos a eles como variáveis numéricas discretas. Assim, por exemplo, o número de maços de cigarro que uma pessoa fuma por dia, arredondado para o maço mais próximo, seria considerado ordinal, enquanto o número real de cigarros seria considerado uma variável numérica. No entanto, existem exemplos que podem ser considerados numéricos e ordinais quando se trata de visualizar dados. 8.2 Estudo de caso: descrevendo a altura dos alunos Aqui, apresentamos um novo problema motivador. É artificial, mas nos ajudará a ilustrar os conceitos necessários para entender as distribuições. Imagine que precisamos descrever as alturas de nossos colegas de classe para ET, um alienígena que nunca viu seres humanos. Como primeiro passo, precisamos coletar dados. Para fazer isso, pedimos aos alunos que indiquem suas alturas em polegadas. Pedimos que você nos forneça informações sobre o seu sexo biológico, porque sabemos que existem duas distribuições diferentes por sexo. Coletamos os dados e salvamos no conjunto de dados heights: library(tidyverse) library(dslabs) data(heights) Uma maneira de passar as alturas para o ET é simplesmente enviar a ele esta lista de 1,050 alturas. No entanto, existem maneiras muito mais eficazes de transmitir informações e, para isso, nos ajudarão a entender o conceito de distribuição. Para simplificar a explicação, primeiro nos concentramos nas alturas masculinas. Examinamos os dados de altura feminina na Seção 8.14. 8.3 A função de distribuição Acontece que, em alguns casos, o desvio médio e padrão são praticamente tudo o que precisamos para entender os dados. Aprenderemos técnicas de visualização de dados que nos ajudarão a determinar quando esse resumo de dois números é apropriado. Essas mesmas técnicas servirão como uma alternativa para quando dois números não forem suficientes. O resumo estatístico mais básico de uma lista de objetos ou números é sua distribuição. A maneira mais simples de pensar em uma distribuição é como uma descrição compacta de uma lista com muitas entradas. Este conceito não deve ser novo para os leitores deste livro. Por exemplo, com dados categóricos, a distribuição simplesmente descreve a proporção de cada categoria exclusiva. O sexo representado no conjunto de dados de altura é: #&gt; #&gt; Female Male #&gt; 0.227 0.773 Essa frequency table de duas categorias é a forma mais simples de uma distribuição. Nós realmente não precisamos visualizá-lo, pois um número descreve tudo o que precisamos saber: 23% são mulheres e o restante são homens. Quando há mais categorias, um gráfico de barras simples descreve a distribuição. Aqui está um exemplo com as regiões estaduais dos EUA. EUA: Este gráfico simplesmente mostra quatro números, um para cada categoria. Geralmente usamos gráficos de barras quando temos poucos números. Embora esse gráfico em particular não forneça muito mais informações do que uma tabela de frequência, ele é um excelente exemplo de como convertemos um vetor em um gráfico que resume sucintamente todas as informações no vetor. Quando os dados são numéricos, a tarefa de exibir distribuições é mais desafiadora. 8.4 Funções de distribuição cumulativa Dados numéricos que não são categóricos também têm distribuições. Em geral, quando os dados não são categóricos, a indicação da frequência de cada entrada não é um resumo efetivo, pois a maioria das entradas é exclusiva. Em nosso estudo de caso, por exemplo, enquanto vários estudantes relataram uma altura de 68 polegadas, um estudante relatou uma altura de 68.503937007874 polegadas e outra uma altura 68.8976377952756 polegadas. Supomos que eles tenham convertido suas alturas em 174 e 175 centímetros, respectivamente. Os livros de estatística nos ensinam que uma maneira mais útil de definir uma distribuição numérica de dados é definir uma função que indica a proporção dos dados abaixo \\(a\\) para todos os valores possíveis de \\(a\\). Essa função é chamada de função de distribuição cumulativa, ou CDF, para abreviar. Nas estatísticas, a seguinte notação é usada: \\[ F(a) = \\mbox{Pr}(x \\leq a) \\] Aqui vemos um gráfico de \\(F\\) para dados de altura masculina: Semelhante ao que a tabela de frequência faz para dados categóricos, o CDF define a distribuição de dados numéricos. No gráfico, podemos ver que 16% dos valores são menores que 65, pois \\(F(66)=\\) 0.164, o que r round (ecdf(heights$height[heights$ sex == &quot;Male&quot;]) (72) * 100)% dos valores são menores que 72, pois \\(F(72)=\\) 0.841 e assim. De fato, podemos relatar a proporção de valores entre duas alturas, digamos \\(a\\) e \\(b\\), ao computar \\(F(b) - F(a)\\). Isso significa que, se enviarmos esse diagrama para o ET, ele terá todas as informações necessárias para reconstruir a lista completa. Parafraseando a expressão “uma imagem vale mais que mil palavras”, neste caso, uma imagem é tão informativa quanto 812 números. Uma observação final: como os CDFs podem ser matematicamente definidos, a palavra empirical é adicionada para distinguir quando os dados são usados. Portanto, usamos o termo CDF empírico ou eCDF para abreviar. 8.5 Histogramas Embora o conceito de CDF seja amplamente discutido nos livros estatísticos, o gráfico não é muito popular na prática. O principal motivo é que ele não transmite facilmente características de interesse, como: Em que valor a distribuição se concentra? A distribuição é simétrica? Quais intervalos contêm 95% dos valores? Os histogramas são preferidos porque facilitam muito a resposta a essas perguntas. Os histogramas sacrificam apenas um pouco de informação para produzir gráficos muito mais fáceis de interpretar. A maneira mais fácil de criar um histograma é dividir o span_dos nossos dados em_bins_não sobrepostos do mesmo tamanho. Então, para cada compartimento, contamos o número de valores que estão nesse intervalo. O histograma representa graficamente essas contagens como barras com a base da barra definida pelos intervalos. Aqui está o histograma para os dados de altura que dividem o intervalo de valores em intervalos de uma polegada: \\([49.5, 50.5], [51.5,52.5],(53.5,54.5],...,(82.5,83.5]\\) Como você pode ver na figura acima, um histograma é semelhante a um gráfico de barras, mas difere no fato de o eixo x ser numérico, não categórico. Se enviarmos esse diagrama para o ET, ele aprenderá imediatamente alguns detalhes importantes sobre nossos dados. Primeiro, o intervalo de dados é de 50 a 84, com a maioria (acima de 95%) entre 63 e 75 polegadas. Além disso, as alturas são quase simétricas em torno de 69 polegadas. Finalmente, adicionando contagens, o ET pode obter uma aproximação muito boa da proporção dos dados em qualquer intervalo. Portanto, o histograma acima não é apenas fácil de interpretar, mas também oferece quase todas as informações contidas na lista bruta de 812 alturas com aproximadamente 30 contagens, uma para cada compartimento. Que informação perdemos? Lembre-se de que todos os valores em cada intervalo são tratados da mesma maneira ao calcular as alturas dos compartimentos. Por exemplo, o histograma não distingue entre 64, 64,1 e 64,2 polegadas. Como essas diferenças são quase imperceptíveis aos olhos, as implicações práticas são desprezíveis e conseguimos resumir os dados em apenas 23 números. Discutimos como codificar histogramas na Seção 8.16. 8.6 Densidade suave Os gráficos de densidade suave são esteticamente mais atraentes que os histogramas. Abaixo, vemos um gráfico de densidade suave para nossos dados de altura: Neste gráfico, não temos mais arestas vivas nos limites do intervalo e muitos dos picos locais foram removidos. Além disso, a escala do eixo y mudou de contagens para densidade. Para entender densidades suaves, precisamos entender estimations, um tópico que não abordamos até mais tarde. No entanto, oferecemos uma explicação heurística para ajudá-lo a entender o básico, para que você possa usar esta útil ferramenta de visualização de dados. O principal novo conceito que você precisa entender é que assumimos que nossa lista de valores observados é um subconjunto de uma lista muito maior de valores não observados. No caso de alturas, você pode imaginar que nossa lista de 812 os estudantes do sexo masculino vêm de uma lista hipotética que contém todas as alturas de todos os estudantes do mundo todo, todos medidos com grande precisão. Digamos que existem 1.000.000 dessas medidas. Essa lista de valores tem uma distribuição, como qualquer lista de valores, e essa distribuição considerável é realmente o que queremos transmitir ao ET, pois é muito mais geral. Infelizmente, não podemos ver essa lista enorme. No entanto, podemos fazer uma suposição que pode nos ajudar a aproximar isso. Se tivéssemos 1.000.000 valores, medidos com muita precisão, poderíamos fazer um histograma com compartimentos muito, muito pequenos. A suposição é que, se mostrarmos isso, a altura dos compartimentos consecutivos será semelhante. Aqui está o que entendemos por macio: não temos grandes saltos nas alturas de compartimentos consecutivos. Aqui está um histograma hipotético com compartimentos de tamanho 1: Quanto menores os compartimentos, mais suave o histograma se torna. Aqui estão os histogramas com larguras de compartimento de 1, 0,5 e 0,1: A densidade suave é basicamente a curva que atravessa a parte superior das barras do histograma quando os compartimentos são muito, muito pequenos. Para que a curva não dependa do tamanho hipotético da lista hipotética, calculamos a curva usando frequências em vez de contagens: Agora de volta à realidade. Não temos milhões de medições. Em vez disso, temos 812 e não podemos fazer um histograma com compartimentos muito pequenos. Portanto, fazemos um histograma, usando tamanhos de compartimento apropriados para nossos dados e frequências de computação em vez de contagens. Além disso, desenhamos uma curva suave que passa pela parte superior das barras do histograma. Os gráficos a seguir mostram as etapas que levam a uma densidade suave: No entanto, lembre-se de que soft (smooth em inglês) é um termo relativo. De fato, podemos controlar a suavidade da curva alterando o número de pontos nos compartimentos. Essa opção é conhecida como bandwidth, span, window size ou, em espanhol, smoothing window ou smoothing parameter e pode ser ajustada na função que calcula a curva de densidade suave. Aqui estão dois exemplos que usam diferentes níveis de suavidade no mesmo histograma: Precisamos tomar essa decisão com cuidado, pois as visualizações resultantes podem alterar nossa interpretação dos dados. Devemos selecionar um grau de suavidade que possamos defender como representativo dos dados subjacentes. No caso da altura, realmente temos motivos para acreditar que a proporção de pessoas com alturas semelhantes deve ser a mesma. Por exemplo, a taxa de 72 polegadas deve ser mais semelhante à taxa de 71 do que a taxa de 78 ou 65. Isso implica que a curva deve ser razoavelmente suave; isto é, a curva deve se parecer mais com o exemplo à direita do que com a esquerda. Embora o histograma seja um resumo sem suposições, a densidade suavizada é baseada em algumas suposições. 8.6.1 Como interpretar o eixo y Observe que a interpretação do eixo y de um gráfico de densidade suave não é óbvia. Ele é dimensionado para que a área sob a curva de densidade seja igual a 1. Se você imaginar que formamos um compartimento com uma base de 1 unidade de comprimento, o valor do eixo y indica a proporção de valores nesse compartimento. No entanto, isso é válido apenas para compartimentos de tamanho 1. Para intervalos de outro tamanho, a melhor proporção de dados nesse intervalo é determinada calculando a proporção da área total contida nesse intervalo. Por exemplo, aqui vemos a proporção de valores entre 65 e 68: A proporção dessa área é de aproximadamente 0.3, o que significa que aproximadamente NaN%das alturas dos machos estão entre 65 e 68 polegadas. Ao entender isso, estamos prontos para usar a densidade suave como um resumo. Para esse conjunto de dados, nos sentimos confortáveis em assumir suavidade e, assim, compartilhar essa figura esteticamente agradável com o ET, que pode usá-lo para entender nossos dados de altura masculinos: 8.6.2 Densidades permitem camadas Como nota final, observamos que uma vantagem das densidades suaves sobre os histogramas para fins de exibição é que as densidades facilitam a comparação de duas distribuições. Isso se deve em grande parte às bordas irregulares do histograma, que agregam confusão. Aqui está um exemplo comparando as alturas masculina e feminina: Com o argumento correto, ggplot sombreia automaticamente a região de interseção com uma cor diferente. Mostraremos amostras de código ggplot2 para densidades na Seção 9 bem como na Seção 8.16. 8.7 Exercícios 1. No conjunto de dados murders, a região é uma variável categórica e sua distribuição é a seguinte: Arredondando para os 5% mais próximos, qual a proporção de estados na região “Norte Central”? 2. Qual dos seguintes é verdadeiro? para. O gráfico acima é um histograma. b. O gráfico acima mostra apenas quatro números com um gráfico de barras. c. As categorias não são números, portanto, não faz sentido representar graficamente a distribuição. d. Cores, não a altura das barras, descrevem a distribuição. 3. O gráfico a seguir mostra o eCDF para a altura dos homens: De acordo com o gráfico, qual a porcentagem de homens com menos de 75 polegadas? para. 100% b. 95% c. 80% d. 72 polegadas 4. Para a polegada mais próxima, qual a altura m tem a propriedade de que metade dos estudantes do sexo masculino é mais alta que m e metade são menores? para. 61 polegadas b. 64 polegadas c. 69 polegadas d. 74 polegadas 5. Aqui está um eCDF das taxas de homicídio entre os estados: Sabendo que existem 51 estados (contando DC) e com base neste gráfico, quantos estados têm taxas de homicídios superiores a 10 por 100.000 pessoas? para. 1 b. 5 c. 10 d. cinquenta 6. De acordo com o eCDF anterior, qual das seguintes afirmações é verdadeira? para. Cerca de metade dos estados têm taxas de homicídio acima de 7 por 100.000 e a outra metade abaixo. b. A maioria dos estados tem taxas de homicídio inferiores a 2 por 100.000. c. Todos os estados têm taxas de homicídio superiores a 2 por 100.000. d. Com exceção de 4 estados, as taxas de homicídio são inferiores a 5 por 100.000. 7. Aqui está o histograma das alturas masculinas do nosso conjunto de dados heights: De acordo com este gráfico, quantos homens existem entre 63,5 e 65,5? para. 10 b. 24 c. 3. 4 d. 100 8. Aproximadamente, o que porcentagem tem menos de 60 polegadas? para. 1% b. 10% c. 25% d. cinquenta% 9. Com base no gráfico de densidade abaixo, aproximadamente qual proporção de estados dos EUA? EUA Você tem populações com mais de 10 milhões de pessoas? para. 0,02 b. 0,15 c. 0,50 d. 0,55 10. Abaixo estão três gráficos de densidade. É possível que eles sejam do mesmo conjunto de dados? Qual das seguintes afirmações é verdadeira? para. É impossível que eles sejam do mesmo conjunto de dados. b. Eles são do mesmo conjunto de dados, mas os gráficos são diferentes devido a erros de código. c. Eles são do mesmo conjunto de dados, mas o primeiro e o segundo gráficos suavizam e o terceiro gráfico suaviza demais. d. Eles são do mesmo conjunto de dados, mas o primeiro não está na escala logarítmica, o segundo suaviza menos e o terceiro ultrapassa o pico. 8.8 A distribuição normal Histogramas e gráficos de densidade fornecem excelentes resumos de uma distribuição. Mas podemos resumir mais? Muitas vezes vemos o desvio médio e padrão usado como um resumo estatístico - um resumo de dois números! Para entender o que são esses resumos e por que são tão usados, precisamos entender a distribuição normal. A distribuição normal, também conhecida como curva de sino e distribuição gaussiana, é um dos conceitos matemáticos mais famosos da história. Uma razão para isso é que distribuições aproximadamente normais ocorrem em muitas situações, incluindo ganhos de jogo, alturas, pesos, pressão arterial, resultados de testes padronizados e erros de medição experimentais. Existem explicações para isso e as descreveremos posteriormente. Aqui, focamos em como a distribuição normal nos ajuda a resumir os dados. Em vez de usar dados, a distribuição normal é definida com uma fórmula matemática. Para qualquer intervalo \\((a,b)\\), a proporção de valores nesse intervalo pode ser calculada usando esta fórmula: \\[\\mbox{Pr}(a &lt; x &lt; b) = \\int_a^b \\frac{1}{\\sqrt{2\\pi}s} e^{-\\frac{1}{2}\\left( \\frac{x-m}{s} \\right)^2} \\, dx\\] Eles não precisam memorizar ou entender os detalhes da fórmula. No entanto, lembre-se de que ele é completamente definido por apenas dois parâmetros: \\(m\\) e \\(s\\). O restante dos símbolos da fórmula representa os extremos do intervalo que determinamos, \\(a\\) e \\(b\\) e as constantes matemáticas conhecidas \\(\\pi\\) e \\(e\\). Esses dois parâmetros, \\(m\\) e \\(s\\), são conhecidos respectivamente como average (ou the mean) e the desvio padrão, ou SD, da distribuição. A distribuição é simétrica, centrada na média, e a maioria dos valores (cerca de 95%) está dentro de 2 DP da média. É assim que a distribuição normal se parece quando a média é 0 e o SD é 1: O fato de a distribuição ser definida por apenas dois parâmetros implica que, se a distribuição de um conjunto de dados puder ser aproximada por uma distribuição normal, todas as informações necessárias para descrever a distribuição poderão ser codificadas em apenas dois números: a média e a Desvio padrão. Agora vamos definir esses valores para uma lista arbitrária de números. Para uma lista de números contidos em um vetor x, a média é definida como: m &lt;- sum(x)/ length(x) e SD é definido como: s &lt;- sqrt(sum((x-mu)^2)/ length(x)) que pode ser interpretado como a distância média entre os valores e sua média. Vamos calcular os valores para a altura dos machos que armazenaremos no objeto \\(x\\): index &lt;- heights$sex == &quot;Male&quot; x &lt;- heights$height[index] Funções predefinidas podem ser usadas mean e sd (observe que, por razões explicadas na Seção ??, sd dividido por length(x)-1 em vez de length(x)) : m &lt;- mean(x) s &lt;- sd(x) c(average = m, sd = s) #&gt; average sd #&gt; 69.31 3.61 Aqui está um gráfico da densidade suave e da distribuição normal com média = 69.3 e SD = 3.6 desenhada como uma linha preta com a densidade suave de nossas alturas de alunos azuis: A distribuição normal parece ser uma boa aproximação aqui. Veremos agora como essa aproximação funciona para prever a proporção de valores dentro dos intervalos. 8.9 Unidades padrão Para dados distribuídos aproximadamente normalmente, é aconselhável pensar em termos de standard units. A unidade padrão de um valor nos diz quantos desvios padrão se desviam da média. Especificamente, para um valor x de um vetor X, definimos o valor de x em unidades padrão como z = (x - m)/s com m e s a média e desvio padrão de X, respectivamente. Por que é conveniente fazer isso? Primeiro revise a fórmula da distribuição normal e observe que o que está sendo exposto é \\(-z^2/2\\) com \\(z\\) equivalente a \\(x\\) em unidades padrão. O fato de que o máximo de \\(e^{-z^2/2}\\) é quando \\(z=0\\) explica por que a distribuição ocorre em média. Também explica a simetria, pois \\(- z^2/2\\) é simétrico em torno de 0. Além disso, observe que, se convertermos dados normalmente distribuídos em unidades padrão, poderemos saber rapidamente se, por exemplo, uma pessoa é aproximadamente média ( \\(z=0\\)), entre os mais altos ( \\(z \\approx 2\\)), entre os menores ( \\(z \\approx -2\\)) ou uma ocorrência extremamente rara ( \\(z &gt; 3\\) ou \\(z &lt; -3\\)) Lembre-se de que, independentemente das unidades originais, essas regras se aplicam a todos os dados que são aproximadamente normais. Em R, podemos obter unidades padrão usando a função scale: z &lt;- scale(x) Agora, para ver quantos homens estão dentro de 2 DP da média, simplesmente escrevemos: mean(abs(z) &lt; 2) #&gt; [1] 0.95 A proporção é de aproximadamente 95%, que é o que a distribuição normal prevê! Para ter ainda mais confirmação de que a aproximação é precisa, podemos usar gráficos QQ (quantile-quantile plots). 8.10 Gráficos QQ Uma maneira sistemática de avaliar quão bem a distribuição normal se ajusta aos dados é verificar se as proporções observadas e previstas correspondem. Em geral, este é o zoom do gráfico QQ. Primeiro, definimos os quantis teóricos para a distribuição normal. Nos livros de estatística, usamos o símbolo \\(\\Phi(x)\\) para definir a função que nos dá a probabilidade de que uma distribuição normal padrão seja menor que \\(x\\). Por exemplo, \\(\\Phi(-1.96) = 0.025\\) e \\(\\Phi(1.96) = 0.975\\). Em R, podemos avaliar \\(\\Phi\\) usando a função pnorm: pnorm(-1.96) #&gt; [1] 0.025 A função inversa \\(\\Phi^{-1}(x)\\) nos fornece os quantis teóricos da distribuição normal. Por exemplo, \\(\\Phi^{-1}(0.975) = 1.96\\). Em R, podemos avaliar o inverso de \\(\\Phi\\) usando a função qnorm. qnorm(0.975) #&gt; [1] 1.96 Observe que esses cálculos são para a distribuição normal padrão padrão (média = 0, desvio padrão = 1), mas também podemos defini-los para qualquer distribuição normal. Podemos fazer isso usando os argumentos mean e sd nas funções pnorm e qnorm. Por exemplo, podemos usar qnorm para determinar quantis de uma distribuição com média específica e desvio padrão: qnorm(0.975, mean = 5, sd = 2) #&gt; [1] 8.92 Para a distribuição normal, todos os cálculos relacionados aos quantis são executados sem dados, daí o nome de quiléticos teóricos. Mas quantis podem ser definidos para qualquer distribuição, mesmo empírica. Então, se tivermos dados em um vetor \\(x\\), podemos definir o quantil associado a qualquer proporção \\(p\\) como ele \\(q\\) para os quais a proporção de valores abaixo \\(q\\) é \\(p\\). Usando o código R, podemos definir q como o valor pelo qual mean(x &lt;= q) = p. Note que nem tudo \\(p\\) tem um \\(q\\) para o qual a proporção é exatamente \\(p\\). Existem várias maneiras de definir as melhores \\(q\\) conforme discutido na página de ajuda da função NA. Como um exemplo rápido, para dados de altura masculina, vemos que: mean(x &lt;= 69.5) #&gt; [1] 0.515 Portanto, cerca de 50% são menores ou iguais a 69 polegadas. Isso implica que se \\(p=0.50\\), tão \\(q=69.5\\). A idéia de um gráfico QQ é que, se seus dados forem bem aproximados pela distribuição normal, os quantis de seus dados deverão ser semelhantes aos quantis de uma distribuição normal. Para criar um gráfico QQ, fazemos o seguinte: Definimos um vetor de \\(m\\) dimensões \\(p_1, p_2, \\dots, p_m\\). Definimos um vetor de quantis \\(q_1, \\dots, q_m\\) para as proporções \\(p_1, \\dots, p_m\\) usando seus dados. Nós nos referimos a eles como os sample quantiles. Definimos um vetor de quantis teóricos para as proporções \\(p_1, \\dots, p_m\\) para uma distribuição normal com a mesma média e desvio padrão dos dados. Plotamos os quantis da amostra versus os quantis teóricos. Vamos construir um diagrama QQ usando o código R. Comece definindo o vetor de proporções. p &lt;- seq(0.05, 0.95, 0.05) Para obter os quantis dos dados, podemos usar a função quantile assim: sample_quantiles &lt;- quantile(x, p) Para obter os quantis teóricos da distribuição normal com DP médio e correspondente, usamos a função qnorm: theoretical_quantiles &lt;- qnorm(p, mean = mean(x), sd = sd(x)) Para ver se eles correspondem ou não, nós os plotamos um contra o outro e desenhamos a linha de identidade: qplot(theoretical_quantiles, sample_quantiles) + geom_abline() Observe que esse código é muito mais fácil se usarmos unidades padrão: sample_quantiles &lt;- quantile(z, p) theoretical_quantiles &lt;- qnorm(p) qplot(theoretical_quantiles, sample_quantiles) + geom_abline() O código acima está incluído para ajudar a descrever gráficos de QQ. No entanto, na prática, é mais fácil usar o código ggplot2 descrito na Seção 8.16: heights %&gt;% filter(sex == &quot;Male&quot;) %&gt;% ggplot(aes(sample = scale(height))) + geom_qq() + geom_abline() Enquanto na ilustração anterior usamos 20 quantis, o valor padrão da função geom_qq é usar a mesma quantidade de quantis que dados. 8.11 Percentis Antes de continuar, vamos definir alguns termos que são comumente usados na análise exploratória de dados. Percentiles_são casos especiais de_quantiles que são comumente usados. Percentis são os quantis obtidos na configuração do \\(p\\) para \\(0.01, 0.02, ..., 0.99\\). Chamamos, por exemplo, o caso de \\(p=0.25\\) o quartil inferior, pois fornece um número para o qual 25% dos dados estão abaixo. O percentil mais famoso é 50, também conhecido como o mediana. Para a distribuição normal, o median e a média são os mesmos, mas esse geralmente não é o caso. Outro caso especial que recebe um nome é o quartiles, obtido através da configuração \\(p=0.25,0.50\\) e \\(0.75\\). 8.12 Diagramas de caixas Para apresentar os gráficos de caixas (boxplots em inglês), retornaremos aos dados de assassinatos nos EUA. EUA Suponha que queremos resumir a distribuição da taxa de homicídios. Usando a técnica de visualização de dados que aprendemos, podemos ver que a aproximação normal não se aplica aqui: Nesse caso, o histograma acima ou o gráfico de densidade suave serviria como um resumo relativamente sucinto. Agora, suponha que aqueles que estão acostumados a receber apenas dois números como resumos solicitem um resumo numérico mais compacto. Aqui Tukey ofereceu alguns conselhos. Primeiro, ele recomendou fornecer um resumo de cinco números compostos pelo intervalo, juntamente com os quartis (percentis 25, 50 e 75). Além disso, Tukey sugeriu ignorar os outliers ao calcular o intervalo e, em vez disso, plotá-los como pontos independentes. Mais adiante, ofereceremos uma explicação detalhada dos outliers. Por fim, ele recomendou que representássemos graficamente esses números como uma “caixa” com “bigodes”, assim: com o gráfico definido pelos percentis 25% e 75% e os bigodes mostrando o intervalo. A distância entre esses dois é chamada de intervalo interquartil. Os dois pontos são discrepantes, conforme definido por Tukey. A mediana é mostrada com uma linha horizontal. A partir deste gráfico simples, agora conhecido como box diagram, sabemos que a mediana é de aproximadamente 2,5, que a distribuição não é simétrica e que o intervalo é de 0 a 5 para a grande maioria dos estados, com duas exceções. Discutimos como criar boxplots na Seção 8.16. 8.13 Estratificação Na análise dos dados, geralmente dividimos as observações em grupos com base nos valores de uma ou mais variáveis associadas a essas observações. Por exemplo, na próxima seção, dividimos os valores de altura em grupos de acordo com uma variável de gênero: mulheres e homens. Chamamos esse procedimento de stratification_e nos referimos aos grupos resultantes como_strata. A estratificação é comum na visualização de dados, porque geralmente estamos interessados em saber como a distribuição de variáveis difere entre diferentes subgrupos. Veremos vários exemplos ao longo desta parte do livro. Revisaremos o conceito de estratificação quando aprendermos a regressão no Capítulo ?? e na parte de aprendizagem da máquina do livro. 8.14 Estudo de caso: descrição das alturas dos alunos (continuação) Usando o histograma, gráficos de densidade e gráficos de QQ, ficamos convencidos de que os dados de altura masculina se aproximam muito de uma distribuição normal. Nesse caso, damos a ET um resumo muito sucinto: as alturas masculinas seguem uma distribuição normal com uma média de 69.3 polegadas e um SD de 3.6 polegadas. Com essas informações, o ET terá uma boa idéia do que esperar quando conhecer nossos alunos do sexo masculino. No entanto, para fornecer uma imagem completa, também devemos fornecer um resumo das alturas das fêmeas. Aprendemos que os boxplots são úteis quando queremos comparar rapidamente duas ou mais distribuições. Aqui vemos as alturas para machos e fêmeas: O diagrama mostra imediatamente que os homens são, em média, mais altos que as mulheres. Os desvios padrão parecem ser semelhantes. Mas a aproximação normal também funciona para os dados de altura feminina coletados pela pesquisa? Esperamos que eles sigam uma distribuição normal, assim como os meninos. No entanto, gráficos exploratórios revelam que a aproximação não é tão útil: Vemos algo que não vemos nos meninos: o gráfico de densidade tem uma segunda protuberância. Além disso, o gráfico QQ mostra que os pontos mais altos tendem a ser mais altos do que o esperado pela distribuição normal. Finalmente, também vemos cinco pontos no gráfico QQ que sugerem alturas inferiores às esperadas para uma distribuição normal. Ao reportar novamente ao ET, podemos precisar fornecer um histograma, em vez de apenas a média e o desvio padrão para as alturas das fêmeas. No entanto, relemos a citação de Tukey e percebemos que percebemos o que não esperávamos ver. Se olharmos para outras distribuições de altura feminina, descobrimos que elas são bem aproximadas com uma distribuição normal. Então, por que nossos alunos são diferentes? Nossa classe é um requisito para o time de basquete feminino? Existe uma pequena proporção de mulheres que dizem ser mais altas do que são? Outra explicação, talvez mais provável, é que, na forma em que os alunos entraram em suas alturas, FEMALE era o sexo padrão e alguns machos entraram em suas alturas, mas esqueceram de mudar a variável sexo. De qualquer forma, a visualização de dados ajudou a descobrir uma possível falha em nossos dados. Em relação aos cinco menores valores, observe que esses valores são: heights %&gt;% filter(sex == &quot;Female&quot;) %&gt;% top_n(5, desc(height)) %&gt;% pull(height) #&gt; [1] 51 53 55 52 52 Como essas são as alturas autorreferidas, uma possibilidade é que os alunos desejem entrar 5'1&quot;, 5'2&quot;, 5'3&quot; ou 5'5&quot;. 8.15 Exercícios 1. Defina variáveis que contêm as alturas de machos e fêmeas desta maneira: library(dslabs) data(heights) male &lt;- heights$height[heights$sex == &quot;Male&quot;] female &lt;- heights$height[heights$sex == &quot;Female&quot;] Quantas medições temos para cada uma? 2. Suponha que não possamos fazer um gráfico e queremos comparar as distribuições lado a lado. Não podemos simplesmente listar todos os números. Em vez disso, veremos os percentis. Crie uma tabela de cinco linhas que mostre female_percentiles e male_percentiles com os percentis 10, 30, 50, …, 90 para cada sexo. Em seguida, crie um data frame com esses dois como colunas. 3. Estude os seguintes gráficos de caixa mostrando os tamanhos da população por país: Qual continente tem o país com o maior tamanho populacional? 4. Qual continente tem o maior tamanho médio de população? 5. Qual é o tamanho médio da população da África para o milhão mais próximo? 6. Qual a proporção de países na Europa com populações inferiores a 14 milhões? para. 0,99 b. 0,75 c. 0,50 d. 0,25 7. Se usarmos uma transformação logarítmica, qual continente dos anteriores tem o maior intervalo interquartil? 8. Carregue o conjunto de dados de altura e crie um vetor x com apenas as alturas masculinas: library(dslabs) data(heights) x &lt;- heights$height[heights$sex==&quot;Male&quot;] Qual a proporção dos dados entre 69 e 72 polegadas (mais alto que 69, mas menor ou igual a 72)? Dica: use um operador lógico e mean. 9. Suponha que a única coisa que você saiba sobre os dados seja o desvio médio e padrão. Use a aproximação normal para estimar a proporção que você acabou de calcular. Dica: Comece calculando a média e o desvio padrão. Então use a função pnorm prever as proporções. 10. Observe que a aproximação calculada na pergunta nove está muito próxima do cálculo exato na primeira pergunta. Agora faça a mesma tarefa para mais discrepâncias. Compare o cálculo exato e a aproximação normal do intervalo (79,81) .Quantas vezes maior é a proporção real do que a aproximação? Onze. Aproxima a distribuição de homens adultos no mundo como uma distribuição normal, com uma média de 69 polegadas e um desvio padrão de 3 polegadas. Usando essa aproximação, encontre a proporção de homens adultos com 7 pés de altura ou mais, conhecidos como “sete rodapés”. Dica: use a função pnorm. 12. Existem cerca de um bilhão de homens entre 18 e 40 anos no mundo. Use sua resposta à pergunta anterior para estimar quantos desses homens (18 a 40 anos) têm sete pés de altura ou mais no mundo. 13. Existem cerca de 10 jogadores da National Basketball Association (NBA) com 7 pés de altura ou mais. Usando a resposta para as duas perguntas anteriores, qual a proporção de sete rodapés do mundo, com idades entre 18 e 40 anos, na NBA? 14. Repita os cálculos feitos na pergunta anterior para a altura do jogador de basquete Lebron James: 6 pés e 8 polegadas. Existem cerca de 150 jogadores que são pelo menos tão altos. Quinze. Ao responder às perguntas acima, descobrimos que não é incomum que um jogador de sete pés se torne jogador da NBA. Então, o que seria uma crítica justa aos nossos cálculos? para. Prática e talento são o que fazem um ótimo jogador de basquete, não a altura. b. A abordagem normal não é apropriada para alturas. c. Conforme observado na Questão 10, a aproximação normal tende a subestimar valores discrepantes. Pode haver mais sete rodapés do que previmos. d. Conforme observado na Questão 10, a aproximação normal tende a superestimar valores discrepantes. Pode haver menos sete rodapés do que previmos. 8.16 Gegplot2 geometrias No capítulo 7 apresentamos o pacote ggplot2 para visualização de dados. Aqui, demonstramos como gerar gráficos relacionados a distribuições, especificamente os gráficos mostrados anteriormente neste capítulo. 8.16.1 Gráficos de barra Para gerar um diagrama de barras (barplots em inglês), podemos usar a geometria geom_bar. Por padrão, R conta os casos em cada categoria e desenha uma barra. Aqui vemos o gráfico de barras para as regiões dos Estados Unidos. murders %&gt;% ggplot(aes(region)) + geom_bar() Muitas vezes, já temos uma tabela com uma distribuição que queremos apresentar como um gráfico de barras. Aqui está um exemplo dessa tabela: data(murders) tab &lt;- murders %&gt;% count(region) %&gt;% mutate(proportion = n/sum(n)) tab #&gt; # A tibble: 4 x 3 #&gt; region n proportion #&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Northeast 9 0.176 #&gt; 2 South 17 0.333 #&gt; 3 North Central 12 0.235 #&gt; 4 West 13 0.255 Não queremos mais geom_bar contar, mas apenas representar graficamente uma barra na altura fornecida pela variável proportion. Para isso, precisamos fornecer x (as categorias) e y (os valores) e use a opção stat=&quot;identity&quot;. tab %&gt;% ggplot(aes(region, proportion)) + geom_bar(stat = &quot;identity&quot;) 8.16.2 Histogramas Para gerar histogramas, usamos geom_histogram. Ao revisar a página de ajuda para esta função, vemos que o único argumento necessário é x, a variável para a qual construiremos um histograma. Nós não usamos o x porque sabemos que é o primeiro argumento. O código fica assim: heights %&gt;% filter(sex == &quot;Female&quot;) %&gt;% ggplot(aes(height)) + geom_histogram() Se executarmos o código acima, ele nos dará uma mensagem: stat_bin() usando bins = 30. Escolha um valor melhor com binwidth. Anteriormente, usamos um tamanho de compartimento de 1 polegada; portanto, o código fica assim: heights %&gt;% filter(sex == &quot;Female&quot;) %&gt;% ggplot(aes(height)) + geom_histogram(binwidth = 1) Finalmente, se por razões estéticas queremos adicionar cor, usamos os argumentos descritos na página de ajuda. Também adicionamos tags e um título: heights %&gt;% filter(sex == &quot;Female&quot;) %&gt;% ggplot(aes(height)) + geom_histogram(binwidth = 1, fill = &quot;blue&quot;, col = &quot;black&quot;) + xlab(&quot;Male heights in inches&quot;) + ggtitle(&quot;Histogram&quot;) 8.16.3 Gráficos de densidade Para criar uma densidade suave, usamos geom_density. Para criar um gráfico de densidade suave com os dados que visualizamos anteriormente como um histograma, podemos usar este código: heights %&gt;% filter(sex == &quot;Female&quot;) %&gt;% ggplot(aes(height)) + geom_density() Para preencher com cores, podemos usar o argumento fill. heights %&gt;% filter(sex == &quot;Female&quot;) %&gt;% ggplot(aes(height)) + geom_density(fill=&quot;blue&quot;) Para alterar a suavidade da densidade, usamos o argumento adjust multiplicar o valor padrão por esse adjust. Por exemplo, se queremos que o parâmetro de suavização seja duas vezes maior, usamos: heights %&gt;% filter(sex == &quot;Female&quot;) + geom_density(fill=&quot;blue&quot;, adjust = 2) 8.16.4 Diagramas de caixas A geometria para a criação de gráficos de caixas é geom_boxplot. Como já discutimos, os boxplots são úteis para comparar distribuições. Por exemplo, abaixo, vemos as alturas mostradas acima para as mulheres, mas aqui em comparação aos homens. Para essa geometria, precisamos dos argumentos x como categorias e argumentos y valores semelhantes: 8.16.5 Gráficos QQ Para gráficos QQ, usamos geometria geom_qq. Na página de ajuda, aprendemos que precisamos especificar o sample (aprenderemos sobre amostras em um capítulo posterior). Aqui está o gráfico QQ para alturas masculinas: heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% ggplot(aes(sample = height)) + geom_qq() Por padrão, a variável de amostra é comparada a uma distribuição normal com uma média de 0 e um desvio padrão de 1. Para alterar isso, usamos o argumento dparams de acordo com a página de ajuda. Para adicionar uma linha de identidade, basta atribuir outra camada. Para linhas retas, usamos a função geom_abline. A linha padrão é a linha de identidade (inclinação = 1, interceptação = 0). params &lt;- heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% summarize(mean = mean(height), sd = sd(height)) heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% ggplot(aes(sample = height)) + geom_qq(dparams = params) + geom_abline() Outra opção aqui é dimensionar os dados primeiro e, em seguida, plotar o QQ em relação à distribuição normal padrão. heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% ggplot(aes(sample = scale(height))) + geom_qq() + geom_abline() 8.16.6 Imagens Não tivemos que usar imagens para os conceitos descritos neste capítulo, mas as usaremos na Seção 10.14, por isso, apresentamos as duas geometrias usadas para criar imagens: __geom_tile__egeom_raster. Eles se comportam de maneira semelhante; Para ver como eles diferem, consulte a página de ajuda. Para criar uma imagem em ggplot2, precisamos de um data frame com as coordenadas xey, além dos valores associados a cada uma delas. Aqui temos um data frame: x &lt;- expand.grid(x = 1:12, y = 1:10) %&gt;% mutate(z = 1:120) Observe que esta é a versão tidy de uma matriz, matrix(1:120, 12, 10). Para representar graficamente a imagem, usamos o seguinte código: x %&gt;% ggplot(aes(x, y, fill = z)) + geom_raster() Com essas imagens, eles geralmente desejam alterar a escala de cores. Isso pode ser feito através da camada scale_fill_gradientn. x %&gt;% ggplot(aes(x, y, fill = z)) + geom_raster() + scale_fill_gradientn(colors = terrain.colors(10)) 8.16.7 Gráficos rápidos Na seção 7.13 apresentamos qplot como uma função útil quando precisamos fazer um diagrama de dispersão rápido. Também podemos usar qplot para criar histogramas, diagramas de densidade, diagramas de caixa, gráficos de QQ e muito mais. Embora não forneça o nível de controle ggplot, qplot é definitivamente útil, pois permite criar um gráfico com um pequeno trecho de código. Suponha que tenhamos as alturas femininas em um objeto x: x &lt;- heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% pull(height) Para fazer um histograma rápido, podemos usar: qplot(x) A função supõe que queremos criar um histograma porque fornecemos apenas uma variável. Na seção 7.13 vimos que, se fornecermos duas variáveis para qplot , cria automaticamente um diagrama de dispersão. Para fazer um gráfico rápido de QQ, eles precisam usar o argumento sample. Lembre-se de que podemos adicionar camadas, como fazemos com ggplot. qplot(sample = scale(x)) + geom_abline() Se fornecermos um fator e um vetor numérico, obteremos um gráfico como o que vemos abaixo. Observe que no código estamos usando o argumento data. Como o data frame não é o primeiro argumento em qplot, temos que usar o operador de ponto. heights %&gt;% qplot(sex, height, data = .) Também podemos selecionar uma geometria específica usando o argumento geom. Portanto, para converter o diagrama acima em um diagrama de caixa, usamos o seguinte código: heights %&gt;% qplot(sex, height, data = ., geom = &quot;boxplot&quot;) Também podemos usar o argumento geom para gerar um gráfico de densidade em vez de um histograma: qplot(x, geom = &quot;density&quot;) Embora não tanto quanto com ggplot, temos alguma flexibilidade para melhorar os resultados de qplot. Olhando a página de ajuda, vemos várias maneiras pelas quais podemos melhorar a aparência do histograma anterior. Por exemplo: qplot(x, bins=15, color = I(&quot;black&quot;), xlab = &quot;Population&quot;) Nota técnica: o motivo pelo qual usamos I(&quot;black&quot;) é porque nós queremos qplot tentar &quot;black&quot; como um personagem ao invés de um fator. Esse é o comportamento padrão dentro aes, que é chamado internamente aqui. Em geral, a função I usado em R para dizer “mantenha como está”. 8.17 Exercícios 1. Agora vamos usar a função geom_histogram para fazer um histograma das alturas no conjunto de dados height. Lendo a documentação para esta função, vemos que ela requer apenas uma atribuição, os valores a serem usados para o histograma. Faça um histograma de todos os gráficos. Qual é a variável que contém as alturas? para. sex b. heights c. height d. heights$height 2. Agora crie um objeto ggplot usando o pipe para atribuir os dados de altura a um objeto ggplot. Atribuir height para os valores de x através da função aes. 3. Agora estamos prontos para adicionar uma camada para fazer o histograma. Use o objeto criado no exercício anterior e a função geom_histogram para fazer o histograma. 4. Quando executamos o código no exercício anterior, recebemos o aviso: stat_bin() usando bins = 30. Escolha um valor melhor com binwidth. Use o argumento binwidth para alterar o histograma criado no exercício anterior para usar compartimentos de 1 polegada. 5. Em vez de um histograma, vamos fazer um gráfico de densidade suave. Nesse caso, não criaremos um objeto, mas criaremos e exibiremos o gráfico com uma linha de código. Altere a geometria no código usado acima para criar uma densidade suave em vez de um histograma. 6. Agora vamos fazer um gráfico de densidade para machos e fêmeas separadamente. Podemos fazer isso usando o argumento group. Atribuímos grupos por meio de mapeamento estético, pois cada ponto precisa de um grupo antes de fazer os cálculos necessários para estimar uma densidade. 7. Também podemos atribuir grupos através do argumento color. Isso tem o benefício adicional de usar cores para distinguir grupos. Mude o código acima para usar cores. 8. Além disso, podemos atribuir grupos através do argumento fill. Isso tem o benefício adicional de usar cores para distinguir grupos como: heights %&gt;% ggplot(aes(height, fill = sex)) + geom_density() No entanto, aqui a segunda densidade é plotada em cima da primeira. Podemos tornar as curvas mais visíveis usando alpha blending para adicionar transparência. Defina o parâmetro alfa como 0,2 na função geom_density para fazer essa alteração. "],
["gapminder.html", "Capítulo 9 Visualização de dados na prática 9.1 Estudo de caso: novas idéias sobre pobreza 9.2 Diagrama de dispersão 9.3 Separe em facetas 9.4 Gráficos de séries temporais 9.5 Transformações de dados 9.6 Como visualizar distribuições multimodais 9.7 Como comparar várias distribuições com gráficos de caixas e gráficos ridge 9.8 A falácia ecológica e a importância de mostrar os dados", " Capítulo 9 Visualização de dados na prática Neste capítulo, demonstraremos como o código ggplot2 relativamente simples pode criar gráficos esclarecedores e esteticamente agradáveis. Como motivação, criaremos gráficos que nos ajudarão a entender melhor as tendências da saúde e da economia global. Vamos implementar o que aprendemos nos capítulos 7 e 8.16 e aprenderemos a expandir o código para aperfeiçoar os gráficos. À medida que prosseguimos em nosso estudo de caso, descreveremos os princípios gerais mais relevantes para a visualização de dados e aprenderemos conceitos como facetes, gráficos de séries temporais, formações_de_formações_e gráficos de crista_. 9.1 Estudo de caso: novas idéias sobre pobreza Hans Rosling26 foi o co-fundador da Fundação Gapminder27, uma organização dedicada a educar o público através de dados para dissipar mitos comuns sobre o chamado mundo em desenvolvimento. A organização usa dados para mostrar como as tendências atuais nos campos da saúde e da economia contradizem as narrativas emanadas da cobertura sensacional da mídia de catástrofes, tragédias e outros eventos infelizes. Conforme declarado no site da Fundação Gapminder: Jornalistas e lobistas contam histórias dramáticas. Esse é o trabalho deles. Eles contam histórias sobre eventos extraordinários e pessoas incomuns. Histórias dramáticas se acumulam na mente das pessoas em visões de mundo excessivamente dramáticas e fortes sentimentos de estresse negativo: “O mundo está piorando!”, “Somos nós contra eles!”, “Outras pessoas estão Estranho!”,&quot; A população continua crescendo! “E” Ninguém liga! &quot; Hans Rosling decidiu, de maneira dramática, educar o público sobre tendências orientadas a dados usando visualizações de dados eficazes. Esta seção é baseada em duas palestras que exemplificam essa perspectiva educacional: Novas perspectivas sobre a pobreza28 e As melhores estatísticas que você já viu29. Especificamente, nesta seção, usamos dados para tentar responder às duas perguntas a seguir: É uma caracterização justa do mundo de hoje dizer que está dividido em nações ocidentais ricas e no mundo em desenvolvimento formado pela África, Ásia e América Latina? A desigualdade de renda piorou em todos os países nos últimos 40 anos? Para responder a essas perguntas, usaremos o conjunto de dados gapminder fornecido por dslabs. Esse conjunto de dados foi criado usando várias planilhas disponíveis da Fundação Gapminder. Eles podem acessar a tabela desta maneira: library(tidyverse) library(dslabs) data(gapminder) gapminder %&gt;% as_tibble() #&gt; # A tibble: 10,545 x 9 #&gt; country year infant_mortality life_expectancy fertility population #&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Albania 1960 115. 62.9 6.19 1636054 #&gt; 2 Algeria 1960 148. 47.5 7.65 11124892 #&gt; 3 Angola 1960 208 36.0 7.32 5270844 #&gt; 4 Antigu… 1960 NA 63.0 4.43 54681 #&gt; 5 Argent… 1960 59.9 65.4 3.11 20619075 #&gt; # … with 10,540 more rows, and 3 more variables: gdp &lt;dbl&gt;, #&gt; # continent &lt;fct&gt;, region &lt;fct&gt; Teste de Hans Rosling Como no vídeo New Insights on Poverty, começamos testando nosso conhecimento das diferenças na mortalidade infantil em diferentes países. Para cada um dos cinco pares de países abaixo, quais países você acha que tiveram as maiores taxas de mortalidade infantil em 2015? Quais pares você acha que são mais parecidos? Sri Lanka ou Turquia Polônia ou Coréia do Sul Malásia ou Rússia Paquistão ou Vietnã Tailândia ou África do Sul Ao responder a essas perguntas sem dados, os países não europeus geralmente são escolhidos como os que apresentam as mais altas taxas de mortalidade infantil: Sri Lanka sobre a Turquia, Coréia do Sul sobre Polônia e Malásia sobre Rússia. Também é comum supor que os países considerados parte do mundo em desenvolvimento - Paquistão, Vietnã, Tailândia e África do Sul - têm taxas de mortalidade igualmente altas. Para responder a essas perguntas com dados, podemos usar dplyr. Por exemplo, para a primeira comparação, vemos que: gapminder %&gt;% filter(year == 2015 &amp; country %in% c(&quot;Sri Lanka&quot;,&quot;Turkey&quot;)) %&gt;% select(country, infant_mortality) #&gt; country infant_mortality #&gt; 1 Sri Lanka 8.4 #&gt; 2 Turkey 11.6 A Turquia tem a maior taxa de mortalidade infantil. Podemos usar esse código em todas as comparações e descobrir o seguinte: country infant mortality country infant mortality Sri Lanka 8.4 Turkey 11.6 Poland 4.5 South Korea 2.9 Malaysia 6.0 Russia 8.2 Pakistan 65.8 Vietnam 17.3 Thailand 10.5 South Africa 33.6 9.2 Diagrama de dispersão A razão para as pontuações baixas decorre da noção preconcebida de que o mundo está dividido em dois grupos: o mundo ocidental (Europa Ocidental e América do Norte), caracterizado por vida longa e famílias pequenas, versus o mundo em desenvolvimento (África). , Ásia e América Latina), caracterizada por curtos períodos de vida e famílias numerosas. Mas os dados suportam essa visão dicotômica? Os dados necessários para responder a esta pergunta também estão disponíveis em nossa tabela. gapminder. Usando nossas habilidades de visualização de dados recém-aprendidas, podemos enfrentar esse desafio. Para analisar essa visão de mundo, nosso primeiro gráfico é um gráfico de dispersão da expectativa de vida versus taxas de fertilidade (número médio de filhos por mulher). Começamos analisando os dados de cerca de 50 anos atrás, quando talvez essa visão tenha sido consolidada em nossas mentes. filter(gapminder, year == 1962) %&gt;% ggplot(aes(fertility, life_expectancy)) + geom_point() A maioria dos pontos é dividida em duas categorias diferentes: Expectativa de vida em torno de 70 anos e 3 ou menos filhos por família. Expectativa de vida de menos de 65 anos e mais de 5 filhos por família. Para confirmar que esses países são das regiões que esperamos, podemos usar uma cor para representar um continente. filter(gapminder, year == 1962) %&gt;% ggplot( aes(fertility, life_expectancy, color = continent)) + geom_point() Em 1962, a visão do “Ocidente versus o mundo em desenvolvimento” era baseada em uma certa realidade. Ainda é assim 50 anos depois? 9.3 Separe em facetas Podemos facilmente representar graficamente os dados de 2012 da mesma maneira que fizemos em 1962. No entanto, para comparação, é preferível representar graficamente lado a lado. No ggplot2, conseguimos isso separando as variáveis em facets: estratificamos os dados por alguma variável e fazemos o mesmo gráfico para cada estrato. Para separar em facetas, adicionamos uma camada com a função facet_grid, que separa automaticamente os gráficos. Essa função permite separar até duas variáveis em facetas usando colunas para representar uma variável e linhas para representar a outra. A função espera que as variáveis de linha e coluna sejam separadas por um ~. Aqui vemos um exemplo de um diagrama de dispersão em que adicionamos facet_grid como a última camada: filter(gapminder, year%in%c(1962, 2012)) %&gt;% ggplot(aes(fertility, life_expectancy, col = continent)) + geom_point() + facet_grid(continent~year) Acima, vemos um gráfico para cada combinação de continente/ ano. No entanto, este é apenas um exemplo e mais do que queremos, que é simplesmente comparar dois anos: 1962 e 2012. Nesse caso, existe apenas uma variável e usamos . para que facet_grid saiba que não estamos usando uma das variáveis: filter(gapminder, year%in%c(1962, 2012)) %&gt;% ggplot(aes(fertility, life_expectancy, col = continent)) + geom_point() + facet_grid(. ~ year) Este gráfico mostra claramente que a maioria dos países mudou do conjunto desenvolvido_mundo para o_estilo ocidental. Em 2012, a visão do mundo ocidental versus o mundo em desenvolvimento não faz mais sentido. Isso é particularmente evidente ao comparar a Europa com a Ásia, este último agora com vários países que fizeram grandes melhorias. 9.3.1 facet_wrap Para explorar como essa transformação ocorreu ao longo dos anos, podemos fazer o gráfico por vários anos. Por exemplo, podemos adicionar os anos 1970, 1980, 1990 e 2000. No entanto, se fizermos isso, não queremos todos os gráficos na mesma linha, que é o que faz facet_grid por padrão, pois eles parecerão muito estreitos para exibir os dados. Em vez disso, queremos usar várias linhas e colunas. A função facet_wrap nos permite fazer isso automaticamente, acomodando a série de gráficos para que cada imagem tenha dimensões visíveis: years &lt;- c(1962, 1980, 1990, 2000, 2012) continents &lt;- c(&quot;Europe&quot;, &quot;Asia&quot;) gapminder %&gt;% filter(year %in% years &amp; continent %in% continents) %&gt;% ggplot( aes(fertility, life_expectancy, col = continent)) + geom_point() + facet_wrap(~year) Este gráfico mostra claramente como a maioria dos países asiáticos melhorou a uma taxa muito mais rápida que a Europa. 9.3.2 Escalas fixas para melhores comparações A escolha padrão da faixa de eixos é importante. Quando não estiver em uso facet esse intervalo é determinado pelos dados mostrados no gráfico. Quando eles usam facet esse intervalo é determinado pelos dados exibidos em todos os gráficos e, portanto, permanece fixo em todos os gráficos. Isso facilita muito as comparações entre gráficos. Por exemplo, no gráfico acima, podemos ver que a expectativa de vida aumentou e a fertilidade diminuiu na maioria dos países. Vemos isso porque a nuvem de pontos está se movendo. Este não é o caso se ajustarmos as escalas: filter(gapminder, year%in%c(1962, 2012)) %&gt;% ggplot(aes(fertility, life_expectancy, col = continent)) + geom_point() + facet_wrap(. ~ year, scales = &quot;free&quot;) No gráfico acima, devemos prestar atenção especial ao intervalo, para observar que o gráfico à direita tem uma expectativa de vida mais longa. 9.4 Gráficos de séries temporais As visualizações acima ilustram efetivamente que os dados não são mais consistentes com a visão de mundo ocidental versus o mundo em desenvolvimento. Vendo esses gráficos, novas questões surgem. Por exemplo, quais países estão melhorando mais e quais são os menos? A melhoria foi constante nos últimos 50 anos ou acelerou mais em determinados períodos? Para uma análise mais detalhada que possa ajudar a responder a essas perguntas, apresentamos gráficos de séries temporais. Os gráficos de séries temporais possuem tempo no eixo x e um resultado ou medida de interesse no eixo y. Por exemplo, aqui está um gráfico da tendência nas taxas de fertilidade nos Estados Unidos: gapminder %&gt;% filter(country == &quot;United States&quot;) %&gt;% ggplot(aes(year, fertility)) + geom_point() Observamos que a tendência não é linear, mas que durante os anos sessenta e setenta há uma queda acentuada abaixo de 2. Em seguida, a tendência retorna a 2 e se estabiliza nos anos noventa. Quando os pontos são espaçados regularmente e densamente, como vemos acima, criamos uma curva que conecta os pontos às linhas, para transmitir que esses dados são provenientes de uma única série, aqui um país. Para fazer isso, usamos a função geom_line em vez de geom_point. gapminder %&gt;% filter(country == &quot;United States&quot;) %&gt;% ggplot(aes(year, fertility)) + geom_line() Isso é particularmente útil ao comparar dois países. Se criarmos um subconjunto dos dados para incluir dois países, um da Europa e outro da Ásia, adaptaremos o código acima: countries &lt;- c(&quot;South Korea&quot;,&quot;Germany&quot;) gapminder %&gt;% filter(country %in% countries) %&gt;% ggplot(aes(year,fertility)) + geom_line() Claramente, esse not é o gráfico que queremos. Em vez de uma linha para cada país, os pontos dos dois países são unidos porque não informamos ggplot que queremos duas linhas independentes. Para que ggplot para entender que existem duas curvas que devem ser feitas separadamente, atribuímos cada ponto a uma group, um para cada país: countries &lt;- c(&quot;South Korea&quot;,&quot;Germany&quot;) gapminder %&gt;% filter(country %in% countries &amp; !is.na(fertility)) %&gt;% ggplot(aes(year, fertility, group = country)) + geom_line() Mas qual linha vai com qual país? Podemos atribuir cores para fazer essa distinção. Um efeito colateral útil do uso do argumento color para atribuir cores diferentes a diferentes países, os dados são agrupados automaticamente: countries &lt;- c(&quot;South Korea&quot;,&quot;Germany&quot;) gapminder %&gt;% filter(country %in% countries &amp; !is.na(fertility)) %&gt;% ggplot(aes(year,fertility, col = country)) + geom_line() O gráfico mostra claramente como a taxa de fertilidade da Coréia do Sul caiu drasticamente durante as décadas de 1960 e 1970, e em 1990 teve uma taxa semelhante à da Alemanha. 9.4.1 Etiquetas em vez de legendas Para gráficos de tendências, recomendamos rotular as linhas em vez de usar legendas, pois o espectador pode ver rapidamente qual linha representa qual país. Essa sugestão se aplica à maioria dos gráficos: os rótulos geralmente são preferidos às legendas. Demonstramos como fazer isso usando dados de expectativa de vida. Definimos uma tabela de dados com os locais dos rótulos e, em seguida, usamos uma segunda atribuição apenas para esses rótulos: labels &lt;- data.frame(country = countries, x = c(1975,1965), y = c(60,72)) gapminder %&gt;% filter(country %in% countries) %&gt;% ggplot(aes(year, life_expectancy, col = country)) + geom_line() + geom_text(data = labels, aes(x, y, label = country), size = 5) + theme(legend.position = &quot;none&quot;) O gráfico mostra claramente como uma melhoria na expectativa de vida se seguiu a quedas nas taxas de fertilidade. Em 1960, os alemães viveram 15 anos a mais que os sul-coreanos, embora em 2010 a diferença esteja completamente fechada. Ele exemplifica a melhoria que muitos países não ocidentais fizeram nos últimos 40 anos. 9.5 Transformações de dados Agora voltamos nossa atenção para a segunda questão relacionada à idéia comum de que a distribuição da riqueza em todo o mundo piorou nas últimas décadas. Quando se pergunta ao público em geral se os países pobres se tornaram mais pobres e os países ricos se tornaram mais ricos, a maioria responde que sim. Usando camadas, histogramas, densidades suaves e gráficos de caixa, podemos ver se esse é realmente o caso. Primeiro, aprenderemos como as transformações às vezes podem ajudar a fornecer resumos e gráficos mais informativos. A tabela de dados gapminder inclui uma coluna com o produto interno bruto do país, ou PIB. O PIB mede o valor de mercado de bens e serviços produzidos por um país em um ano. O PIB por pessoa é frequentemente usado como um resumo aproximado da riqueza de um país. Aqui, dividimos esse valor por 365 para obter a medida mais interpretável de dólares por dia. Usando o dólar atual como unidade, uma pessoa que sobrevive com uma renda inferior a US $ 2 por dia é definida como vivendo em “pobreza absoluta”. Nós adicionamos essa variável à tabela de dados: gapminder &lt;- gapminder %&gt;% mutate(dollars_per_day = gdp/population/365) Os valores do PIB são ajustados pela inflação e representam o dólar atual, portanto esses valores devem ser comparáveis ao longo dos anos. Obviamente, essas são as médias dos países e dentro de cada país há muita variabilidade. Todos os gráficos e idéias descritos abaixo se referem às médias dos países e não aos indivíduos dentro deles. 9.5.1 Transformação logarítmica Abaixo está um histograma de ganhos diários de 1970: past_year &lt;- 1970 gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% ggplot(aes(dollars_per_day)) + geom_histogram(binwidth = 1, color = &quot;black&quot;) Nós usamos o argumento color = &quot;black&quot; traçar um limite e distinguir claramente os compartimentos. Neste gráfico, vemos que para a maioria dos países, as médias estão abaixo de $10 por día. Sin embargo, la mayoría del eje-x está dedicado a 35 países con promedio de menos de $ 10. Portanto, o gráfico não é muito informativo em relação aos países com valores abaixo de $ 10 por dia. Seria mais informativo ver rapidamente quantos países têm renda média diária de cerca de $1 (extremadamente pobre), $ 2 (muito ruim), $4 (pobre), $ 8 (média), $16 (acomodado), $ 32 (rico), $ 64 (muito rico) por dia. Essas mudanças são multiplicativas, e as transformações logarítmicas convertem as alterações multiplicativas em aditivos: quando a base 2 é usada, dobrar um valor se torna um aumento de 1. Aqui está a distribuição se aplicarmos uma transformação logarítmica de base 2: gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% ggplot(aes(log2(dollars_per_day))) + geom_histogram(binwidth = 1, color = &quot;black&quot;) É assim que observamos mais de perto os países de renda média e baixa. 9.5.2 Qual base? No caso anterior, usamos a base 2 nas transformações logarítmicas. Outras opções comuns são básicas \\(\\mathrm{e}\\) (o logaritmo natural) e base 10. Em geral, não recomendamos o uso do logaritmo natural para exploração e visualização de dados. A razão é porque enquanto \\(2^2, 2^3, 2^4, \\dots\\) ou \\(10^2, 10^3, \\dots\\) eles são fáceis de calcular em nossas cabeças, o mesmo não é verdade para \\(\\mathrm{e}^2, \\mathrm{e}^3, \\dots\\) portanto, a escala não é intuitiva nem fácil de interpretar. No exemplo de dólares por dia, usamos a base 2 em vez da base 10 porque o intervalo resultante é mais fácil de interpretar. O intervalo dos valores plotados é 0.327, 48.885. Na base 10, isso se torna um intervalo que inclui muito poucos números inteiros: apenas 0 e 1. Com a base dois, nosso intervalo inclui -2, -1, 0, 1, 2, 3, 4 e 5. É mais fácil de calcular \\(2^x\\) e \\(10^x\\) quando \\(x\\) é um número inteiro e está entre -10 e 10, portanto, preferimos ter números inteiros menores na escala. Outra conseqüência de um intervalo limitado é que escolher a largura da bandeja (binwidth em inglês) é mais difícil. Com o log de base 2, sabemos que uma largura de compartimento de 1 se tornará um compartimento com alcance \\(x\\) para \\(2x\\). Para um exemplo em que a base 10 faz mais sentido, considere o tamanho da população. Um logaritmo de base 10 é preferível, pois o intervalo para eles é: filter(gapminder, year == past_year) %&gt;% summarize(min = min(population), max = max(population)) #&gt; min max #&gt; 1 46075 8.09e+08 Abaixo está o histograma dos valores transformados: gapminder %&gt;% filter(year == past_year) %&gt;% ggplot(aes(log10(population))) + geom_histogram(binwidth = 0.5, color = &quot;black&quot;) No gráfico acima, vemos rapidamente que as populações dos países variam entre dez bilhões e dez bilhões. 9.5.3 Transformar valores ou escala? Existem duas maneiras de usar transformações logarítmicas em gráficos. Podemos pegar o logaritmo dos valores antes de representá-los graficamente ou usar escalas logarítmicas nos eixos. Ambas as abordagens são úteis e têm vantagens diferentes. Se tomarmos o logaritmo dos dados, podemos interpretar mais facilmente os valores intermediários na escala. Por exemplo, se virmos: ----1----x----2--------3---- para dados transformados com o logaritmo, sabemos que o valor de \\(x\\) é aproximadamente 1,5. Se usarmos escalas logarítmicas: ----1----x----10------100--- para determinar x precisamos calcular \\(10^{1.5}\\), o que não é fácil de fazer mentalmente. A vantagem de usar escalas logarítmicas é que vemos os valores originais nos eixos. No entanto, a vantagem de exibir escalas logarítmicas é que os valores originais são mostrados no gráfico e são mais fáceis de interpretar. Por exemplo, veríamos “US $ 32 por dia” em vez de “5 bases de log US $ 2 por dia”. Como aprendemos anteriormente, se queremos escalar o eixo com logaritmos, podemos usar a função scale_x_continuous. Em vez de primeiro pegar o logaritmo dos valores, aplicamos esta camada: gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% ggplot(aes(dollars_per_day)) + geom_histogram(binwidth = 1, color = &quot;black&quot;) + scale_x_continuous(trans = &quot;log2&quot;) Observe que a transformação logarítmica da base 10 tem sua própria função: scale_x_log10(), mas atualmente a base 2 não, embora possamos definir facilmente uma. Outras transformações estão disponíveis através do argumento trans. Como aprenderemos mais adiante, a transformação da raiz quadrada ( sqrt) é útil ao considerar contagens. A transformação logística ( logit) é útil ao plotar proporções entre 0 e 1. A transformação reverse é útil quando queremos que os menores valores estejam à direita ou acima. 9.6 Como visualizar distribuições multimodais No histograma acima, vemos dois solavancos: um aos 4 e outro aos 32. Estatisticamente, esses solos às vezes são chamados de modes. O modo de uma distribuição é o valor com a frequência mais alta. O modo de distribuição normal é a média. Quando uma distribuição, como a acima, não diminui monotonicamente na moda, chamamos os lugares onde ela sobe e desce novamente - moda local - e dizemos que a distribuição tem - moda múltipla. O histograma acima sugere que a distribuição de renda dos países em 1970 possui dois modos: um de aproximadamente US $ 2 por dia (1 na escala log 2) e outro de aproximadamente US $ 32 por dia (5 na escala log 2) . Essa bimodalidade é consistente com um mundo dicotômico composto por países com renda média inferior a US $ 8 (3 na escala log 2) por dia e países acima disso. 9.7 Como comparar várias distribuições com gráficos de caixas e gráficos ridge De acordo com o histograma, os valores da distribuição de renda para 1970 mostram uma dicotomia. No entanto, o histograma não nos mostra se os dois grupos de países estão no oeste ou em parte do mundo em desenvolvimento. Vamos começar analisando rapidamente os dados por região. Reorganizamos as regiões pela mediana e usamos uma escala logarítmica. gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% mutate(region = reorder(region, dollars_per_day, FUN = median)) %&gt;% ggplot(aes(dollars_per_day, region)) + geom_point() + scale_x_continuous(trans = &quot;log2&quot;) Já podemos ver que existe de fato uma dicotomia “oeste versus o resto”: existem dois grupos claros, com o rico grupo constituído pela América do Norte, Europa do Norte e Ocidental, Nova Zelândia e Austrália. Definimos grupos com base nesta observação: gapminder &lt;- gapminder %&gt;% mutate(group = case_when( region %in% c(&quot;Western Europe&quot;, &quot;Northern Europe&quot;,&quot;Southern Europe&quot;, &quot;Northern America&quot;, &quot;Australia and New Zealand&quot;) ~ &quot;West&quot;, region %in% c(&quot;Eastern Asia&quot;, &quot;South-Eastern Asia&quot;) ~ &quot;East Asia&quot;, region %in% c(&quot;Caribbean&quot;, &quot;Central America&quot;, &quot;South America&quot;) ~ &quot;Latin America&quot;, continent == &quot;Africa&quot; &amp; region != &quot;Northern Africa&quot; ~ &quot;Sub-Saharan&quot;, TRUE ~ &quot;Others&quot;)) Nós convertemos essa variável group em um fator para controlar a ordem dos níveis: gapminder &lt;- gapminder %&gt;% mutate(group = factor(group, levels = c(&quot;Others&quot;, &quot;Latin America&quot;, &quot;East Asia&quot;, &quot;Sub-Saharan&quot;, &quot;West&quot;))) Na próxima seção, mostramos como visualizar e comparar distribuições entre grupos. 9.7.1 Diagrama de caixa A análise exploratória dos dados acima revelou duas características sobre a distribuição de renda média em 1970. Usando um histograma, encontramos uma distribuição bimodal com modos relacionados a países ricos e pobres. Agora, queremos comparar a distribuição entre esses cinco grupos para confirmar a dicotomia “oeste versus repouso”. O número de pontos em cada categoria é grande o suficiente para que um gráfico de resumo possa ser útil. Poderíamos gerar cinco histogramas ou cinco gráficos de densidade, mas pode ser mais prático ter todos os resumos visuais em um gráfico. Portanto, começamos empilhando diagramas de caixas um ao lado do outro. Observe que adicionamos a camada theme(axis.text.x = element_text(angle = 90, hjust = 1)) para que os rótulos dos grupos sejam verticais, pois não cabem se os mostrarmos horizontalmente e para removê-los do eixo para liberar espaço. p &lt;- gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% ggplot(aes(group, dollars_per_day)) + geom_boxplot() + scale_y_continuous(trans = &quot;log2&quot;) + xlab(&quot;&quot;) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) p Os diagramas de caixa têm a limitação de que, resumindo os dados em cinco números, recursos importantes dos dados podem ser perdidos. Uma maneira de evitar isso é exibindo os dados. p + geom_point(alpha = 0.5) Gráficos ### ridge Mostrar cada ponto individual nem sempre revela características importantes da distribuição. Embora esse não seja o caso aqui, quando o número de pontos de dados é muito grande, acabamos sobrescrevendo e exibindo os dados pode sair pela culatra. Os diagramas de caixa ajudam nisso, fornecendo um resumo de cinco números, mas isso também tem limitações. Por exemplo, gráficos de caixa não revelam distribuições bimodais. Para ver isso, observe os dois gráficos abaixo que resumem o mesmo conjunto de dados: Nos casos em que estamos preocupados que o resumo do gráfico de caixa seja muito simplista, podemos exibir densidades suaves ou histogramas empilhados usando ridge graphs. Como estamos acostumados a visualizar densidades com valores no eixo x, as empilhamos verticalmente. Além disso, como precisamos de mais espaço nessa abordagem, é conveniente cobri-los. O pacote ggridges inclui uma função conveniente para fazer isso. Abaixo estão os dados da receita, mostrados acima com gráficos de caixa, mas agora exibidos com uma crista gráfica _. library(ggridges) p &lt;- gapminder %&gt;% filter(year == past_year &amp; !is.na(dollars_per_day)) %&gt;% ggplot(aes(dollars_per_day, group)) + scale_x_continuous(trans = &quot;log2&quot;) p + geom_density_ridges() Lembre-se de que precisamos investir o x e y que foram usados para o gráfico da caixa. Um parâmetro útil de geom_density_ridges é scale, o que lhes permite determinar quanto se sobrepõem; por exemplo, scale = 1 significa que não há sobreposição. Valores maiores que 1 resultam em maior sobreposição. Se o número de pontos de dados for pequeno o suficiente, podemos adicioná-los ao gráfico ridge usando o seguinte código: p + geom_density_ridges(jittered_points = TRUE) Por padrão, a altura dos pontos é jittered e não deve ser interpretada de forma alguma. Para exibir pontos de dados, mas sem usar jitter, podemos usar o código a seguir para adicionar o que é conhecido como uma representação rug dos dados. p + geom_density_ridges(jittered_points = TRUE, position = position_points_jitter(height = 0), point_shape = &#39;|&#39;, point_size = 3, point_alpha = 1, alpha = 0.7) 9.7.2 Exemplo: distribuições de renda 1970 versus 2010 A exploração dos dados mostra claramente que em 1970 havia uma dicotomia “oeste versus o resto”. Mas essa dicotomia persiste? Vamos usar facet_grid para ver como as distribuições mudaram. Para começar, focamos em dois grupos: o oeste e o resto. Fazemos quatro histogramas. past_year &lt;- 1970 present_year &lt;- 2010 years &lt;- c(past_year, present_year) gapminder %&gt;% filter(year %in% years &amp; !is.na(gdp)) %&gt;% mutate(west = ifelse(group == &quot;West&quot;, &quot;West&quot;, &quot;Developing&quot;)) %&gt;% ggplot(aes(dollars_per_day)) + geom_histogram(binwidth = 1, color = &quot;black&quot;) + scale_x_continuous(trans = &quot;log2&quot;) + facet_grid(year ~ west) Antes de interpretar as conclusões deste gráfico, notamos que há mais países representados nos histogramas de 2010 do que em 1970: a contagem total é maior. Uma razão para isso é que vários países foram fundados após 1970. Por exemplo, a União Soviética foi dividida em diferentes países durante os anos 90. Outra razão é que mais dados estão disponíveis para mais países em 2010. Nós refizemos os gráficos usando apenas países com dados disponíveis para os dois anos. Na parte data wrangling deste livro, aprenderemos como usar as ferramentas tidyverse que nos permitirão escrever código eficiente para isso, mas aqui podemos usar código simples usando a função intersect: country_list_1 &lt;- gapminder %&gt;% filter(year == past_year &amp; !is.na(dollars_per_day)) %&gt;% pull(country) country_list_2 &lt;- gapminder %&gt;% filter(year == present_year &amp; !is.na(dollars_per_day)) %&gt;% pull(country) country_list &lt;- intersect(country_list_1, country_list_2) Estes 108 constituir 86% da população mundial, portanto esse subconjunto deve ser representativo. Vamos refazer o gráfico, mas apenas para este subconjunto simplesmente adicionando country %in% country_list para a função filter: Agora vemos que os países ricos ficaram um pouco mais ricos, mas em termos de porcentagem, os países pobres parecem ter melhorado mais. Em particular, vemos que a proporção de países em desenvolvimento que ganha mais de US $ 16 por dia aumentou substancialmente. Para ver quais regiões específicas melhoraram mais, podemos refazer os boxplots que fizemos anteriormente, mas agora adicionamos o ano de 2010 e usamos facet para comparar os dois anos. gapminder %&gt;% filter(year %in% years &amp; country %in% country_list) %&gt;% ggplot(aes(group, dollars_per_day)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + scale_y_continuous(trans = &quot;log2&quot;) + xlab(&quot;&quot;) + facet_grid(. ~ year) Aqui, fazemos uma pausa para introduzir outro recurso importante do ggplot2. Como queremos comparar cada região antes e depois, seria conveniente ter o diagrama de caixa de 1970 próximo ao de 2010 para cada região. Em geral, as comparações são mais fáceis quando os dados são plotados um ao lado do outro. Portanto, em vez de nos separarmos em facetas, mantemos os dados de cada ano juntos e pedimos para colorir (ou preenchê-los) por ano. Observe que os grupos são separados automaticamente por ano e cada par de diagramas de caixas é desenhado lado a lado. Como o ano é um número, nós o tornamos um fator, já que ggplot2 atribui automaticamente uma cor a cada categoria de um fator. Lembre-se que temos que converter as colunas year de numérico para fator. gapminder %&gt;% filter(year %in% years &amp; country %in% country_list) %&gt;% mutate(year = factor(year)) %&gt;% ggplot(aes(group, dollars_per_day, fill = year)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + scale_y_continuous(trans = &quot;log2&quot;) + xlab(&quot;&quot;) Finalmente, destacamos que, se estivermos mais interessados em comparar os valores antes e depois, pode fazer mais sentido traçar aumentos percentuais. Ainda não estamos prontos para aprender como codificar isso, mas é assim que o gráfico seria: A exploração de dados anteriores sugere que a diferença de renda entre países ricos e pobres diminuiu consideravelmente nos últimos 40 anos. Usamos uma série de histogramas e gráficos de caixa para ver isso. Sugerimos uma maneira sucinta de transmitir essa mensagem com apenas um gráfico. Vamos começar observando que os gráficos de densidade para a distribuição de renda em 1970 e 2010 eles transmitem a mensagem de que a lacuna está se fechando: gapminder %&gt;% filter(year %in% years &amp; country %in% country_list) %&gt;% ggplot(aes(dollars_per_day)) + geom_density(fill = &quot;grey&quot;) + scale_x_continuous(trans = &quot;log2&quot;) + facet_grid(. ~ year) No gráfico de 1970 vemos duas tendências claras: países pobres e ricos. No 2010 alguns dos países pobres parecem ter mudado para a direita, diminuindo a diferença. A próxima mensagem que devemos transmitir é que a razão para essa mudança na distribuição é que vários países pobres ficaram mais ricos do que alguns países ricos ficaram mais pobres. Para fazer isso, podemos atribuir uma cor aos grupos que identificamos durante a exploração de dados. No entanto, precisamos primeiro aprender a suavizar essas densidades de maneira a preservar informações sobre o número de países em cada grupo. Para entender por que precisamos disso, lembre-se da discrepância no tamanho de cada grupo: Developing West 87 21 Porém, quando sobrepomos duas densidades, o comportamento padrão é que a área representada por cada distribuição some 1, independentemente do tamanho de cada grupo: gapminder %&gt;% filter(year %in% years &amp; country %in% country_list) %&gt;% mutate(group = ifelse(group == &quot;West&quot;, &quot;West&quot;, &quot;Developing&quot;)) %&gt;% ggplot(aes(dollars_per_day, fill = group)) + scale_x_continuous(trans = &quot;log2&quot;) + geom_density(alpha = 0.2) + facet_grid(year ~ .) O gráfico acima mostra que há o mesmo número de países em cada grupo. Para mudar isso, precisaremos aprender como acessar as variáveis calculadas com a função geom_density. 9.7.3 Como acessar variáveis calculadas Para tornar as áreas dessas densidades proporcionais ao tamanho do grupo, simplesmente multiplicamos os valores do eixo y pelo tamanho do grupo. No arquivo de ajuda de geom_density, vemos que as funções calculam uma variável chamada count quem faz exatamente isso. Queremos que essa variável, e não a densidade, esteja no eixo y. Em ggplot2, obtemos acesso a essas variáveis cercando o nome com dois pontos. Portanto, usaremos o seguinte mapeamento: aes(x = dollars_per_day, y = ..count..) Agora podemos criar o diagrama desejado simplesmente alterando o mapeamento no snippet de código acima. Também estenderemos os limites do eixo x. p &lt;- gapminder %&gt;% filter(year %in% years &amp; country %in% country_list) %&gt;% mutate(group = ifelse(group == &quot;West&quot;, &quot;West&quot;, &quot;Developing&quot;)) %&gt;% ggplot(aes(dollars_per_day, y = ..count.., fill = group)) + scale_x_continuous(trans = &quot;log2&quot;, limit = c(0.125, 300)) p + geom_density(alpha = 0.2) + facet_grid(year ~ .) Se queremos que as densidades sejam mais suaves, usamos o argumento bw para que o mesmo parâmetro de suavização seja usado para cada densidade. Selecionamos 0,75 após testar vários valores. p + geom_density(alpha = 0.2, bw = 0.75) + facet_grid(year ~ .) Este gráfico agora mostra o que está acontecendo muito claramente. A distribuição do mundo em desenvolvimento está mudando. Uma terceira tendência aparece, composta pelos países que mais diminuíram a diferença. Para visualizar se algum dos grupos definidos acima é a principal causa dessas alterações, podemos criar rapidamente um gráfico ridge: gapminder %&gt;% filter(year %in% years &amp; !is.na(dollars_per_day)) %&gt;% ggplot(aes(dollars_per_day, group)) + scale_x_continuous(trans = &quot;log2&quot;) + geom_density_ridges(adjust = 1.5) + facet_grid(. ~ year) Outra maneira de conseguir isso é empilhando as densidades umas sobre as outras: gapminder %&gt;% filter(year %in% years &amp; country %in% country_list) %&gt;% group_by(year) %&gt;% mutate(weight = population/sum(population)*2) %&gt;% ungroup() %&gt;% ggplot(aes(dollars_per_day, fill = group)) + scale_x_continuous(trans = &quot;log2&quot;, limit = c(0.125, 300)) + geom_density(alpha = 0.2, bw = 0.75, position = &quot;stack&quot;) + facet_grid(year ~ .) Aqui podemos ver claramente como as distribuições para o leste da Ásia, América Latina e outros se deslocam visivelmente para a direita. Enquanto a África subsaariana permanece estagnada. Observe que ordenamos os níveis do grupo para que a densidade do Ocidente seja grafada primeiro, depois na África Subsaariana. Tendo ambas as extremidades representadas graficamente primeiro, permite-nos ver melhor a bimodalidade restante. 9.7.4 Densidades ponderadas Como ponto final, notamos que essas distribuições pesam cada país igualmente. Portanto, se a maioria da população está melhorando, mas morando em um país muito grande, como a China, talvez não gostemos disso. De fato, podemos ponderar as densidades suaves usando o argumento de mapeamento weight. O gráfico fica assim: Esta figura em particular mostra muito claramente como a lacuna na distribuição de renda está diminuindo e que a maioria dos países ainda em situação de pobreza está na África Subsaariana. 9.8 A falácia ecológica e a importância de mostrar os dados Ao longo desta seção, comparamos regiões do mundo. Vimos que, em média, algumas regiões têm melhor desempenho que outras. Nesta seção, vamos nos concentrar na descrição da importância da variabilidade dentro dos grupos, examinando a relação entre as taxas de mortalidade infantil de um país e a renda média. Definimos mais algumas regiões e comparamos as médias entre as regiões: A relação entre essas duas variáveis é quase perfeitamente linear e o gráfico mostra uma diferença dramática. Enquanto menos de 0,5% dos bebês morrem no Ocidente, na África Subsaariana a taxa é superior a 6%! Observe que o gráfico usa uma nova transformação, a transformação logística. 9.8.1 Transformação de logística Transformação logística ou logit para uma proporção ou taxa \\(p\\) é definido como: \\[f(p) = \\log \\left( \\frac{p}{1-p} \\right)\\] Quando \\(p\\) é uma proporção ou probabilidade, a quantidade que transformamos com o logaritmo, \\(p/(1-p)\\), é chamado odds. Neste caso \\(p\\) é a proporção de bebês que sobreviveram. Os filhos nos dizem quantos mais bebês devem sobreviver à morte. A transformação logarítmica torna simétrica. Se as taxas forem iguais, as log odds serão 0. Aumentos multiplicativos são convertidos em aumentos positivos ou negativos, respectivamente. Essa escala é útil quando queremos destacar diferenças próximas de 0 ou 1. Para taxas de sobrevivência, isso é importante porque uma taxa de sobrevivência de 90% é inaceitável, enquanto uma sobrevivência de 99% é relativamente boa. Preferiríamos muito uma taxa de sobrevivência próxima de 99,9%. Queremos que nossa escala destaque essas diferenças e o logit o faz. Lembre-se de que 99,9/ 0,1 é aproximadamente 10 vezes maior que 99/1, que é aproximadamente 10 vezes maior que 90/10. Usando o logaritmo, esses incrementos multiplicativos são convertidos em aumentos constantes. 9.8.2 Mostrar os dados Agora, de volta ao nosso gráfico. Com base no gráfico acima, concluímos que um país de baixa renda está destinado a ter uma baixa taxa de sobrevivência? Além disso, concluímos que as taxas de sobrevivência na África subsaariana são mais baixas do que no sul da Ásia, que por sua vez são mais baixas do que nas ilhas do Pacífico e assim por diante? Saltar para esta conclusão com base em um gráfico mostrando médias é chamado de falácia ecológica. A relação quase perfeita entre taxas de sobrevivência e renda é observada apenas para médias regionais. Depois de mostrar todos os dados, vemos uma história mais complicada: Especificamente, vemos que há muita variabilidade. Vemos que os países nas mesmas regiões podem ser bem diferentes e que os países com a mesma renda podem ter taxas de sobrevivência diferentes. Por exemplo, enquanto a África Subsaariana teve os piores resultados econômicos e de saúde em média, há uma grande variabilidade dentro desse grupo. Maurício e Botsuana são melhores que Angola e Serra Leoa, com Maurício comparável aos países ocidentais. https://en.wikipedia.org/wiki/Hans_Rosling↩ http://www.gapminder.org/↩ https://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en↩ https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen↩ "],
["princípios-de-visualização-de-dados.html", "Capítulo 10 Princípios de visualização de dados 10.1 Como codificar dados usando pistas visuais 10.2 Saiba quando incluir 0 10.3 Não distorça quantidades 10.4 Classificar categorias por valor significativo 10.5 Mostrar os dados 10.6 Como facilitar comparações 10.7 Pense no daltônico 10.8 Gráficos para duas variáveis 10.9 Como codificar uma terceira variável 10.10 Evitar gráficos pseudo-tridimensionais 10.11 Evite muitos dígitos significativos 10.12 Conheça o seu público 10.13 Exercícios 10.14 Estudo de caso: vacinas e doenças infecciosas 10.15 Exercícios", " Capítulo 10 Princípios de visualização de dados Já definimos algumas regras a serem seguidas ao criar gráficos para nossos exemplos. Aqui, nosso objetivo é oferecer alguns princípios gerais que podemos usar como um guia para uma visualização eficaz dos dados. Grande parte desta seção é baseada em uma palestra de Karl Broman30 intitulada “Criando figuras e tabelas eficazes”31 e inclui algumas das figuras que foram criadas com o código que Karl disponibiliza em seu repositório GitHub32, bem como as notas da classe &quot; Introdução à visualização de dados &quot;, de Peter Aldhous33. Seguindo a abordagem de Karl, mostramos alguns exemplos de estilos de gráfico a serem evitados, explicamos como melhorá-los e depois os usamos como motivação para uma lista de princípios. Além disso, comparamos e contrastamos gráficos que seguem esses princípios com outros que os ignoram. Os princípios são baseados principalmente em pesquisas relacionadas à forma como os seres humanos detectam padrões e fazem comparações visuais. As abordagens preferidas são as que melhor se adaptam à maneira como nossos cérebros processam as informações visuais. Ao escolher as ferramentas de visualização, é importante ter em mente nosso objetivo. Podemos comparar um número suficientemente pequeno de números que podem ser distinguidos, descrevendo distribuições de dados categóricos ou valores numéricos, comparando os dados de dois grupos ou descrevendo a relação entre duas variáveis e isso afeta a apresentação que escolheremos. Como observação final, queremos enfatizar que é importante para os cientistas de dados adaptar e otimizar gráficos para o público. Por exemplo, um gráfico exploratório feito para nós será diferente de uma tabela destinada a comunicar uma descoberta a um público em geral. Vamos usar estes pacotes: library(tidyverse) library(dslabs) library(gridExtra) 10.1 Como codificar dados usando pistas visuais Começamos descrevendo alguns princípios para codificar dados. Existem várias abordagens à nossa disposição, incluindo posição, comprimento, ângulos, área, brilho e tom da cor. Para ilustrar como algumas dessas estratégias se comparam, suponha que desejamos relatar os resultados de duas pesquisas hipotéticas, realizadas em 2000 e depois em 2015, em relação à preferência do navegador. Para cada ano, estamos simplesmente comparando cinco quantidades: as cinco porcentagens. Uma representação gráfica de porcentagens amplamente usada e popularizada pelo Microsoft Excel é o gráfico de pizza: Aqui estamos representando quantidades com áreas e ângulos, pois o ângulo e a área de cada seção do gráfico são proporcionais à quantidade que o setor representa. Isso acaba sendo uma opção abaixo do ideal, pois, como mostram os estudos perceptivos, os seres humanos não são bons em quantificar com precisão ângulos e são ainda piores quando a área é o único sinal visual disponível. O gráfico em anel é um exemplo de gráfico que usa apenas a área: Para ver como é difícil quantificar ângulos e área, lembre-se de que as classificações e todas as porcentagens nos gráficos acima foram alteradas de 2000 para 2015. Você pode determinar as porcentagens reais e classificar a popularidade dos navegadores? Você pode ver como as porcentagens mudaram de 2000 para 2015? Não é fácil diferenciá-lo do gráfico. De fato, a função pie da página de ajuda de R observa que: Os gráficos de pizza são uma maneira muito ruim de exibir informações. O olho é bom em julgar medições lineares e ruim em julgar áreas relativas. Um diagrama de barras ou pontos é uma maneira preferível de exibir esse tipo de dados. Nesse caso, simplesmente exibir os números não é apenas mais claro, mas também reduziria os custos de impressão se você imprimir uma cópia em papel: Browser 2000 2015 Opera 3 2 Safari 21 22 Firefox 23 21 Chrome 26 29 IE 28 27 A maneira preferida de representar graficamente essas quantidades é usar comprimento e posição como pistas visuais, uma vez que os humanos são muito melhores em julgar medições lineares. O gráfico de barras usa essa abordagem usando barras de comprimento proporcional às quantias de interesse. Ao adicionar linhas horizontais aos valores escolhidos estrategicamente, neste caso, a cada múltiplo de 10, aliviaremos a carga visual da quantificação através da posição da parte superior das barras. Compare e compare as informações que podemos extrair dos dois pares de gráficos a seguir. Observe como é fácil ver as diferenças no gráfico de barras. De fato, agora podemos determinar as porcentagens reais seguindo uma linha horizontal para o eixo x. Se, por algum motivo, você precisar criar um gráfico circular, rotule cada seção do círculo com sua respectiva porcentagem para que o público não precise inferi-las a partir dos ângulos ou da área: Em geral, ao exibir quantidades, posição e comprimento sobre ângulos e/ ou área são os preferidos. O brilho e a cor são ainda mais difíceis de quantificar do que os ângulos. Mas, como veremos mais adiante, às vezes são úteis quando mais de duas dimensões devem ser exibidas ao mesmo tempo. 10.2 Saiba quando incluir 0 Ao usar gráficos de barras, é errado não iniciar as barras em 0. Isso ocorre porque, ao usar um gráfico de barras, estamos sugerindo que o comprimento é proporcional às quantidades mostradas. Ao evitar 0, você pode fazer com que diferenças relativamente pequenas pareçam muito maiores do que realmente são. Essa abordagem é frequentemente usada por políticos ou pela mídia que tentam exagerar a diferença. Abaixo está um exemplo ilustrativo usado por Peter Aldhous nesta palestra: http://paldhous.github.io/ucb/2016/dataviz/week2.htmlfont^^ http://paldhous.github.io/ucb/2016/ dataviz/ week2.html]. (Fonte: Fox News, via Media Matters34.) No gráfico acima, as detenções parecem ter triplicado quando, de fato, elas aumentaram apenas aproximadamente 16%. Iniciar o gráfico em 0 ilustra isso claramente: Abaixo, vemos outro exemplo, que é descrito em detalhes em um artigo do blog “Flowing Data”: (Fonte: Fox News, através da Flowing Data35) Este gráfico faz um aumento de 13% parecer cinco vezes maior. Aqui está o gráfico apropriado: Finalmente, aqui está um exemplo extremo que faz uma diferença muito pequena de menos de 2% parecer 10 a 100 vezes maior: (Fonte: Televisão venezuelana via Pakistan Today36 e Diego Mariano) Aqui está o gráfico apropriado: Ao usar a posição em vez do comprimento, não é necessário incluir 0. Esse é particularmente o caso quando queremos comparar as diferenças entre os grupos em relação à variabilidade dentro de um grupo. Aqui está um exemplo ilustrativo que mostra a expectativa média de vida de cada país estratificado por continente em 2012: Observe que, no gráfico à esquerda, que inclui 0, o espaço entre 0 e 43 não adiciona informações e dificulta a comparação da variabilidade entre e dentro do grupo. 10.3 Não distorça quantidades Durante o discurso do Estado da União de Barack Obama em 2011, o gráfico a seguir foi usado para comparar o PIB dos EUA. EUA com o PIB de quatro nações concorrentes: (Fonte: endereço do estado da união de 201137) Se julgarmos pela área dos círculos, os Estados Unidos parecem ter uma economia cinco vezes maior que a da China e mais de 30 vezes maior que a da França. No entanto, se olharmos para os números atuais, veremos que esse não é o caso. As proporções são 2,6 e 5,8 vezes superiores às da China e da França, respectivamente. A razão para essa distorção é que o raio do círculo, em vez da área, se tornou proporcional à quantidade, o que implica que a razão entre as áreas é quadrada: 2,6 se torna 6,5 e 5,8 se torna 34,1. Aqui está uma comparação dos círculos que obtemos se tornarmos o valor proporcional ao raio e à área: Não é surpresa que, por padrão, ggplot2 use a área em vez do raio. No entanto, nesse caso, realmente não devemos usar a área, pois podemos usar a posição e o comprimento: 10.4 Classificar categorias por valor significativo Quando um dos eixos é usado para exibir categorias, como é feito nos gráficos de barras, o comportamento padrão de ggplot2 é classificar as categorias em ordem alfabética quando definidas por cadeias de caracteres. Se eles são definidos por fatores, são ordenados de acordo com os níveis dos fatores. Raramente queremos usar a ordem alfabética. Em vez disso, devemos solicitar uma quantidade significativa. Em todos os casos anteriores, os gráficos de barras foram ordenados de acordo com os valores mostrados. A exceção foram gráficos de barras comparando navegadores. Nesse caso, mantivemos a ordem igual em todos os gráficos de barras para facilitar a comparação. Especificamente, em vez de solicitar navegadores separadamente nos dois anos, solicitamos os dois anos pelo valor médio de 2000 e 2015. Anteriormente, aprendemos a usar a função reorder, o que nos ajuda a alcançar esse objetivo. Para avaliar como a ordem correta pode ajudar a transmitir uma mensagem, suponha que desejamos criar um gráfico para comparar a taxa de homicídios em todos os estados dos EUA.estamos particularmente interessados nos estados mais perigosos e mais seguros. Lembre-se da diferença quando solicitamos alfabeticamente, a ação padrão, versus quando solicitamos pela taxa real: Podemos fazer o segundo gráfico assim: data(murders) murders %&gt;% mutate(murder_rate = total/ population * 100000) %&gt;% mutate(state = reorder(state, murder_rate)) %&gt;% ggplot(aes(state, murder_rate)) + geom_bar(stat=&quot;identity&quot;) + coord_flip() + theme(axis.text.y = element_text(size = 6)) + xlab(&quot;&quot;) A função reorder também nos permite reordenar grupos. Anteriormente, vimos um exemplo relacionado à distribuição de renda entre regiões. Aqui vemos as duas versões representadas graficamente uma contra a outra: O primeiro gráfico classifica as regiões em ordem alfabética, enquanto o segundo gráfico as classifica pela mediana do grupo. 10.5 Mostrar os dados Nosso foco foi exibir quantidades únicas em todas as categorias. Agora, voltamos nossa atenção para a visualização de dados, com foco na comparação de grupos. Para motivar nosso primeiro princípio, “mostrar os dados”, retornamos ao nosso exemplo artificial de descrição de alturas para ET, um alienígena. Desta vez, suponha que o ET esteja interessado na diferença de altura entre homens e mulheres. Um gráfico comumente usado para comparações de grupo e popularizado por software como o Microsoft Excel, é o dynamite plot, que mostra os erros médios e padrão (os erros padrão são definidos em um capítulo posterior, mas não os confundem com o desvio padrão do dados). O gráfico fica assim: A média de cada grupo é representada pelo topo de cada barra e as antenas se estendem da média para a média mais dois erros padrão. Se tudo o que o ET obtiver for esse gráfico, você terá poucas informações sobre o que esperar se encontrar um grupo de homens e mulheres. As barras vão para 0: isso significa que existem seres humanos pequenos com menos de um pé de altura? Todos os homens são mais altos que as mulheres mais altas? Existe uma variedade de alturas? O ET é incapaz de responder a essas perguntas, pois mal lhe fornecemos informações sobre a distribuição da altura. Isso nos leva ao nosso primeiro princípio: exibir os dados. Esse código ggplot2 simples já gera um gráfico mais informativo que o gráfico de barras, exibindo todos os pontos de dados: heights %&gt;% ggplot(aes(sex, height)) + geom_point() O gráfico acima nos dá uma idéia do alcance dos dados. No entanto, este gráfico também possui limitações, pois não podemos realmente ver toda a soma FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE y 812pontos plotados para mulheres e homens, respectivamente, e muitos pontos são plotados um sobre o outro. Como descrevemos anteriormente, visualizar a distribuição é muito mais informativo. Porém, antes de fazer isso, apontamos duas maneiras de melhorar um gráfico que mostra todos os pontos. O primeiro é adicionar jitter, que adiciona um pequeno deslocamento aleatório a cada ponto. Nesse caso, adicionar horizontal jitter não altera a interpretação, pois as alturas dos pontos não mudam, mas minimizamos o número de pontos que se sobrepõem e, portanto, temos uma melhor ideia visual de como os dados são distribuídos. Uma segunda melhoria vem do uso de alpha blending, que torna os pontos um pouco transparentes. Quanto mais pontos se sobrepuserem, mais escuro será o gráfico, o que também nos ajudará a ter uma idéia de como os pontos são distribuídos. Aqui está o mesmo gráfico com jitter_e_alpha blending: heights %&gt;% ggplot(aes(sex, height)) + geom_jitter(width = 0.1, alpha = 0.2) Agora começamos a sentir que, em média, os machos são mais altos que as fêmeas. Também observamos faixas pontilhadas horizontais escuras, o que mostra que muitos alunos indicam valores arredondados para o número inteiro mais próximo. 10.6 Como facilitar comparações 10.6.1 Use eixos comuns Como existem muitos pontos, é mais eficaz mostrar distribuições do que pontos individuais. Portanto, mostramos histogramas para cada grupo: No entanto, olhando para o gráfico acima, não é imediatamente óbvio que os homens são, em média, mais altos que as mulheres. Temos que observar com atenção para perceber que o eixo x tem uma faixa de valores mais alta no histograma masculino. Um princípio importante aqui é manter os eixos iguais ao comparar dados em dois gráficos. A seguir, vemos como a comparação se torna mais fácil: 10.6.2 Alinhe os gráficos verticalmente para ver alterações horizontais e horizontalmente para ver alterações verticais Nesses histogramas, o sinal visual relacionado a reduções ou aumentos de altura são alterações à esquerda ou à direita, respectivamente: alterações horizontais. O alinhamento vertical dos gráficos nos ajuda a ver essa alteração quando os eixos são fixos: heights %&gt;% ggplot(aes(height, ..density..)) + geom_histogram(binwidth = 1, color=&quot;black&quot;) + facet_grid(sex~.) O gráfico acima facilita a observação de que os machos são, em média, mais altos. Se quisermos obter o resumo compacto que os boxplots oferecem, precisamos alinhá-los horizontalmente, pois, por padrão, os boxplots se movem para cima e para baixo com base nas mudanças de altura. Seguindo o princípio “mostrar os dados”, sobrepusemos todos os pontos de dados: heights %&gt;% ggplot(aes(sex, height)) + geom_boxplot(coef=3) + geom_jitter(width = 0.1, alpha = 0.2) + ylab(&quot;Height in inches&quot;) Agora compare e contraste estes três gráficos, com base exatamente nos mesmos dados: Observe quanto aprendemos mais nos dois gráficos à direita. Os gráficos de barras são úteis para exibir um número, mas não são muito úteis quando queremos descrever distribuições. 10.6.3 Considere transformações Incentivamos o uso de transformação logarítmica nos casos em que as alterações são multiplicativas. O tamanho da população foi um exemplo em que descobrimos que uma transformação logarítmica produziu uma transformação mais informativa. A combinação de um gráfico de barras escolhido incorretamente e a não utilização de uma transformação logarítmica, quando necessário, podem ser particularmente distorcidas. Como exemplo, considere este gráfico de barras mostrando os tamanhos médios da população para cada continente em 2015: Observando o gráfico acima, conclui-se que os países asiáticos são muito mais populosos do que os de outros continentes. Seguindo o princípio “mostrar os dados”, notamos rapidamente que isso se deve a dois países muito grandes, que assumimos serem a Índia e a China: Usar uma transformação logarítmica aqui produz um gráfico muito mais informativo. Comparamos o gráfico de barras original com um gráfico de caixa usando a transformação de escala logarítmica para o eixo y: Com o novo gráfico, percebemos que os países africanos realmente têm uma população mediana maior que a Ásia. Outras transformações a considerar são a transformação logística ( logit), que é útil para ver melhor as alterações nas probabilidades e a transformação da raiz quadrada ( sqrt), o que é útil para contagens. 10.6.4 As indicações visuais comparadas devem ser adjacentes Para cada continente, vamos comparar a renda em 1970 versus 2010. Ao comparar dados de renda entre regiões entre 1970 e 2010, fizemos um gráfico semelhante ao seguinte, mas desta vez investigamos os continentes em vez de regiões. O comportamento padrão de ggplot2 é classificar os rótulos em ordem alfabética para que os rótulos com 1970 apareçam antes dos rótulos com 2010. Isso dificulta as comparações porque a distribuição de um continente em 1970 é visualmente distante da sua distribuição em 2010. É muito mais fácil Faça a comparação entre 1970 e 2010 para cada continente quando as plotagens da caixa para esse continente estiverem próximas uma da outra: 10.6.5 Use cores A comparação é ainda mais fácil se usarmos cores para indicar as duas coisas que queremos comparar: 10.7 Pense no daltônico Cerca de 10% da população é daltônica. Infelizmente, as cores padrão usadas em ggplot2 não são ideais para este grupo. No entanto, ggplot2 facilita a alteração da paleta de cores usada nos gráficos. Aqui está um exemplo de como podemos usar uma paleta que considera daltônicos: [http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palettefont/5(http:// www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palette): color_blind_friendly_cols &lt;- c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) Aqui estão as cores: Além disso, existem vários recursos que podem ajudá-lo a selecionar cores, por exemplo, este: [http://bconnelly.net/2013/10/creating-colorblind-friendly-figures/font/1(http://bconnelly.net/2013/ 10/ create-colorblind-friendly-figures/). 10.8 Gráficos para duas variáveis Em geral, eles devem usar diagramas de dispersão para visualizar o relacionamento entre duas variáveis. Para cada caso em que examinamos a relação entre duas variáveis, incluindo mortes totais versus tamanho da população, expectativa de vida versus taxas de fertilidade e mortalidade infantil versus renda, usamos gráficos de dispersão e esse é o gráfico que geralmente recomendamos. No entanto, existem algumas exceções e aqui descrevemos dois gráficos alternativos: o slope chart_e o_Bland-Altman chart. 10.8.1 Gráficos de inclinação Uma exceção na qual outro tipo de gráfico pode ser mais informativo é quando variáveis do mesmo tipo são comparadas, mas em momentos diferentes e para um número relativamente pequeno de comparações. Por exemplo, se estivermos comparando a expectativa de vida entre 2010 e 2015. Nesse caso, poderíamos recomendar um slope chart. Não há geometria para os _slope charts_emggplot2, mas podemos construir uma usando geom_line. Precisamos fazer alguns ajustes para adicionar etiquetas. Aqui está um exemplo comparando 2010 a 2015 para os grandes países ocidentais: west &lt;- c(&quot;Western Europe&quot;,&quot;Northern Europe&quot;,&quot;Southern Europe&quot;, &quot;Northern America&quot;,&quot;Australia and New Zealand&quot;) dat &lt;- gapminder %&gt;% filter(year%in% c(2010, 2015) &amp; region %in% west &amp; !is.na(life_expectancy) &amp; population &gt; 10^7) dat %&gt;% mutate(location = ifelse(year == 2010, 1, 2), location = ifelse(year == 2015 &amp; country %in% c(&quot;United Kingdom&quot;, &quot;Portugal&quot;), location+0.22, location), hjust = ifelse(year == 2010, 1, 0)) %&gt;% mutate(year = as.factor(year)) %&gt;% ggplot(aes(year, life_expectancy, group = country)) + geom_line(aes(color = country), show.legend = FALSE) + geom_text(aes(x = location, label = country, hjust = hjust), show.legend = FALSE) + xlab(&quot;&quot;) + ylab(&quot;Life Expectancy&quot;) Uma vantagem do slope chart é que ele rapidamente nos dá uma idéia das mudanças com base na inclinação das linhas. Embora estejamos usando o ângulo como uma sugestão visual, também estamos usando a posição para determinar valores exatos. Comparar melhorias é um pouco mais difícil com um diagrama de dispersão: No diagrama de dispersão, seguimos o princípio de usar eixos comuns, porque os estamos comparando antes e depois. No entanto, se tivermos muitos pontos, os slope charts não serão mais úteis, pois é difícil ver todas as linhas. 10.8.2 Gráfico de Bland-Altman Como estamos interessados principalmente na diferença, faz sentido dedicar um de nossos eixos a ela. O gráfico de Bland-Altman, também conhecido como gráfico de diferença média de Tukey e MA plot, mostra a diferença em relação à média: library(ggrepel) dat %&gt;% mutate(year = paste0(&quot;life_expectancy_&quot;, year)) %&gt;% select(country, year, life_expectancy) %&gt;% spread(year, life_expectancy) %&gt;% mutate(average = (life_expectancy_2015 + life_expectancy_2010)/2, difference = life_expectancy_2015 - life_expectancy_2010) %&gt;% ggplot(aes(average, difference, label = country)) + geom_point() + geom_text_repel() + geom_abline(lty = 2) + xlab(&quot;Average of 2010 and 2015&quot;) + ylab(&quot;Difference between 2015 and 2010&quot;) Aqui, simplesmente olhando o eixo y, vemos rapidamente quais países mostraram a maior melhoria. Além disso, temos uma idéia do valor geral do eixo x. 10.9 Como codificar uma terceira variável Um gráfico de dispersão anterior mostrou a relação entre sobrevivência infantil e renda média. Abaixo está uma versão deste gráfico que codifica três variáveis: participação na OPEP, região e população. Codificamos variáveis categóricas com cor e forma. Essas formas podem ser controladas com o argumento shape. Abaixo, mostramos as formas disponíveis para uso em R. Nos últimos cinco, a cor preenche a forma. Para variáveis contínuas, podemos usar cor, intensidade ou tamanho. Aqui está um exemplo de como fazer isso com um estudo de caso. Ao selecionar cores para quantificar uma variável numérica, escolhemos entre duas opções: seqüencial e divergente. As cores seqüenciais são adequadas para dados que variam do mais alto ao mais baixo. Valores altos são claramente diferenciados de valores baixos. Aqui estão alguns exemplos oferecidos pelo pacote RColorBrewer: library(RColorBrewer) display.brewer.all(type=&quot;seq&quot;) Cores divergentes são usadas para representar valores que divergem de um centro. Colocamos ênfase igual nos dois extremos do intervalo de dados: mais alto que o centro e mais baixo que o centro. Um exemplo de quando usaríamos um padrão divergente seria quando mostramos a altura em quantos desvios padrão ela é da média. Aqui estão alguns exemplos de padrões divergentes: library(RColorBrewer) display.brewer.all(type=&quot;div&quot;) 10.10 Evitar gráficos pseudo-tridimensionais A figura a seguir, extraída da literatura científica38, mostra três variáveis: dose, tipo de medicamento e sobrevida. Embora suas telas ou páginas do livro sejam planas e bidimensionais, o gráfico tenta imitar três dimensões e atribui uma dimensão a cada variável. (Imagem cortesia de Karl Broman) Os seres humanos não são bons em ver em três dimensões (o que explica por que o estacionamento paralelo é difícil) e nossa limitação é ainda pior em relação às pseudo-três dimensões. Para ver isso, tente determinar os valores da variável de sobrevivência no gráfico acima. Você pode dizer quando a fita roxa cruza a fita vermelha? Este é um exemplo em que podemos usar cores facilmente para representar a variável categórica em vez de um pseudo-3D: Veja como é mais fácil determinar os valores de sobrevivência. Às vezes, o pseudo-3D é usado de forma totalmente gratuita - os gráficos são criados para parecer 3D, mesmo quando a terceira dimensão não representa uma quantidade. Isso apenas aumenta a confusão e dificulta a transmissão da sua mensagem. Aqui estão dois exemplos: (Imagens cortesia de Karl Broman) 10.11 Evite muitos dígitos significativos Por padrão, softwares estatísticos como R retornam muitos dígitos significativos. O comportamento padrão em R é exibir 7 dígitos significativos. Esse número de dígitos geralmente não adiciona informações e a desordem visual adicionada pode dificultar o entendimento da mensagem. Como exemplo, aqui estão as taxas de doença por 10.000 para a Califórnia em cinco décadas, calculadas a partir dos totais e da população com R: state year Measles Pertussis Polio California 1940 37.8826320 18.3397861 0.8266512 California 1950 13.9124205 4.7467350 1.9742639 California 1960 14.1386471 NA 0.2640419 California 1970 0.9767889 NA NA California 1980 0.3743467 0.0515466 NA Estamos relatando precisão de até 0,00001 casos por 10.000, um valor muito pequeno no contexto de alterações que ocorrem ao longo do tempo. Nesse caso, dois números significativos são mais que suficientes e indicam claramente que as taxas estão diminuindo: state year Measles Pertussis Polio California 1940 37.9 18.3 0.8 California 1950 13.9 4.7 2.0 California 1960 14.1 NA 0.3 California 1970 1.0 NA NA California 1980 0.4 0.1 NA Para alterar o número de dígitos significativos ou números redondos, usamos signif e round. Eles podem definir o número de dígitos significativos em todo o mundo, configurando opções como esta: options(digits = 3). Outro princípio relacionado à exibição de tabelas é colocar os valores que são comparados em colunas em vez de linhas. Observe que nossa tabela acima é mais fácil de ler do que esta: state disease 1940 1950 1960 1970 1980 California Measles 37.9 13.9 14.1 1 0.4 California Pertussis 18.3 4.7 NA NA 0.1 California Polio 0.8 2.0 0.3 NA NA 10.12 Conheça o seu público Os gráficos podem ser usados para 1) nossas próprias análises exploratórias de dados, 2) para transmitir uma mensagem aos especialistas ou 3) para ajudar a contar uma história para o público em geral. Certifique-se de que o público-alvo entenda cada elemento do gráfico. Como um exemplo simples, considere que pode ser mais útil para sua própria exploração transformar os dados logaritmicamente e depois fazer um gráfico. No entanto, para uma audiência geral que não está familiarizada com a conversão de valores logarítmicos em medições originais, será muito mais fácil entender o uso de uma escala logarítmica para o eixo, em vez de valores transformados logaritmicamente. 10.13 Exercícios Para esses exercícios, usaremos os dados da vacina no pacote dslabs: library(dslabs) data(us_contagious_diseases) 1. Os gráficos de pizza são adequados: para. Quando queremos mostrar porcentagens. b. Quando ggplot2 não estiver disponível. c. Quando estou jogando frisbee. d. Nunca. Gráficos e tabelas de barras são sempre melhores. 2. Qual é o problema com o gráfico a seguir? para. Os valores estão errados. A votação final foi 306-232. b. O eixo não começa em 0. A julgar pelo comprimento, parece que Trump recebeu três vezes mais votos quando, na verdade, ele era aproximadamente 30% a mais. c. As cores devem ser as mesmas. d. As porcentagens devem ser mostradas como um gráfico de pizza. 3. Veja os dois gráficos a seguir. Eles mostram a mesma informação: taxas de sarampo em 1928 em todos os 50 estados. Qual gráfico é mais fácil de ler se você deseja determinar quais são os melhores e os piores estados em termos de taxas e por quê? para. Eles dão a mesma informação, então ambos são igualmente bons. b. O gráfico à esquerda é melhor porque ordena os estados em ordem alfabética. c. O gráfico à direita é melhor porque a ordem alfabética não tem nada a ver com a doença e, ordenando de acordo com a taxa real, vemos rapidamente os estados com as taxas mais alta e mais baixa. d. Ambos os gráficos devem ser um gráfico de pizza. 4. Para fazer o gráfico à esquerda, precisamos reordenar os níveis das variáveis de estado. dat &lt;- us_contagious_diseases %&gt;% filter(year == 1967 &amp; disease==&quot;Measles&quot; &amp; !is.na(population)) %&gt;% mutate(rate = count/ population * 10000 * 52/ weeks_reporting) Lembre-se do que acontece quando criamos um gráfico de barras: dat %&gt;% ggplot(aes(state, rate)) + geom_bar(stat=&quot;identity&quot;) + coord_flip() Defina estes objetos: state &lt;- dat$state rate &lt;- dat$count/dat$population*10000*52/dat$weeks_reporting Redefinir o objeto state para que os níveis sejam reorganizados. Imprimir o novo objeto state e seus níveis para que você possa ver que os níveis não reorganizam o vetor. 5. Agora edite o código acima para redefinir dat para que os níveis da variável state reordenar por variável rate. Em seguida, faça um gráfico de barras usando o código acima, mas para este novo dat. 6. Digamos que você esteja interessado em comparar as taxas de homicídio por arma de fogo em todas as regiões dos EUA. EUA Veja este gráfico: library(dslabs) data(&quot;murders&quot;) murders %&gt;% mutate(rate = total/population*100000) %&gt;% group_by(region) %&gt;% summarize(avg = mean(rate)) %&gt;% mutate(region = factor(region)) %&gt;% ggplot(aes(region, avg)) + geom_bar(stat=&quot;identity&quot;) + ylab(&quot;Murder Rate Average&quot;) e decide se mudar para um estado na região oeste. Qual é o principal problema com esta interpretação? para. As categorias são organizadas em ordem alfabética. b. O gráfico não mostra erros padrão. c. O gráfico não mostra todos os dados. Não vemos variabilidade dentro de uma região e os estados mais seguros podem não estar no oeste. d. O Nordeste tem a menor média. 7. Faça um gráfico de caixa das taxas de homicídio definidas como: data(&quot;murders&quot;) murders %&gt;% mutate(rate = total/population*100000) por região, mostrando todos os pontos e ordenando as regiões pela taxa média. 8. Os gráficos abaixo mostram três variáveis contínuas. A linha \\(x=2\\) parece separar os pontos. Mas, na realidade, não é o caso, como vemos quando representamos graficamente os dados em um par de pontos bidimensionais. Por que acontece isso? para. Os seres humanos não são bons em ler gráficos pseudo-3D. b. Deve haver um erro no código. c. Cores nos confundem. d. Diagramas de dispersão não devem ser usados para comparar duas variáveis quando tivermos acesso a três variáveis. 9. Reproduza o gráfico de imagem que criamos anteriormente, mas para varíola. Para este gráfico, não inclua anos em que nenhum caso foi relatado em 10 ou mais semanas. 10. Agora, repita o gráfico de séries temporais que criamos anteriormente, mas desta vez seguindo as instruções da pergunta anterior. Onze. Para o estado da Califórnia, faça gráficos de séries temporais que mostrem as taxas de todas as doenças. Inclua apenas anos nos quais os dados são fornecidos em 10 ou mais semanas. Use uma cor diferente para cada doença. 12. Agora faça o mesmo para as taxas dos EUA. Dica: calcule a taxa dos EUA. EUA usando summarize, o número total de casos dividido pela população total. 10.14 Estudo de caso: vacinas e doenças infecciosas As vacinas ajudaram a salvar milhões de vidas. No século 19, antes da imunização em grupo por meio de programas de vacinação, as mortes por doenças infecciosas, como varíola e poliomielite, eram comuns. No entanto, hoje os programas de vacinação se tornaram um tanto controversos, apesar de todas as evidências científicas de sua importância. A controvérsia começou com um artigo39 publicado em 1988 e liderado por Andrew Wakefield, que alegava a existência de uma ligação entre a administração da vacina contra sarampo, caxumba e rubéola e autismo e doenças intestinais. Apesar do grande conjunto de evidências científicas que contradizem esse achado, os relatos dos tablóides e o alarmante daqueles que acreditam em teorias da conspiração levaram partes do público a acreditar que as vacinas eram prejudiciais. Como resultado, muitos pais deixaram de vacinar seus filhos. Essa prática perigosa pode ser potencialmente desastrosa, uma vez que os Centros de Controle de Doenças dos EUA, ou CDC, estimam que as vacinas impedirão mais de 21 milhões de hospitalizações e 732.000 mortes entre crianças nascidas nos Estados Unidos. últimos 20 anos (consulte “Benefícios da imunização durante a era do programa Vacinas para crianças - Estados Unidos, 1994-2013, MMWR”40). Desde então, “The Lancet” retirou o artigo e Andrew Wakefield foi finalmente “removido do prontuário médico em maio de 2010 com uma observação indicando a falsificação fraudulenta que ele incorreu e sua licença para praticar medicina no Reino Unido revogada.” (Fonte: Wikipedia41). No entanto, os equívocos permanecem, em parte por causa de ativistas autoproclamados que continuam divulgando informações erradas sobre vacinas. A comunicação eficaz de dados é um forte antídoto para desinformação e medo. Anteriormente, mostramos um exemplo de um artigo do Wall Street Journal42 que mostra dados relacionados ao impacto das vacinas na luta contra doenças infecciosas. Vamos reconstruir esse exemplo abaixo. Os dados usados para esses gráficos foram coletados, organizados e distribuídos pelo Tycho^Project [http://www.tycho.pitt.edu/] e incluem contagens semanais relatadas para sete doenças de 1928 a 2011 nos cinquenta estados dos EUA. Incluímos os totais anuais no pacote dslabs: library(tidyverse) library(RColorBrewer) library(dslabs) data(us_contagious_diseases) names(us_contagious_diseases) #&gt; [1] &quot;disease&quot; &quot;state&quot; &quot;year&quot; #&gt; [4] &quot;weeks_reporting&quot; &quot;count&quot; &quot;population&quot; Criamos um objeto temporário dat ele armazena apenas os dados do sarampo, inclui a taxa por 100.000, classifica os estados de acordo com o valor médio da doença e elimina o Alasca e o Havaí desde que esses dois se tornaram estados no final da década de 1950. Observe que existem uma coluna weeks_reporting que nos diz quantas semanas do ano foram relatados dados. Temos que ajustar esse valor ao calcular a taxa: the_disease &lt;- &quot;Measles&quot; dat &lt;- us_contagious_diseases %&gt;% filter(!state%in%c(&quot;Hawaii&quot;,&quot;Alaska&quot;) &amp; disease == the_disease) %&gt;% mutate(rate = count/ population * 10000 * 52/ weeks_reporting) %&gt;% mutate(state = reorder(state, rate)) Agora podemos mapear facilmente as taxas de doenças por ano. Aqui estão os fatos sobre o sarampo na Califórnia: dat %&gt;% filter(state == &quot;California&quot; &amp; !is.na(rate)) %&gt;% ggplot(aes(year, rate)) + geom_line() + ylab(&quot;Cases per 10,000&quot;) + geom_vline(xintercept=1963, col = &quot;blue&quot;) Adicionamos uma linha vertical em 1963, já que foi quando a vacina foi introduzida43. Agora podemos exibir dados para todos os estados em um gráfico? Temos três variáveis para incluir: ano, status e taxa. Na figura do WSJ, eles usam o eixo x para o ano, o eixo y para o estado e o tom da cor para representar as taxas. No entanto, a escala de cores que eles usam, de amarelo a azul, de verde a laranja e vermelho, pode ser melhorada. Em nosso exemplo, queremos usar uma paleta seqüencial, pois não há um centro significativo, apenas taxas baixas e altas. Usamos geometria geom_tile tecer a região com cores que representam as taxas de doenças. Usamos uma transformação de raiz quadrada para impedir que contagens particularmente altas dominem o gráfico. Observe que os valores ausentes são mostrados em cinza. Além disso, observe que, assim que uma doença foi praticamente erradicada, alguns estados deixaram de relatar casos completamente. É por essa razão que vemos tanto cinza depois de 1980. dat %&gt;% ggplot(aes(year, state, fill = rate)) + geom_tile(color = &quot;grey50&quot;) + scale_x_continuous(expand=c(0,0)) + scale_fill_gradientn(colors = brewer.pal(9, &quot;Reds&quot;), trans = &quot;sqrt&quot;) + geom_vline(xintercept=1963, col = &quot;blue&quot;) + theme_minimal() + theme(panel.grid = element_blank(), legend.position=&quot;bottom&quot;, text = element_text(size = 8)) + ggtitle(the_disease) + ylab(&quot;&quot;) + xlab(&quot;&quot;) Este gráfico fornece evidências esmagadoras em favor da contribuição das vacinas. No entanto, uma limitação é que ela usa cores para representar quantidade, o que, como explicado acima, dificulta o conhecimento exato de quão altos os valores são. Posição e comprimento são melhores sinais. Se estamos dispostos a perder informações de status, podemos fazer uma versão do gráfico que mostra os valores com a posição. Também podemos mostrar a média dos EUA. EUA, que calculamos assim: avg &lt;- us_contagious_diseases %&gt;% filter(disease==the_disease) %&gt;% group_by(year) %&gt;% summarize(us_rate = sum(count, na.rm = TRUE)/ sum(population, na.rm = TRUE) * 10000) Agora, para fazer o gráfico, simplesmente usamos a geometria geom_line: dat %&gt;% filter(!is.na(rate)) %&gt;% ggplot() + geom_line(aes(year, rate, group = state), color = &quot;grey50&quot;, show.legend = FALSE, alpha = 0.2, size = 1) + geom_line(mapping = aes(year, us_rate), data = avg, size = 1) + scale_y_continuous(trans = &quot;sqrt&quot;, breaks = c(5, 25, 125, 300)) + ggtitle(&quot;Cases per 10,000 by state&quot;) + xlab(&quot;&quot;) + ylab(&quot;&quot;) + geom_text(data = data.frame(x = 1955, y = 50), mapping = aes(x, y, label=&quot;US average&quot;), color=&quot;black&quot;) + geom_vline(xintercept=1963, col = &quot;blue&quot;) Em teoria, poderíamos usar cores para representar estados, que são uma variável categórica, mas é difícil escolher 50 cores diferentes. 10.15 Exercícios Reproduza o mapa da matriz que fizemos anteriormente, mas para varíola. Para este gráfico, não inclua os anos em que nenhum caso foi relatado por 10 ou mais semanas. Agora reproduza o gráfico de séries temporais que criamos anteriormente, mas desta vez seguindo as instruções da pergunta anterior para a varíola. Para o estado da Califórnia, faça um gráfico de série temporal mostrando as taxas de todas as doenças. Inclua apenas anos com relatórios de 10 ou mais semanas. Use uma cor diferente para cada doença. Agora faça o mesmo para as taxas dos EUA. Dica: calcule a taxa dos EUA. EUA usando summarize: total dividido pela população total. http://kbroman.org/↩ https://www.biostat.wisc.edu/~kbroman/presentations/graphs2017.pdf↩ https://github.com/kbroman/Talk_Graphs↩ http://paldhous.github.io/ucb/2016/dataviz/index.html↩ http://mediamatters.org/blog/2013/04/05/fox-news-newest-dishonest-chart-immigration-enf/193507↩ http://flowingdata.com/2012/08/06/fox-news-continues-charting-excellence/↩ https://www.pakistantoday.com.pk/2018/05/18/whats-at-stake-in-venezuelan-presidential-vote↩ https://www.youtube.com/watch?v=kl2g40GoRxg↩ https://projecteuclid.org/download/pdf_1/euclid.ss/1177010488↩ http://www.thelancet.com/journals/lancet/article/PIIS0140-6736 (97) 11096-0/ abstract↩ https://www.cdc.gov/mmwr/preview/mmwrhtml/mm6316a4.htm↩ https://es.wikipedia.org/wiki/Andrew_Wakefield↩ http://graphics.wsj.com/infectious-diseases-and-vaccines/↩ Control, Centers for Disease; Prevenção (2014). Informações de saúde do CDC para viagens internacionais em 2014 (o livro amarelo). p. 250. ISBN 9780199948505↩ "],
["robust-summaries.html", "Capítulo 11 Resumos robustos 11.1 Valores atípicos 11.2 Mediana 11.3 O intervalo interquartil (IQR) 11.4 Definição de Tukey de um outlier 11.5 Desvio absoluto mediano 11.6 Exercícios 11.7 Estudo de caso: altura do aluno autorreferida", " Capítulo 11 Resumos robustos 11.1 Valores atípicos Anteriormente, descrevemos como os boxplots mostram outliers, mas não oferecemos uma definição precisa. Aqui discutimos discrepâncias, abordagens que podem ajudar a detectá-las e resumos que levam sua presença em consideração. Os valores discrepantes são muito comuns na ciência de dados. A coleta de dados pode ser complexa e é comum observar pontos de dados gerados com erro. Por exemplo, um dispositivo de monitoramento antigo pode ler medições sem sentido antes de falhar completamente. O erro humano também é uma fonte de outliers, principalmente quando a entrada de dados é feita manualmente. Um indivíduo, por exemplo, pode digitar erroneamente sua altura em centímetros em vez de polegadas ou colocar o decimal no lugar errado. Como distinguimos um outlier de medições que são muito grandes ou muito pequenas simplesmente por causa da variabilidade esperada? Nem sempre é uma pergunta fácil de responder, mas tentaremos oferecer algumas orientações. Vamos começar com um caso simples. Suponha que um colega seja responsável por coletar dados demográficos para um grupo de homens. Os dados indicam a altura em pés e são armazenados no objeto: library(tidyverse) library(dslabs) data(outlier_example) str(outlier_example) #&gt; num [1:500] 5.59 5.8 5.54 6.15 5.83 5.54 5.87 5.93 5.89 5.67 ... Nosso colega usa o fato de que as alturas geralmente são bem aproximadas por uma distribuição normal e resume os dados com a média e o desvio padrão: mean(outlier_example) #&gt; [1] 6.1 sd(outlier_example) #&gt; [1] 7.8 e escreva um relatório sobre o fato interessante de que esse grupo de meninos é muito mais alto que o normal. A altura média é superior a seis pés! No entanto, usando seus conhecimentos de ciência de dados, eles percebem algo inesperado: o desvio padrão é superior a 7 pés. Ao somar e subtrair dois desvios padrão, eles observam que 95% dessa população parece ter alturas entre -9.489, 21.697 pés, isso não faz sentido. Um gráfico rápido mostra o problema: boxplot(outlier_example) Parece haver pelo menos um valor que não faz sentido, pois sabemos que uma altura de 180 pés é impossível. O gráfico da caixa detecta esse ponto como um erro externo. 11.2 Mediana Quando temos um erro externo como esse, a média pode ser muito grande. Matematicamente, podemos tornar a média tão grande quanto queremos simplesmente mudando um número: com 500 pontos de dados, podemos aumentar a média em qualquer quantidade \\(\\Delta\\) adicionando \\(\\Delta \\times\\) NA para um único número. A mediana, definida como o valor pelo qual metade dos valores é menor e a outra metade é maior, é robusta para esses valores extremos. Por maior que seja o ponto maior, a mediana permanece a mesma. Com esses dados, a mediana é: median(outlier_example) #&gt; [1] 5.74 que é sobre 5 pés e 9 polegadas. A mediana é o que os gráficos da caixa mostram como uma linha horizontal. 11.3 O intervalo interquartil (IQR) A caixa em um gráfico de caixa é definida pelo primeiro e terceiro quartis. Estes têm como objetivo fornecer uma idéia da variabilidade nos dados: 50% dos dados estão dentro desse intervalo. A diferença entre o terceiro e o primeiro quartil (ou os percentis 75 e 25) é conhecida como intervalo interquartil, ou IQR. Assim como a mediana, essa quantidade será robusta para valores discrepantes, uma vez que valores grandes não a afetam. Podemos fazer alguns cálculos e observar que, para os dados que seguem a distribuição normal, o IQR/ 1.349 se aproxima do desvio padrão dos dados se um erro externo não estiver presente. Podemos ver que isso funciona bem em nosso exemplo, pois obtemos uma estimativa do desvio padrão de: IQR(outlier_example)/ 1.349 #&gt; [1] 0.245 que fica perto de 3 polegadas. 11.4 Definição de Tukey de um outlier Em R, os pontos que ficam fora dos bigodes do box plot são chamados de outliers, uma definição que Tukey introduziu. O bigode superior termina no 75º percentil mais 1,5 \\(\\times\\) IQR, enquanto o bigode inferior termina no 25º percentil menos 1,5 \\(\\times\\) IQR. Se definirmos o primeiro e o terceiro quartis como \\(Q_1\\) e \\(Q_3\\), respectivamente, um valor externo é qualquer valor fora do intervalo: \\[[Q_1 - 1.5 \\times (Q_3 - Q1), Q_3 + 1.5 \\times (Q_3 - Q1)].\\] Quando os dados são normalmente distribuídos, as unidades padrão desses valores são: q3 &lt;- qnorm(0.75) q1 &lt;- qnorm(0.25) iqr &lt;- q3 - q1 r &lt;- c(q1 - 1.5*iqr, q3 + 1.5*iqr) r #&gt; [1] -2.7 2.7 Usando a função pnorm, nós vemos que 99.3% de dados cai nesse intervalo. Observe que este não é um evento tão extremo: se tivermos 1000 pontos de dados normalmente distribuídos, esperamos ver cerca de 7 fora desse intervalo. Mas estes não seriam discrepantes, como esperamos vê-los sob variação típica. Se queremos que um outlier seja mais estranho, podemos mudar 1,5 para um número maior. Tukey também usou 3 e os chamou de outliers extremos_ ou outliers extremos. Com uma distribuição normal, 100% dos dados cai nesse intervalo. Isso se traduz em cerca de 2 em um milhão de chances de estar fora de alcance. Na função geom_boxplot, isso pode ser controlado usando o argumento outlier.size, que por padrão é 1,5. A medição de 180 polegadas está além da faixa dos dados de altura: max_height &lt;- quantile(outlier_example, 0.75) + 3*IQR(outlier_example) max_height #&gt; 75% #&gt; 6.91 Se removermos esse valor, podemos ver que os dados são normalmente distribuídos conforme o esperado: x &lt;- outlier_example[outlier_example &lt; max_height] qqnorm(x) qqline(x) 11.5 Desvio absoluto mediano Outra opção para estimar o desvio padrão de maneira robusta na presença de outliers é usar o desvio absoluto médio, ou MAD. Para calcular o MAD, primeiro calculamos a mediana e, em seguida, para cada valor, calculamos a distância entre esse valor e a mediana. MAD é definido como a mediana dessas distâncias. Por razões técnicas não discutidas aqui, esse valor deve ser multiplicado por 1,4826 para garantir que ele se aproxime do desvio padrão real. A função mad já incorpora essa correção. Para os dados de altura, obtemos um MAD de: mad(outlier_example) #&gt; [1] 0.237 que fica perto de 3 polegadas. 11.6 Exercícios Nós vamos usar o pacote HistData. Se você não o instalou, pode fazê-lo assim: install.packages(&quot;HistData&quot;) Carregue o conjunto de dados de altura e crie um vetor x ele contém apenas as alturas masculinas dos dados de Galton sobre pais e filhos de suas pesquisas históricas sobre herança. library(HistData) data(Galton) x &lt;- Galton$child 1. Calcule a média e a mediana desses dados. 2. Calcule a mediana e o MAD desses dados. 3. Agora, suponha que Galton tenha cometido um erro ao inserir o primeiro valor e esqueceu de usar o ponto decimal. Você pode imitar esse erro digitando: x_with_error &lt;- x x_with_error[1] &lt;- x_with_error[1]*10 Quantas polegadas a média cresce como resultado desse erro? 4. Quantas polegadas o SD cresce como resultado desse erro? 5. Quantas polegadas a mediana cresce como resultado desse erro? 6. Quantas polegadas o MAD cresce como resultado desse erro? 7. Como poderíamos usar a análise exploratória de dados para detectar que um erro foi cometido? para. Como é apenas um valor entre muitos, isso não pode ser detectado. b. Veríamos uma mudança óbvia na distribuição. c. Um gráfico de caixa, histograma ou QQ revelaria um erro óbvio. d. Um diagrama de dispersão mostraria altos níveis de erro de medição. 8. Quanto a média pode crescer acidentalmente com erros como esse? Escreva uma função chamada error_avg que leva um valor k e retorna a média do vetor x após a primeira entrada mudar para k. Mostrar resultados para k=10000 e k=-10000. 11.7 Estudo de caso: altura do aluno autorreferida As alturas que estudamos não são as alturas originais relatadas pelos alunos. As alturas originais também estão incluídas no pacote dslabs e podem ser carregadas assim: library(dslabs) data(&quot;reported_heights&quot;) Height é um vetor de caracteres, por isso criamos uma nova coluna com a versão numérica: reported_heights &lt;- reported_heights %&gt;% mutate(original_heights = height, height = as.numeric(height)) #&gt; Warning: NAs introduced by coercion Observe que recebemos um aviso sobre NAs. Isso ocorre porque algumas das alturas autorreferidas não eram números. Podemos ver por que temos essas NAs: reported_heights %&gt;% filter(is.na(height)) %&gt;% head() #&gt; time_stamp sex height original_heights #&gt; 1 2014-09-02 15:16:28 Male NA 5&#39; 4&quot; #&gt; 2 2014-09-02 15:16:37 Female NA 165cm #&gt; 3 2014-09-02 15:16:52 Male NA 5&#39;7 #&gt; 4 2014-09-02 15:16:56 Male NA &gt;9000 #&gt; 5 2014-09-02 15:16:56 Male NA 5&#39;7&quot; #&gt; 6 2014-09-02 15:17:09 Female NA 5&#39;3&quot; Alguns alunos relataram suas alturas usando pés e polegadas em vez de apenas polegadas. Outros usavam centímetros e outros estavam apenas trollando. Por enquanto, removeremos essas entradas: reported_heights &lt;- filter(reported_heights, !is.na(height)) Se calcularmos a média e o desvio padrão, observamos que obtemos resultados estranhos. A média e o desvio padrão são diferentes da mediana e do MAD: reported_heights %&gt;% group_by(sex) %&gt;% summarize(average = mean(height), sd = sd(height), median = median(height), MAD = mad(height)) #&gt; # A tibble: 2 x 5 #&gt; sex average sd median MAD #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Female 63.4 27.9 64.2 4.05 #&gt; 2 Male 103. 530. 70 4.45 Isso sugere que temos discrepâncias, o que é confirmado pela criação de um gráfico de caixa: Vemos alguns valores extremos. Para ver quais são esses valores, podemos analisar rapidamente os valores maiores usando a função arrange: reported_heights %&gt;% arrange(desc(height)) %&gt;% top_n(10, height) #&gt; time_stamp sex height original_heights #&gt; 1 2014-09-03 23:55:37 Male 11111 11111 #&gt; 2 2016-04-10 22:45:49 Male 10000 10000 #&gt; 3 2015-08-10 03:10:01 Male 684 684 #&gt; 4 2015-02-27 18:05:06 Male 612 612 #&gt; 5 2014-09-02 15:16:41 Male 511 511 #&gt; 6 2014-09-07 20:53:43 Male 300 300 #&gt; 7 2014-11-28 12:18:40 Male 214 214 #&gt; 8 2017-04-03 16:16:57 Male 210 210 #&gt; 9 2015-11-24 10:39:45 Male 192 192 #&gt; 10 2014-12-26 10:00:12 Male 190 190 #&gt; 11 2016-11-06 10:21:02 Female 190 190 As primeiras sete entradas parecem erros estranhos. No entanto, as seguintes entradas parecem ter sido inseridas em centímetros em vez de polegadas. Como 184 cm equivale a seis pés de altura, suspeitamos que 184 signifique 72 polegadas. Podemos revisar todas as respostas sem sentido examinando os dados que Tukey considera far out_ou_extremos: whisker &lt;- 3*IQR(reported_heights$height) max_height &lt;- quantile(reported_heights$height, .75) + whisker min_height &lt;- quantile(reported_heights$height, .25) - whisker reported_heights %&gt;% filter(!between(height, min_height, max_height)) %&gt;% select(original_heights) %&gt;% head(n=10) %&gt;% pull(original_heights) #&gt; [1] &quot;6&quot; &quot;5.3&quot; &quot;511&quot; &quot;6&quot; &quot;2&quot; &quot;5.25&quot; &quot;5.5&quot; &quot;11111&quot; #&gt; [9] &quot;6&quot; &quot;6.5&quot; Verificando cuidadosamente essas alturas, vemos dois erros comuns: entradas em centímetros, que se tornam muito grandes, e entradas do tipo x.y com x e y representando pés e polegadas, respectivamente, que acabam sendo muito pequenos. Alguns dos valores ainda menores, como 1.6, podem ser entradas em metros. Na parte data wrangling deste livro, aprenderemos técnicas para corrigir esses valores e convertê-los em polegadas. Aqui, conseguimos detectar esse problema explorando cuidadosamente os dados para descobrir problemas com eles - o primeiro passo na grande maioria dos projetos de ciência de dados. "],
["introdução-às-estatísticas-com-r.html", "Capítulo 12 Introdução às estatísticas com R", " Capítulo 12 Introdução às estatísticas com R A análise de dados é um dos principais focos deste livro. Embora as ferramentas de computação que introduzimos sejam desenvolvimentos relativamente recentes, a análise de dados existe há mais de um século. Ao longo dos anos, os analistas de dados que trabalham em projetos específicos tiveram idéias e conceitos gerais para muitas outras aplicações. Eles também identificaram maneiras comuns pelas quais padrões aparentes nos dados e realidades matemáticas importantes que não são imediatamente óbvias podem nos enganar. O acúmulo dessas idéias e perspectivas deu origem à disciplina estatística, que oferece uma estrutura matemática para facilitar a descrição formal e a avaliação dessas idéias. Para evitar repetir erros comuns e perder tempo reinventando a roda, é importante que os analistas de dados tenham um profundo entendimento das estatísticas. Devido à maturidade da disciplina, existem dezenas de excelentes livros já publicados sobre esse assunto e, portanto, não nos concentramos em descrever a estrutura matemática aqui. Em vez disso, apresentamos brevemente conceitos e, em seguida, oferecemos estudos de caso que demonstram como as estatísticas são usadas na análise de dados, juntamente com o código R que implementa essas idéias. Também usamos o código R para ajudar a esclarecer alguns dos principais conceitos estatísticos geralmente descritos usando a matemática. Recomendamos complementar este capítulo com um livro de estatística. Dois exemplos são Estatísticas de Freedman, Pisani e Purves e Statistical Inference de Casella e Berger. Os conceitos específicos que discutimos nesta parte do livro são Probabilidade, Inferência Estatística, Modelos Estatísticos, Regressão e Modelos Lineares, que são os principais tópicos abordados em um curso de estatística. Os estudos de caso que apresentamos referem-se à crise financeira, previsão de resultados das eleições, como entender a herança e como formar um time de beisebol. "],
["probabilidade.html", "Capítulo 13 Probabilidade 13.1 Probabilidade discreta 13.2 Independência 13.3 Probabilidades condicionais 13.4 Regras de adição e multiplicação 13.5 Combinações e permutações 13.6 Exemplos 13.7 Exercícios 13.8 Probabilidade contínua 13.9 Distribuições teóricas contínuas 13.10 Distribuições contínuas 13.11 Exercícios", " Capítulo 13 Probabilidade Nos jogos de azar, a probabilidade tem uma definição muito intuitiva. Por exemplo, sabemos o que significa quando dizemos que a probabilidade de um par de dados rolar sete é 1 em 6. No entanto, esse não é o caso em outros contextos. Hoje, a teoria das probabilidades é usada de maneira muito mais ampla com a palavra “probabilidade” e parte da linguagem cotidiana. Se digitarmos “Quais são as probabilidades de” no Google, a função de preenchimento automático nos fornecerá: “ter gêmeos”, “ter gêmeos” e “ganhar na loteria”. Um dos objetivos desta parte do livro é ajudá-lo a entender como a probabilidade é útil para entender e descrever eventos do mundo real quando realizamos análises de dados. Desde que saber calcular as probabilidades oferece uma vantagem no jogo, muitas pessoas inteligentes ao longo da história, incluindo matemáticos famosos como Cardano, Fermat e Pascal, gastaram tempo e energia pensando na matemática desses jogos. . Como resultado, nasceu a teoria das probabilidades. A probabilidade ainda é muito útil nos jogos de azar modernos. Por exemplo, no pôquer, podemos calcular a probabilidade de ganhar uma mão com base nas cartas da mesa. Além disso, os cassinos confiam na teoria das probabilidades para desenvolver jogos que quase sempre garantem ganhos. A teoria da probabilidade é útil em muitos outros contextos e, em particular, em áreas que de alguma forma dependem dos dados afetados pelo acaso. Todos os outros capítulos desta parte são baseados na teoria das probabilidades. Portanto, o conhecimento da probabilidade é indispensável para a ciência de dados. 13.1 Probabilidade discreta Começamos explorando alguns princípios básicos relacionados aos dados categóricos. Essa parte da probabilidade é conhecida como a probabilidade discreta. Isso nos ajudará a entender a teoria das probabilidades que apresentaremos posteriormente para dados numéricos e contínuos, muito mais comuns em aplicativos de ciência de dados. Probabilidade discreta é mais útil em jogos de cartas e, portanto, nós as usamos como exemplos. 13.1.1 Frequência relativa Embora a palavra probabilidade seja usada na linguagem cotidiana, é difícil responder a perguntas sobre probabilidade, se não impossível, porque o conceito de “probabilidade” não está bem definido. Aqui discutimos uma definição matemática de probability que nos permite dar respostas precisas a certas perguntas. Por exemplo, se eu tiver 2 bolinhas vermelhas e 3 bolinhas azuis dentro de uma urna44 (muitos livros de probabilidade usam esse termo arcaico, também o fazemos) e escolha um aleatoriamente, qual é a probabilidade de escolher um vermelho? Nossa intuição nos diz que a resposta é 2/5 ou 40%. Uma definição precisa pode ser dada observando que existem cinco resultados possíveis, dos quais dois atendem à condição necessária para o evento “escolha um mármore vermelho”. Dado que cada um dos cinco resultados tem a mesma probabilidade de ocorrência, concluímos que a probabilidade é de 0,4 para vermelho e 0,6 para azul. Uma maneira mais tangível de pensar sobre a probabilidade de um evento é a proporção de vezes que o evento ocorre quando repetimos o experimento um número infinito de vezes, independentemente e sob as mesmas condições. 13.1.2 Notação Usamos a notação \\(\\mbox{Pr}(A)\\) para denotar a probabilidade de evento acontecer \\(A\\). Usamos o termo geral event para nos referirmos a coisas que podem acontecer quando algo acontece por acaso. No exemplo anterior, o evento foi “escolha um mármore vermelho”. Em uma pesquisa política na qual chamamos aleatoriamente 100 prováveis eleitores americanos, um exemplo de evento é “ligar para 48 democratas e 52 republicanos”. Em aplicativos de ciência de dados, frequentemente trabalharemos com variáveis contínuas. Esses eventos costumam ser coisas como “essa pessoa tem mais de um metro e oitenta”? Nesse caso, escrevemos eventos de uma forma mais matemática: \\(X \\geq 6\\). Veremos mais desses exemplos abaixo. Aqui nos concentramos em dados categóricos. 13.1.3 Distribuições de probabilidade Se conhecermos a frequência relativa das diferentes categorias, definir uma distribuição para resultados categóricos é relativamente simples. Simplesmente atribuímos uma probabilidade a cada categoria. Nos casos que podem ser considerados bolinhas de gude em uma urna, para cada tipo de mármore, sua proporção define a distribuição. Se estivermos chamando aleatoriamente prováveis eleitores de uma população que é 44% democrata, 44% republicana, 10% indecisa e 2% Partido Verde, essas proporções definem a probabilidade de cada grupo. A distribuição de probabilidade é: Pr (escolha um republicano) = 0,44 Pr (escolha um democrata) = 0,44 Pr (escolha indecisa) = 0,10 Pr (escolha um verde) = 0,02 Simulações de Monte Carlo para dados categóricos Os computadores oferecem uma maneira de realizar o experimento aleatório simples descrito acima: escolhendo um mármore aleatoriamente a partir de uma urna que contém três bolinhas azuis e duas vermelhas. Geradores de números aleatórios nos permitem imitar o processo de escolha aleatória. Um exemplo é a função sample em R. Demonstramos seu uso no código abaixo. Primeiro, usamos a função rep para gerar a urna: beads &lt;- rep(c(&quot;red&quot;, &quot;blue&quot;), times = c(2,3)) beads #&gt; [1] &quot;red&quot; &quot;red&quot; &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; e depois usamos sample escolher um mármore aleatoriamente: sample(beads, 1) #&gt; [1] &quot;blue&quot; Essa linha de código produz um resultado aleatório. Queremos repetir esse experimento um número infinito de vezes, mas é impossível repeti-lo para sempre. No entanto, podemos repetir o experimento um número suficientemente grande de vezes para que os resultados sejam praticamente equivalentes a repeti-lo para sempre. Este é um exemplo de simulação de Monte Carlo. Muito do que estudam estatísticos matemáticos e teóricos, que não discutimos neste livro, refere-se a fornecer definições rigorosas de “virtualmente equivalente”, além de estudar o quão perto um grande número de experimentos nos leva ao que acontece no limite. . Mais adiante nesta seção, oferecemos uma abordagem prática para determinar o que é “grande o suficiente”. Para realizar nossa primeira simulação de Monte Carlo, usamos a função replicate, o que nos permite repetir a mesma tarefa várias vezes. Aqui, repetimos o evento aleatório \\(B =\\) 10.000 vezes: B &lt;- 10000 events &lt;- replicate(B, sample(beads, 1)) Agora podemos ver se nossa definição realmente concorda com essa abordagem de simulação de Monte Carlo. Podemos usar table para ver a distribuição: tab &lt;- table(events) tab #&gt; events #&gt; blue red #&gt; 6009 3991 e prop.table nos dá as proporções: prop.table(tab) #&gt; events #&gt; blue red #&gt; 0.601 0.399 Os números acima são probabilidades estimadas fornecidas por uma simulação de Monte Carlo. A teoria estatística, que não discutimos aqui, nos diz que onde \\(B\\) à medida que aumenta, as estimativas se aproximam de 3/5 = 0,6 e 2/5 = 0,4. Embora este seja um exemplo simples e pouco útil, usaremos simulações de Monte Carlo para estimar probabilidades nos casos em que é difícil calcular quantidades exatas. Antes de nos aprofundarmos em exemplos mais complexos, usaremos alguns simples para demonstrar as ferramentas de computação disponíveis em R. 13.1.4 Defina a semente aleatória Antes de continuar, explicaremos brevemente a seguinte linha de código importante: set.seed(1986) Ao longo deste livro, usamos geradores de números aleatórios. Isso implica que muitos dos resultados que apresentamos podem mudar por acaso e uma versão congelada do livro pode mostrar um resultado diferente do que eles obtêm quando tentam codificar como observam no livro. Isso não é um problema, pois os resultados são aleatórios e podem mudar. No entanto, se você quiser garantir que os resultados sejam exatamente os mesmos sempre que executá-los, poderá definir a semente de geração de número aleatório R (seed) para um número específico. Definimos isso em 1986. Queremos evitar o uso da mesma semente todas as vezes. Uma maneira popular de escolher a semente é subtraindo o mês e o dia do ano. Por exemplo, para 20 de dezembro de 2018, lançamos a semente em 1986: \\(2018 - 12 - 20 = 1986\\). Você pode obter mais informações sobre como corrigir a semente consultando a documentação: ?set.seed Nos exercícios, podemos pedir que você conserte a semente para garantir que seus resultados sejam exatamente o que esperamos. 13.1.5 Com e sem substituição A função sample ele tem um argumento que nos permite escolher mais de um item da urna. No entanto, por padrão, essa seleção ocorre sem substituição: após a seleção de uma bola de gude, ela não é colocada de volta na urna. Observe o que acontece quando pedimos para selecionar cinco bolas de gude aleatoriamente: sample(beads, 5) #&gt; [1] &quot;red&quot; &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; &quot;red&quot; sample(beads, 5) #&gt; [1] &quot;red&quot; &quot;red&quot; &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; sample(beads, 5) #&gt; [1] &quot;blue&quot; &quot;red&quot; &quot;blue&quot; &quot;red&quot; &quot;blue&quot; Isso resulta em rearranjos que sempre têm três bolinhas azuis e duas vermelhas. Se pedirmos seis bolas de gude para serem selecionadas, obteremos um erro: sample(beads, 6) Error in sample.int(length(x), size, replace, prob) : cannot take a sample larger than the population when 'replace = FALSE' No entanto, a função sample pode ser usado diretamente, sem o uso de replicate, para repetir o mesmo experimento, escolhendo 1 dos 5 mármores, continuamente, nas mesmas condições. Para fazer isso, amostramos com substituição: o mármore é devolvido à urna após a seleção. Nós podemos dizer sample eu fazer isso mudando o argumento replace que por padrão é FALSE, para replace = TRUE: events &lt;- sample(beads, B, replace = TRUE) prop.table(table(events)) #&gt; events #&gt; blue red #&gt; 0.602 0.398 Não surpreende que obtenham resultados muito semelhantes aos obtidos anteriormente com replicate. 13.2 Independência Dizemos que dois eventos são independentes se o resultado de um não afeta o outro. O exemplo clássico é o lançamento de moedas. Toda vez que jogamos uma moeda, a probabilidade de ver caras é 1/2, independentemente dos resultados dos lançamentos anteriores. O mesmo acontece quando coletamos bolas de gude de uma urna de substituição. No exemplo acima, a probabilidade de vermelho é 0,40, independentemente das seleções anteriores. Muitos exemplos de eventos não independentes vêm de jogos de cartas. Quando negociamos a primeira carta, a probabilidade de obter um K é 1/13, pois há treze possibilidades: Duas, Três, \\(\\dots\\), Ten, J, Q, K e As. Mas se dermos um K como primeira carta e não a substituirmos no baralho, a probabilidade de uma segunda carta ser K é menor, porque só restam três Ks: a probabilidade é 3 de 51. Portanto, esses eventos não são independentes: o primeiro resultado afeta o seguinte. Para ver um caso extremo de eventos não independentes, considere nosso exemplo de escolha de cinco bolinhas aleatoriamente sem substituição: x &lt;- sample(beads, 5) Se eles tiverem que adivinhar a cor do primeiro mármore, irão prever o azul, pois o azul tem 60% de chance. Mas se mostrarmos o resultado dos últimos quatro resultados: x[2:5] #&gt; [1] &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; &quot;red&quot; você ainda adivinharia o azul? Claro que não. Agora você sabe que a probabilidade de vermelho é 1, pois o único mármore restante é vermelho. Os eventos não são independentes, portanto as probabilidades mudam. 13.3 Probabilidades condicionais Quando os eventos não são independentes, as probabilidades condicionais são úteis. Já vimos um exemplo de probabilidade condicional: calculamos a probabilidade de que uma segunda carta seja K desde que a primeira carta fosse K. Em probabilidade, usamos a seguinte notação: \\[ \\mbox{Pr}(\\mbox{Card 2 is a king} \\mid \\mbox{Card 1 is a king}) = 3/51 \\] Nós usamos o \\(\\mid\\) como uma abreviação de “dado isso” ou “condicional”. Quando dois eventos, digamos \\(A\\) e \\(B\\), eles são independentes, temos: \\[ \\mbox{Pr}(A \\mid B) = \\mbox{Pr}(A) \\] Esta é a maneira matemática de dizer: o fato de que \\(B\\) aconteceu não afeta a probabilidade de \\(A\\) acontecer. De fato, isso pode ser considerado a definição matemática de independência. 13.4 Regras de adição e multiplicação Regra de multiplicação Se queremos saber a probabilidade de dois eventos ocorrerem, digamos \\(A\\) e \\(B\\), podemos usar a regra de multiplicação: \\[ \\mbox{Pr}(A \\mbox{ and } B) = \\mbox{Pr}(A)\\mbox{Pr}(B \\mid A) \\] Vamos usar o jogo de cartas do Blackjack como exemplo. No Blackjack, eles recebem duas cartas aleatoriamente. Depois de ver o que têm, podem pedir mais cartões. O objetivo é chegar mais perto dos 21 do que o revendedor, sem passar. Os face cards valem 10 pontos e os ases valem 11 ou 1 (um escolhe). Portanto, no Blackjack, para calcular as probabilidades de conseguir um 21 recebendo um ás e depois uma carta de face, calculamos a probabilidade de a primeira carta ser um ás e multiplicamos pela probabilidade de tirar uma carta de cara ou 10, dado que o primeiro foi um ás: \\(1/13 \\times 16/51 \\approx 0.025\\). A regra de multiplicação também se aplica a mais de dois eventos. Podemos usar a indução para incluir mais eventos: \\[ \\mbox{Pr}(A \\mbox{ and } B \\mbox{ and } C) = \\mbox{Pr}(A)\\mbox{Pr}(B \\mid A)\\mbox{Pr}(C \\mid A \\mbox{ and } B) \\] Regra de multiplicação sob independência Quando temos eventos independentes, a regra de multiplicação se torna mais simples: \\[ \\mbox{Pr}(A \\mbox{ and } B \\mbox{ and } C) = \\mbox{Pr}(A)\\mbox{Pr}(B)\\mbox{Pr}(C) \\] Mas devemos ter muito cuidado antes de usar isso, pois assumir a independência quando ela realmente não existe pode resultar em cálculos de probabilidade muito diferentes e incorretos. Como exemplo, imagine um caso em que o suspeito seja descrito como tendo bigode e barba. O réu tem bigode e barba e a acusação traz um “especialista” que testemunha que 1/10 dos homens têm barba e 1/5 têm bigode, portanto, usando a regra da multiplicação, concluímos que apenas \\(1/10 \\times 1/5\\) ou 0,02 tem ambos. Mas para se multiplicar assim, precisamos assumir a independência! Digamos que a probabilidade condicional de um homem ter um bigode condicionado no qual ele tem barba é 0,95. Portanto, o cálculo correto da probabilidade resulta em um número muito maior: \\(1/10 \\times 95/100 = 0.095\\). A regra de multiplicação também nos fornece uma fórmula geral para calcular probabilidades condicionais: \\[ \\mbox{Pr}(B \\mid A) = \\frac{\\mbox{Pr}(A \\mbox{ and } B)}{ \\mbox{Pr}(A)} \\] Para ilustrar como usamos essas fórmulas e conceitos na prática, usaremos vários exemplos relacionados a jogos de cartas. Regra de adição A regra de adição nos diz que: \\[ \\mbox{Pr}(A \\mbox{ or } B) = \\mbox{Pr}(A) + \\mbox{Pr}(B) - \\mbox{Pr}(A \\mbox{ and } B) \\] Essa regra é intuitiva: pense em um diagrama de Venn. Se simplesmente adicionarmos as probabilidades, contaremos a interseção duas vezes, portanto, devemos subtrair uma instância. 13.5 Combinações e permutações Em nosso primeiro exemplo, imaginamos uma urna com cinco bolinhas de gude. Lembre-se de que, para calcular a distribuição de probabilidade de um empate, simplesmente listamos todas as probabilidades. Havia cinco e, portanto, para cada evento, contamos quantas dessas probabilidades estavam associadas ao evento. A probabilidade de escolher um mármore azul é de 3/5, devido aos cinco resultados possíveis, três eram azuis. Para casos mais complicados, os cálculos não são tão simples. Por exemplo, qual é a probabilidade de que, se eu escolher cinco cartas sem substituição, receberei todas as cartas do mesmo naipe (suit em inglês), conhecidas como “flush” no poker? Em um curso de probabilidade discreta, você aprende a teoria de como fazer esses cálculos. Aqui, focamos em como usar o código R para calcular as respostas. Primeiro, vamos construir um baralho de cartas. Para isso, usaremos as funções expand.grid e paste. Nós usamos paste para criar cadeias juntando cadeias menores. Para fazer isso, pegamos o número e o naipe de um cartão e criamos o nome do cartão assim: number &lt;- &quot;Three&quot; suit &lt;- &quot;Hearts&quot; paste(number, suit) #&gt; [1] &quot;Three Hearts&quot; paste também funciona em pares de vetores que executam a operação elemento a elemento: paste(letters[1:5], as.character(1:5)) #&gt; [1] &quot;a 1&quot; &quot;b 2&quot; &quot;c 3&quot; &quot;d 4&quot; &quot;e 5&quot; A função expand.grid nos fornece todas as combinações de duas entradas de vetor. Por exemplo, se eles têm calças azuis e pretas e camisas brancas, cinza e xadrez (plaid), todas as suas combinações são: expand.grid(pants = c(&quot;blue&quot;, &quot;black&quot;), shirt = c(&quot;white&quot;, &quot;grey&quot;, &quot;plaid&quot;)) #&gt; pants shirt #&gt; 1 blue white #&gt; 2 black white #&gt; 3 blue grey #&gt; 4 black grey #&gt; 5 blue plaid #&gt; 6 black plaid Aqui está como geramos um baralho de cartas: suits &lt;- c(&quot;Diamonds&quot;, &quot;Clubs&quot;, &quot;Hearts&quot;, &quot;Spades&quot;) numbers &lt;- c(&quot;Ace&quot;, &quot;Deuce&quot;, &quot;Three&quot;, &quot;Four&quot;, &quot;Five&quot;, &quot;Six&quot;, &quot;Seven&quot;, &quot;Eight&quot;, &quot;Nine&quot;, &quot;Ten&quot;, &quot;Jack&quot;, &quot;Queen&quot;, &quot;King&quot;) deck &lt;- expand.grid(number=numbers, suit=suits) deck &lt;- paste(deck$number, deck$suit) Com o baralho construído, podemos verificar se a probabilidade de um K ser a primeira carta é 1/13 calculando a proporção de possíveis resultados que satisfazem nossa condição: kings &lt;- paste(&quot;King&quot;, suits) mean(deck %in% kings) #&gt; [1] 0.0769 Agora, que tal a probabilidade condicional de que a segunda carta seja um K desde que a primeira carta fosse um K? Anteriormente, deduzimos que, se um K já estiver fora do baralho e restarem 51 cartas, a probabilidade será de 3/51. Vamos confirmar listando todos os resultados possíveis. Para fazer isso, podemos usar a função permutations do pacote gtools. Para qualquer lista de tamanho n, essa função calcula todas as diferentes combinações que podemos obter quando selecionamos r artigos. Aqui estão todas as maneiras pelas quais podemos escolher dois números de uma lista que consiste em 1,2,3: library(gtools) permutations(3, 2) #&gt; [,1] [,2] #&gt; [1,] 1 2 #&gt; [2,] 1 3 #&gt; [3,] 2 1 #&gt; [4,] 2 3 #&gt; [5,] 3 1 #&gt; [6,] 3 2 Observe que a ordem é importante aqui: 3.1 é diferente de 1.3. Além disso, observe que (1,1), (2,2) e (3,3) não aparecem porque, quando escolhemos um número, ele não pode aparecer novamente. Opcionalmente, podemos adicionar um vetor. Se desejar ver cinco números de telefone aleatórios (sete dígitos) de todos os números de telefone possíveis (sem repetição), você pode escrever: all_phone_numbers &lt;- permutations(10, 7, v = 0:9) n &lt;- nrow(all_phone_numbers) index &lt;- sample(n, 5) all_phone_numbers[index,] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] #&gt; [1,] 1 3 8 0 6 7 5 #&gt; [2,] 2 9 1 6 4 8 0 #&gt; [3,] 5 1 6 0 9 8 2 #&gt; [4,] 7 4 6 0 2 8 1 #&gt; [5,] 4 6 5 9 2 8 0 Em vez de usar os números de 1 a 10, o padrão, R usa o que fornecemos através v: os dígitos de 0 a 9. Para calcular todas as maneiras possíveis, podemos escolher duas cartas quando a ordem importa, escrevemos: hands &lt;- permutations(52, 2, v = deck) Esta é uma matriz com duas colunas e 2652 linhas. Com uma matriz, podemos obter a primeira e a segunda letras assim: first_card &lt;- hands[,1] second_card &lt;- hands[,2] Agora, os casos para os quais a primeira letra é um K podem ser calculados assim: kings &lt;- paste(&quot;King&quot;, suits) sum(first_card %in% kings) #&gt; [1] 204 Para obter a probabilidade condicional, calculamos qual fração deles tem um K como a segunda letra: sum(first_card%in%kings &amp; second_card%in%kings)/ sum(first_card%in%kings) #&gt; [1] 0.0588 que é exatamente 3/51, como já deduzimos. Observe que o código acima é equivalente a: mean(first_card%in%kings &amp; second_card%in%kings)/ mean(first_card%in%kings) #&gt; [1] 0.0588 o que você usa mean ao invés de sum e é uma versão R de: \\[ \\frac{\\mbox{Pr}(A \\mbox{ and } B)}{ \\mbox{Pr}(A)} \\] E se a ordem não importar? Por exemplo, no Blackjack, se você receber um Ás e uma carta de face como primeira mão, ela será chamada Natural 21 e você ganhará automaticamente. Se quiséssemos calcular a probabilidade de isso acontecer, listaríamos as combinações, não as permutações, pois a ordem não importa. combinations(3,2) #&gt; [,1] [,2] #&gt; [1,] 1 2 #&gt; [2,] 1 3 #&gt; [3,] 2 3 Na segunda linha, o resultado não inclui (2.1) porque (1.2) já foi listado. O mesmo se aplica a (3.1) e (3.2). Portanto, para calcular a probabilidade de um Natural 21, podemos fazer o seguinte: aces &lt;- paste(&quot;Ace&quot;, suits) facecard &lt;- c(&quot;King&quot;, &quot;Queen&quot;, &quot;Jack&quot;, &quot;Ten&quot;) facecard &lt;- expand.grid(number = facecard, suit = suits) facecard &lt;- paste(facecard$number, facecard$suit) hands &lt;- combinations(52, 2, v = deck) mean(hands[,1] %in% aces &amp; hands[,2] %in% facecard) #&gt; [1] 0.0483 Na última linha, assumimos que o ás é a primeira carta que recebemos. Sabemos disso porque, sabendo como combination listar as probabilidades, entendemos que você listará esse caso primeiro. Mas, com certeza, poderíamos ter produzido a mesma resposta escrevendo o seguinte: mean((hands[,1] %in% aces &amp; hands[,2] %in% facecard) | (hands[,2] %in% aces &amp; hands[,1] %in% facecard)) #&gt; [1] 0.0483 Exemplo de Monte Carlo Ao invés de usar combinations para deduzir a probabilidade exata de um Natural 21, podemos usar uma simulação de Monte Carlo para estimar essa probabilidade. Nesse caso, escolhemos duas cartas repetidamente e observamos quantos 21s temos. Nós podemos usar a função sample escolher duas placas sem substituições: hand &lt;- sample(deck, 2) hand #&gt; [1] &quot;Queen Clubs&quot; &quot;Seven Spades&quot; E depois verifique se uma carta é um Ás e a outra uma figura ou 10. A partir de agora, incluímos 10 quando dizemos figure card_ou_figure. Agora precisamos verificar as duas probabilidades: (hands[1] %in% aces &amp; hands[2] %in% facecard) | (hands[2] %in% aces &amp; hands[1] %in% facecard) #&gt; [1] FALSE Se repetirmos isso 10.000 vezes, obteremos uma aproximação muito boa da probabilidade de um Natural 21. Vamos começar escrevendo uma função que escolhe uma mão e retorna VERDADEIRO se obtivermos 21. A função não precisa de argumentos porque usa objetos definidos no ambiente global. blackjack &lt;- function(){ hand &lt;- sample(deck, 2) (hand[1] %in% aces &amp; hand[2] %in% facecard) | (hand[2] %in% aces &amp; hand[1] %in% facecard) } Aqui temos que verificar as duas probabilidades: Ás primeiro ou Ás segundo, porque não estamos usando a função combinations. A função retorna TRUE se tivermos 21 e FALSE de outra maneira: blackjack() #&gt; [1] FALSE Agora podemos jogar este jogo, digamos 10.000 vezes: B &lt;- 10000 results &lt;- replicate(B, blackjack()) mean(results) #&gt; [1] 0.0475 13.6 Exemplos Nesta seção, descrevemos dois exemplos populares de probabilidade discreta: o problema de Monty Hall e o problema do aniversário. Usamos R para ajudar a ilustrar conceitos matemáticos. Problema de Monty Hall Nos anos 1970, nos EUA, houve um programa de perguntas e respostas chamado “Let's Make a Deal” e Monty Hall foi o apresentador. Em algum momento do jogo, o competidor foi convidado a escolher uma das três portas. Atrás de uma porta havia um prêmio, enquanto atrás das outras portas eles tinham uma cabra que indicava que o competidor havia perdido. Depois que o competidor escolheu uma porta, e antes de revelar se a porta continha um prêmio, Monty Hall abriu uma das outras duas portas e mostrou ao competidor que não havia prêmio atrás daquela porta. Então ele perguntou ao competidor: “Você quer trocar de porta?” O que você faria? Podemos usar a probabilidade para mostrar que, se eles mantiverem a opção do portão original, suas chances de ganhar um prêmio permanecerão 1 em 3. No entanto, se mudarem para o outro portão, suas chances de ganhar o dobro para 2 em 3! ! Isso parece contraditório. Muitas pessoas pensam incorretamente que ambas as probabilidades são de 1 em 2, uma vez que se escolhe entre duas opções. Você pode ver uma explicação matemática detalhada na Khan Academy45 ou leia uma na Wikipedia46. Em seguida, usamos uma simulação de Monte Carlo para ver qual é a melhor estratégia. Observe que este código é escrito com mais detalhes do que o necessário para fins pedagógicos. Vamos começar com a estratégia de não trocar de porta: B &lt;- 10000 monty_hall &lt;- function(strategy){ doors &lt;- as.character(1:3) prize &lt;- sample(c(&quot;car&quot;, &quot;goat&quot;, &quot;goat&quot;)) prize_door &lt;- doors[prize == &quot;car&quot;] my_pick &lt;- sample(doors, 1) show &lt;- sample(doors[!doors %in% c(my_pick, prize_door)],1) stick &lt;- my_pick stick == prize_door switch &lt;- doors[!doors%in%c(my_pick, show)] choice &lt;- ifelse(strategy == &quot;stick&quot;, stick, switch) choice == prize_door } stick &lt;- replicate(B, monty_hall(&quot;stick&quot;)) mean(stick) #&gt; [1] 0.342 switch &lt;- replicate(B, monty_hall(&quot;switch&quot;)) mean(switch) #&gt; [1] 0.668 Enquanto escrevemos o código, notamos que as linhas que começam com my_pick e show eles não afetam a última operação lógica quando mantemos nossa escolha original. A partir disso, devemos perceber que a probabilidade é de 1 em 3, a mesma com a qual começamos. Quando mudamos, a estimativa de Monte Carlo confirma o cálculo de 2/3. Isso nos ajuda a entender melhor o problema, mostrando que estamos removendo uma porta, show, que definitivamente não esconde um prêmio de nossas opções. Também vemos que, a menos que acertemos na primeira escolha, você ganha: 1 - 1/3 = 2/3. 13.6.1 Problema de aniversário Imagine que você está em uma sala de aula com 50 pessoas. Se assumirmos que este é um grupo de 50 pessoas selecionadas aleatoriamente, qual é a probabilidade de pelo menos duas pessoas terem o mesmo aniversário? Embora seja um pouco avançado, podemos deduzir isso matematicamente. Faremos isso mais tarde, mas aqui usamos uma simulação de Monte Carlo. Por simplicidade, assumimos que ninguém nasceu em 29 de fevereiro. Isso realmente não muda muito a resposta. Primeiro, lembre-se de que aniversários podem ser representados como números entre 1 e 365, para que você possa obter uma amostra de 50 aniversários como este: n &lt;- 50 bdays &lt;- sample(1:365, n, replace = TRUE) Para verificar se neste conjunto específico de 50 pessoas, temos pelo menos dois com o mesmo aniversário, podemos usar a função duplicated, que retorna TRUE sempre que um elemento de um vetor for duplicado. Aqui está um exemplo: duplicated(c(1,2,3,1,4,3,5)) #&gt; [1] FALSE FALSE FALSE TRUE FALSE TRUE FALSE Na segunda vez que 1 e 3 aparecem, obtemos um TRUE. Portanto, para verificar se dois aniversários são iguais, simplesmente usamos as funções any e duplicated assim: any(duplicated(bdays)) #&gt; [1] TRUE Nesse caso, vemos o que aconteceu. Pelo menos duas pessoas tiveram o mesmo aniversário. Para estimar a probabilidade de um aniversário compartilhado no grupo, repetimos esse experimento amostrando conjuntos de 50 aniversários repetidamente: B &lt;- 10000 same_birthday &lt;- function(n){ bdays &lt;- sample(1:365, n, replace=TRUE) any(duplicated(bdays)) } results &lt;- replicate(B, same_birthday(50)) mean(results) #&gt; [1] 0.969 Você esperava que a probabilidade fosse tão alta? As pessoas tendem a subestimar essas probabilidades. Para ter uma idéia de por que é tão alto, pense no que acontece quando o tamanho do grupo se aproxima dos 365. Com o 365, estamos fora de dias e a probabilidade é uma. Digamos que queremos usar esse conhecimento para apostar com os amigos se duas pessoas em um grupo têm o mesmo aniversário. Com um grupo de qual tamanho as probabilidades estão acima de 50%? Mais de 75%? Vamos começar criando uma tabela de pesquisa. Podemos criar rapidamente uma função para calcular isso para qualquer tamanho de grupo: compute_prob &lt;- function(n, B=10000){ results &lt;- replicate(B, same_birthday(n)) mean(results) } Usando a função sapply, podemos executar operações elemento a elemento em qualquer função: n &lt;- seq(1,60) prob &lt;- sapply(n, compute_prob) Agora podemos representar graficamente as probabilidades estimadas de duas pessoas que têm o mesmo aniversário em um grupo de tamanho \\(n\\): library(tidyverse) prob &lt;- sapply(n, compute_prob) qplot(n, prob) Agora vamos calcular as probabilidades exatas em vez de usar simulações de Monte Carlo. Não apenas obtemos a resposta exata usando a matemática, mas os cálculos são muito mais rápidos, pois não precisamos realizar experimentos. Para simplificar a matemática, em vez de calcular a probabilidade de ocorrência, calcularemos a probabilidade de não ocorrer. Para isso, usamos a regra de multiplicação. Vamos começar com a primeira pessoa. A probabilidade de que a Pessoa 1 tenha um aniversário único é 1. A probabilidade de que a Pessoa 2 tenha um aniversário único, considerando que a Pessoa 1 já foi atribuída por dia, é 364/365. Então, como as duas primeiras pessoas têm aniversários únicos, a pessoa 3 tem 363 dias para escolher. Continuamos assim e descobrimos que as chances de todas as 50 pessoas terem um aniversário único são: \\[ 1 \\times \\frac{364}{365}\\times\\frac{363}{365} \\dots \\frac{365-n + 1}{365} \\] Podemos escrever uma função que faça isso para qualquer número: exact_prob &lt;- function(n){ prob_unique &lt;- seq(365,365-n+1)/365 1 - prod( prob_unique) } eprob &lt;- sapply(n, exact_prob) qplot(n, prob) + geom_line(aes(n, eprob), col = &quot;red&quot;) Este gráfico mostra que a simulação de Monte Carlo fornece uma estimativa muito boa da probabilidade exata. Se não fosse possível calcular as probabilidades exatas, ainda poderíamos estimar com precisão as probabilidades. Infinito na prática A teoria descrita aqui requer repetir experimentos repetidamente para sempre. Na prática, não podemos fazer isso. Nos exemplos acima, usamos \\(B=10,000\\) experimentos em Monte Carlo, e isso nos deu estimativas precisas. Quanto maior esse número, mais precisa será a estimativa até que a aproximação seja tão boa que seus computadores não consigam perceber a diferença. Mas em cálculos mais complexos, 10.000 podem ser insuficientes. Além disso, para alguns cálculos, 10.000 experimentos podem não ser computacionalmente viáveis. Na prática, não saberemos qual é a resposta, portanto, não saberemos se nossa estimativa de Monte Carlo é precisa. Sabemos que quanto maior é \\(B\\), melhor será a aproximação. Mas quão grande precisamos que seja? Essa é realmente uma pergunta desafiadora e, para respondê-la com frequência, é necessário treinamento avançado em estatística teórica. Uma abordagem prática que descreveremos aqui é verificar a estabilidade da estimativa. Aqui está um exemplo do problema de aniversário para um grupo de 25 pessoas. B &lt;- 10^seq(1, 5, len = 100) compute_prob &lt;- function(B, n=25){ same_day &lt;- replicate(B, same_birthday(n)) mean(same_day) } prob &lt;- sapply(B, compute_prob) qplot(log10(B), prob, geom = &quot;line&quot;) Neste gráfico, podemos ver que os valores começam a se estabilizar (ou seja, eles variam menos de 0,01) em torno de 1000. Observe que a probabilidade exata, que neste caso sabemos, é 0.569. 13.7 Exercícios 1. Um mármore é escolhido aleatoriamente a partir de uma caixa contendo: 3 bolinhas de ciano, 5 bolinhas de magenta e 7 bolinhas de gude amarelas. Qual é a probabilidade de o mármore ser ciano? 2. Qual é a probabilidade de o mármore não ser ciano? 3. Em vez de escolher apenas uma bola de gude, escolha duas bolas de gude. Retire o primeiro mármore sem devolvê-lo à caixa. Esta é uma amostra sem substituição. Qual é a probabilidade de o primeiro mármore ser ciano e o segundo mármore não ser ciano? 4. Agora repita o experimento, mas desta vez, depois de tirar o primeiro mármore e anotar a cor, coloque-o novamente na caixa e agite a caixa. Esta é uma amostra com substituição. Qual é a probabilidade de o primeiro mármore ser ciano e o segundo mármore não ser ciano? 5. Dois eventos \\(A\\) e \\(B\\) são independentes se \\(\\mbox{Pr}(A \\mbox{ and } B) = \\mbox{Pr}(A) P(B)\\). Em que situação a seleção é independente? para. Não substitui o artigo selecionado. b. Substitua o item selecionado. c. Nenhum. d. Ambos. 6. Digamos que você removeu 5 bolinhas de gude da caixa, com reposição, e todas elas foram amarelas. Qual é a probabilidade de o próximo ser amarelo? 7. Se você rolar um dado de 6 lados seis vezes, qual é a probabilidade de não ver um 6? 8. Dois times de basquete, dizem os Celtics e os Cavs, estão jogando uma série de sete jogos. Os Cavs são um time melhor e têm 60% de chance de vencer cada jogo. Qual é a probabilidade de o Celtics vencer pelo menos um jogo? 9. Crie uma simulação de Monte Carlo para confirmar sua resposta ao problema anterior. Usar B &lt;- 10000 simulações. Dica: use o seguinte código para gerar os resultados dos quatro primeiros jogos: celtic_wins &lt;- sample(c(0,1), 4, replace = TRUE, prob = c(0.6, 0.4)) O Celtics deve vencer um desses 4 jogos. 10. Dois times de basquete, dizem os Cavs e Warriors, estão jogando uma série de sete jogos do campeonato. O primeiro a vencer quatro jogos, portanto, vence a série. As equipes são igualmente boas, então cada uma tem uma chance de 50 a 50 de vencer cada jogo. Se os Cavs perderem o primeiro jogo, qual é a probabilidade de ganharem a série? Onze. Confirme os resultados da pergunta anterior com uma simulação de Monte Carlo. 12. Duas equipes \\(A\\) e \\(B\\), eles estão jogando uma série de sete jogos. Equipamento \\(A\\) é melhor que equipe \\(B\\) e tem um \\(p&gt;0.5\\) probabilidade de ganhar cada jogo. Dado um valor \\(p\\), a probabilidade de o time não favorito \\(B\\) a série win pode ser calculada com a seguinte função com base em uma simulação de Monte Carlo: prob_win &lt;- function(p){ B &lt;- 10000 result &lt;- replicate(B, { b_win &lt;- sample(c(1,0), 7, replace = TRUE, prob = c(1-p, p)) sum(b_win)&gt;=4 }) mean(result) } Usar função sapply para calcular a probabilidade, chame-o Pr ganhar por p &lt;- seq(0.5, 0.95, 0.025). Em seguida, faça um gráfico do resultado. 13. Repita o exercício anterior, mas agora mantenha a probabilidade fixa em p &lt;- 0.75 e calcule a probabilidade de diferentes números de jogos necessários para concluir a série: ganhe 1 jogo, ganhe 2 de 3 jogos, ganhe 3 de 5 jogos, … Especificamente, N &lt;- seq(1, 25, 2). Dica: use esta função: prob_win &lt;- function(N, p=0.75){ B &lt;- 10000 result &lt;- replicate(B, { b_win &lt;- sample(c(1,0), N, replace = TRUE, prob = c(1-p, p)) sum(b_win)&gt;=(N+1)/2 }) mean(result) } 13.8 Probabilidade contínua Na seção 8.4 explicamos por que, resumindo uma lista de valores numéricos, como alturas, não é útil construir uma distribuição que defina uma proporção para cada resultado possível. Por exemplo, suponha que medimos cada pessoa em uma grande população, digamos em tamanho \\(n\\), com precisão extremamente alta. Como não há duas pessoas exatamente da mesma altura, devemos atribuir a proporção \\(1/n\\) em cada valor observado e, como conseqüência, nenhum resumo útil é obtido. Da mesma forma, ao definir distribuições de probabilidade, não é útil atribuir uma probabilidade muito pequena para cada altura. Assim como quando as distribuições são usadas para resumir dados numéricos, é muito mais prático definir uma função que opera em intervalos, em vez de valores individuais. A maneira padrão de fazer isso é usar a Cumulated Distribution Function, ou CDF. Descrevemos a função de distribuição cumulativa empírica, ou eCDF, na Seção 8.4 como um resumo básico de uma lista de valores numéricos. Como exemplo, definimos anteriormente a distribuição de altura para estudantes adultos do sexo masculino. Aqui nós definimos o vetor \\(x\\) para conter essas alturas: library(tidyverse) library(dslabs) data(heights) x &lt;- heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% pull(height) Definimos a função de distribuição cumulativa empírica como: F &lt;- function(a) mean(x&lt;=a) isso por qualquer valor a, fornece a proporção de valores na lista x que são menores ou iguais a a. Observe que ainda não discutimos a probabilidade no contexto das CDFs. Vamos fazer isso perguntando o seguinte: Se eu escolher um dos alunos do sexo masculino aleatoriamente, qual é a probabilidade de ele ser mais alto que 70,5 polegadas? Como cada aluno tem a mesma probabilidade de ser escolhido, a resposta é equivalente à proporção de alunos com mais de 70,5 polegadas. Usando o CDF, obtemos uma resposta escrevendo: 1 - F(70) #&gt; [1] 0.377 Uma vez definido um CDF, podemos usá-lo para calcular a probabilidade de qualquer subconjunto. Por exemplo, a probabilidade de um aluno estar entre a altura a e altura b é: F(b)-F(a) Como podemos calcular a probabilidade de qualquer evento possível dessa maneira, a função de probabilidade cumulativa define a distribuição de probabilidade para escolher uma altura aleatória do nosso vetor de altura. x. 13.9 Distribuições teóricas contínuas Na seção 8.8 apresentamos a distribuição normal como uma aproximação útil para muitas distribuições naturais, incluindo a altura. A distribuição cumulativa para a distribuição normal é definida por uma fórmula matemática que pode ser obtida em R com a função pnorm. Dizemos que uma quantidade aleatória é normalmente distribuída com uma média m e desvio padrão s se sua distribuição de probabilidade for definida por: F(a) = pnorm(a, m, s) Isso é útil porque, se estamos dispostos a usar a aproximação normal para, por exemplo, altura, não precisamos de todo o conjunto de dados para responder a perguntas como: qual é a probabilidade de um aluno selecionado aleatoriamente ter mais de 70 anos polegadas? Só precisamos da altura média e do desvio padrão: m &lt;- mean(x) s &lt;- sd(x) 1 - pnorm(70.5, m, s) #&gt; [1] 0.371 13.9.1 Distribuições teóricas como aproximações A distribuição normal é derivada matematicamente: não precisamos de dados para defini-la. Para os cientistas de dados, quase tudo o que fazemos na prática envolve dados. Os dados são sempre, do ponto de vista técnico, discretos. Por exemplo, podemos considerar nossos dados de altura categóricos com cada altura específica como uma única categoria. A distribuição de probabilidade é definida pela proporção de alunos que indicam cada altura. Aqui está um gráfico dessa distribuição de probabilidade: Enquanto a maioria dos estudantes arredondava suas alturas para a polegada mais próxima, outros indicavam valores com mais precisão. Um aluno indicou que sua altura era de 69,6850393700787 polegadas, o que equivale a 177 centímetros. A probabilidade atribuída a esta altura é 0.001 ou 1 em 812. A probabilidade de 70 polegadas é muito maior em 0.106 mas faz sentido pensar que a probabilidade de ter exatamente 70 polegadas é diferente de 69,6850393700787? Claramente, é muito mais útil para fins de análise de dados tratar esse resultado como uma variável numérica contínua, considerando que muito poucas pessoas, ou talvez não, tenham exatamente 70 polegadas e que a razão pela qual obtemos mais valores 70 é porque as pessoas arredondam para a polegada mais próxima. Com distribuições contínuas, a probabilidade de um valor singular não é definida. Por exemplo, não faz sentido perguntar qual é a probabilidade de um valor distribuído normalmente ser 70. Em vez disso, definimos probabilidades para intervalos. Portanto, poderíamos perguntar qual é a probabilidade de alguém medir entre 69,5 e 70,5. Em casos como altura, onde os dados são arredondados, a aproximação normal é particularmente útil se estivermos trabalhando com intervalos que incluem exatamente um número redondo. Por exemplo, a distribuição normal é útil para aproximar a proporção de alunos que relatam valores de intervalo, como os três seguintes: mean(x &lt;= 68.5) - mean(x &lt;= 67.5) #&gt; [1] 0.115 mean(x &lt;= 69.5) - mean(x &lt;= 68.5) #&gt; [1] 0.119 mean(x &lt;= 70.5) - mean(x &lt;= 69.5) #&gt; [1] 0.122 Observe como chegamos perto da abordagem normal: pnorm(68.5, m, s) - pnorm(67.5, m, s) #&gt; [1] 0.103 pnorm(69.5, m, s) - pnorm(68.5, m, s) #&gt; [1] 0.11 pnorm(70.5, m, s) - pnorm(69.5, m, s) #&gt; [1] 0.108 No entanto, a aproximação não é tão útil para outros intervalos. Por exemplo, observe como a aproximação se divide quando tentamos estimar: mean(x &lt;= 70.9) - mean(x&lt;=70.1) #&gt; [1] 0.0222 com: pnorm(70.9, m, s) - pnorm(70.1, m, s) #&gt; [1] 0.0836 Em geral, chamamos essa situação de “discriminação”. Embora a distribuição real da altura seja contínua, as alturas relatadas tendem a ser mais comuns em valores discretos, neste caso, devido ao arredondamento. Desde que saibamos lidar com essa realidade, a abordagem normal pode ser uma ferramenta muito útil. 13.9.2 A densidade de probabilidade Para distribuições categóricas, podemos definir a probabilidade de uma categoria. Por exemplo, um dado, vamos chamá-lo \\(X\\), pode ser 1,2,3,4,5 ou 6. A probabilidade de 4 é definida como: \\[ \\mbox{Pr}(X=4) = 1/6 \\] O CDF pode então ser facilmente definido: \\[ F(4) = \\mbox{Pr}(X\\leq 4) = \\mbox{Pr}(X = 4) + \\mbox{Pr}(X = 3) + \\mbox{Pr}(X = 2) + \\mbox{Pr}(X = 1) \\] Embora para distribuições contínuas a probabilidade de um único valor \\(\\mbox{Pr}(X=x)\\) não definido, existe uma definição teórica que tem uma interpretação semelhante. A densidade de probabilidade em \\(x\\) é definido como a função \\(f(a)\\) tal que: \\[ F(a) = \\mbox{Pr}(X\\leq a) = \\int_{-\\infty}^a f(x)\\, dx \\] Para quem conhece o cálculo, lembre-se de que a integral está relacionada a uma soma: é a soma das barras com larguras próximas a 0. Se você não conhece o cálculo, pode pensar em \\(f(x)\\) como uma curva para a qual a área sob essa curva até o valor \\(a\\) dá a eles a probabilidade \\(\\mbox{Pr}(X\\leq a)\\). Por exemplo, para usar a aproximação normal para estimar a probabilidade de alguém ter mais de 76 polegadas, usamos: 1 - pnorm(76, m, s) #&gt; [1] 0.0321 que matematicamente é a área cinza abaixo: A curva que você vê é a densidade de probabilidade para a distribuição normal. Em R, obtemos isso usando a função dnorm. Embora possa não ser imediatamente óbvio por que é útil conhecer as densidades de probabilidade, entender esse conceito será essencial para aqueles que desejam ajustar modelos a dados para os quais não há funções predefinidas disponíveis. Simulações de Monte Carlo para variáveis contínuas R fornece funções para gerar resultados normalmente distribuídos. Especificamente, a função rnorm são necessários três argumentos: tamanho, média (padrão 0) e desvio padrão (padrão 1) e produz números aleatórios. Aqui está um exemplo de como podemos gerar dados que se parecem com nossas alturas: n &lt;- length(x) m &lt;- mean(x) s &lt;- sd(x) simulated_heights &lt;- rnorm(n, m, s) Não é de surpreender que a distribuição pareça normal: Essa é uma das funções mais úteis em R, pois permite gerar dados que imitam eventos naturais e responder a perguntas relacionadas ao que poderia acontecer por acaso ao executar simulações de Monte Carlo. Se, por exemplo, escolhermos 800 homens aleatoriamente, qual é a distribuição da pessoa mais alta? Quão raro é um homem de sete pés e sete pés de rodapé em um grupo de 800 homens? A seguinte simulação de Monte Carlo nos ajuda a responder a essa pergunta: B &lt;- 10000 tallest &lt;- replicate(B, { simulated_data &lt;- rnorm(800, m, s) max(simulated_data) }) Ter um pés de rodapé é bastante raro: mean(tallest &gt;= 7*12) #&gt; [1] 0.0199 Aqui vemos a distribuição resultante: Observe que isso não parece normal. 13.10 Distribuições contínuas Apresentamos a distribuição normal na Seção 8.8 e foi usado como um exemplo acima. A distribuição normal não é a única distribuição teórica útil. Outras distribuições contínuas que podemos encontrar são t de Student (Estudante t em inglês), qui-quadrado, exponencial, gama, beta e beta-binomial. R fornece funções para calcular densidade, quantis, funções de distribuição cumulativa e gerar simulações de Monte Carlo. R usa uma convenção que nos ajuda a lembrar nomes: use letras d, q, p e r na frente de uma abreviação do nome da distribuição. Já vimos as funções dnorm, pnorm e rnorm para a distribuição normal. A função qnorm nos dá os quantis. Portanto, podemos traçar uma distribuição como esta: x &lt;- seq(-4, 4, length.out = 100) qplot(x, f, geom = &quot;line&quot;, data = data.frame(x, f = dnorm(x))) Para a distribuição t do aluno, descrita mais adiante na Seção ??, a abreviação t é usado para funções serem dt para densidade, qt para quantis, pt para a função de distribuição cumulativa e rt para a simulação de Monte Carlo. 13.11 Exercícios 1. Suponha que a distribuição das alturas femininas seja aproximada por uma distribuição normal com uma média de 64 polegadas e um desvio padrão de 3 polegadas. Se escolhermos uma mulher aleatoriamente, qual é a probabilidade de ela ter um metro ou menos? 2. Suponha que a distribuição das alturas femininas seja aproximada por uma distribuição normal com uma média de 64 polegadas e um desvio padrão de 3 polegadas. Se escolhermos uma mulher aleatoriamente, qual é a probabilidade de ela ter um metro e oitenta ou mais? 3. Suponha que a distribuição das alturas femininas seja aproximada por uma distribuição normal com uma média de 64 polegadas e um desvio padrão de 3 polegadas. Se escolhermos uma mulher aleatoriamente, qual é a probabilidade de ela ter entre 61 e 67 polegadas? 4. Repita o exercício anterior, mas converta tudo em centímetros. Ou seja, multiplique cada altura, incluindo o desvio padrão, por 2,54. Qual é a resposta agora? 5. Observe que a resposta à pergunta não muda quando as unidades mudam. Isso faz sentido, pois a resposta à pergunta não deve ser afetada pelas unidades que usamos. De fato, se você olhar de perto, verá que 61 e 64 estão a 1 SD da média. Encontre a probabilidade de uma variável aleatória aleatória e normalmente distribuída estar dentro de 1 DP da média. 6. Para ver a matemática que explica por que as respostas às perguntas 3, 4 e 5 são as mesmas, suponha que você tenha uma variável aleatória média \\(m\\) e erro padrão \\(s\\). Suponha que você queira saber a probabilidade de que \\(X\\) é menor ou igual a \\(a\\). Lembre-se de que, por definição \\(a\\) é \\((a - m)/s\\) desvio padrão \\(s\\) da média \\(m\\). A probabilidade é: \\[ \\mbox{Pr}(X \\leq a) \\] Agora subtraia \\(\\mu\\) em ambos os lados e depois divida os dois lados por \\(\\sigma\\): \\[ \\mbox{Pr}\\left(\\frac{X-m}{s} \\leq \\frac{a-m}{s} \\right) \\] A quantidade à esquerda é uma variável aleatória normal padrão. Tem uma média de 0 e um erro padrão de 1. Vamos chamá-lo \\(Z\\): \\[ \\mbox{Pr}\\left(Z \\leq \\frac{a-m}{s} \\right) \\] Portanto, independentemente das unidades, a probabilidade de \\(X\\leq a\\) é igual à probabilidade de uma variável normal padrão ser menor que \\((a - m)/s\\). Sim mu é a média e sigma o erro padrão, qual dos seguintes códigos R fornecerá a resposta correta em cada situação? para. mean(X&lt;=a) b. pnorm((a - m)/s) c. pnorm((a - m)/s, m, s) d. pnorm(a) 7. Imagine que a distribuição de machos adultos seja aproximadamente normal, com um valor esperado de 69 e um desvio padrão de 3. Qual é a altura do macho no percentil 99? Dica: use qnorm. 8. A distribuição das pontuações de QI, ou QI, é aproximadamente distribuída normalmente. A média é 100 e o desvio padrão é 15. Suponha que você queira saber a distribuição dos QIs mais altos em todas as turmas de graduação de cada distrito escolar, cada uma com 10.000 pessoas. Execute uma simulação de Monte Carlo com B=1000 gerando 10.000 pontos de QI e mantendo os QIs mais altos. Faça um histograma. https://en.wikipedia.org/wiki/Urn_problem↩ https://www.khanacademy.org/math/precalculus/prob-comb/dependent-events-precalc/v/monty-hall-problem↩ https://en.wikipedia.org/wiki/Monty_Hall_problem↩ "],
["variáveis-aleatórias.html", "Capítulo 14 Variáveis aleatórias 14.1 Variáveis aleatórias 14.2 Modelos de amostragem 14.3 A distribuição de probabilidade de uma variável aleatória 14.4 Distribuições versus distribuições de probabilidade 14.5 O valor esperado e o erro padrão 14.6 Teorema do limite central 14.7 Propriedades estatísticas das médias 14.8 Lei dos grandes números 14.9 Exercícios 14.10 Estudo de caso: o grande short 14.11 Exercícios", " Capítulo 14 Variáveis aleatórias Na ciência de dados, geralmente trabalhamos com dados que são afetados de alguma forma por acaso. Alguns exemplos são dados provenientes de uma amostra aleatória, dados afetados por um erro de medição ou dados que medem um resultado de natureza aleatória. Ser capaz de quantificar a incerteza introduzida pela aleatoriedade é um dos trabalhos mais importantes dos analistas de dados. A inferência estatística oferece uma estrutura, bem como várias ferramentas práticas para isso. O primeiro passo é aprender a descrever matematicamente variáveis aleatórias. Neste capítulo, apresentamos variáveis aleatórias e suas propriedades começando com sua aplicação em jogos de azar. Em seguida, descrevemos alguns dos eventos que cercam a crise financeira de 2007-200847 usando a teoria da probabilidade. Essa crise financeira foi causada em parte pela subestimação do risco de certos valores mobiliários vendidos por instituições financeiras. Especificamente, os riscos de títulos lastreados em hipotecas (MBSs) e obrigações de dívida colateralizada (CDOs) foram amplamente subestimados. Esses ativos foram vendidos a preços que muitos proprietários esperavam que pagassem pontualmente, e a probabilidade de que isso não ocorresse foi calculada como baixa. Uma combinação de fatores resultou em muito mais inadimplências do que o esperado, levando a uma queda nos preços desses títulos. Como conseqüência, os bancos perderam tanto dinheiro que precisavam de resgates do governo para evitar o fechamento completo. 14.1 Variáveis aleatórias Variáveis aleatórias são os resultados numéricos de processos aleatórios. Podemos facilmente gerar variáveis aleatórias usando alguns dos exemplos acima. Por exemplo, defina X será 1 se a conta (bead em inglês) for azul e 0 caso contrário: beads &lt;- rep( c(&quot;red&quot;, &quot;blue&quot;), times = c(2,3)) X &lt;- ifelse(sample(beads, 1) == &quot;blue&quot;, 1, 0) Aqui X é uma variável aleatória: toda vez que selecionamos uma nova conta, o resultado muda aleatoriamente. Olhar para baixo: ifelse(sample(beads, 1) == &quot;blue&quot;, 1, 0) #&gt; [1] 1 ifelse(sample(beads, 1) == &quot;blue&quot;, 1, 0) #&gt; [1] 0 ifelse(sample(beads, 1) == &quot;blue&quot;, 1, 0) #&gt; [1] 0 Às vezes é 1 e às vezes é 0. 14.2 Modelos de amostragem Muitos procedimentos de geração de dados, aqueles que produzem os dados que estudamos, podem ser modelados relativamente bem como escolhas de urnas. Por exemplo, podemos modelar o provável processo de votação dos eleitores, como obter 0s (republicanos) e 1s (democratas) de uma urna que contém os códigos 0 e 1 para todos os prováveis eleitores. Em estudos epidemiológicos, geralmente assumimos que os sujeitos de nosso estudo são uma amostra aleatória da população de interesse. Os dados relacionados a um resultado específico podem ser modelados como uma amostra aleatória de uma urna contendo o resultado para toda a população de interesse. Da mesma forma, na pesquisa experimental, geralmente assumimos que os organismos individuais que estamos estudando, por exemplo, vermes, moscas ou ratos, são uma amostra aleatória de uma população maior. Experimentos aleatórios também podem ser modelados, selecionando uma urna, dada a maneira como os indivíduos são designados para grupos: quando designados, o grupo é escolhido aleatoriamente. Os modelos de amostragem são, portanto, onipresentes na ciência de dados. Os jogos de cassino oferecem um grande número de exemplos de situações do mundo real nas quais modelos de amostragem são usados para responder a perguntas específicas. Portanto, começaremos com esses exemplos. Suponha que um cassino muito pequeno o contrate para ver se eles devem incluir roletas em seus jogos. Para simplificar o exemplo, assumiremos que 1.000 pessoas jogarão e que a única aposta que podem fazer na roleta é apostar em vermelho ou preto. O cassino quer que eles prevejam quanto dinheiro ganharão ou perderão. Eles querem uma gama de valores e, em particular, querem saber qual é a probabilidade de perder dinheiro. Se essa probabilidade for muito alta, eles não instalarão roletas. Nós vamos definir uma variável aleatória \\(S\\) que representará o total de ganhos do cassino. Vamos começar construindo a urna. Uma roleta tem 18 bolsos vermelhos, 18 bolsos pretos e 2 verdes. Portanto, jogar uma cor em um jogo de roleta é equivalente a escolher na caixa a seguir: color &lt;- rep(c(&quot;Black&quot;, &quot;Red&quot;, &quot;Green&quot;), c(18, 18, 2)) Os 1.000 resultados de 1.000 pessoas jogando são empates independentes desta urna. Se aparecer vermelho, o jogador ganha e o cassino perde um dólar, então empatamos um - $1. De otra manera, el casino gana un dólar y sacamos un $ 1. Para construir nossa variável aleatória \\(S\\), podemos usar este código: n &lt;- 1000 X &lt;- sample(ifelse(color == &quot;Red&quot;, -1, 1), n, replace = TRUE) X[1:10] #&gt; [1] -1 1 1 -1 -1 -1 1 1 1 1 Como sabemos as proporções de 1s e -1s, podemos gerar as eleições com uma linha de código, sem definir color: X &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19)) Chamamos isso de modelo de amostragem, pois estamos modelando o comportamento aleatório da roleta com a amostragem das opções das urnas. Ganhos totais \\(S\\) eles são simplesmente a soma desses 1.000 empates independentes: X &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19)) S &lt;- sum(X) S #&gt; [1] 22 14.3 A distribuição de probabilidade de uma variável aleatória Se você executar o código acima, verá que \\(S\\) muda sempre. Isto é porque \\(S\\) é uma variável aleatória. A distribuição de probabilidade de uma variável aleatória nos diz a probabilidade de que o valor observado caia em um determinado intervalo. Por exemplo, se queremos saber a probabilidade de perdermos dinheiro, estamos perguntando a probabilidade de que \\(S\\) estar no intervalo \\(S&lt;0\\). Lembre-se de que, se pudermos definir uma função de distribuição cumulativa \\(F(a) = \\mbox{Pr}(S\\leq a)\\), então podemos responder a qualquer pergunta relacionada à probabilidade de eventos definida por nossa variável aleatória \\(S\\), incluindo o evento \\(S&lt;0\\). A esta \\(F\\) nós dizemos a você a função de distribuição da variável aleatória. Podemos estimar a função de distribuição para a variável aleatória \\(S\\) usando uma simulação de Monte Carlo para gerar muitas realizações da variável aleatória. Com esse código, realizamos o experimento de ter 1.000 pessoas jogando roleta várias vezes, especificamente \\(B = 10,000\\) vezes: n &lt;- 1000 B &lt;- 10000 roulette_winnings &lt;- function(n){ X &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19)) sum(X) } S &lt;- replicate(B, roulette_winnings(n)) Agora podemos perguntar o seguinte: em nossas simulações, com que frequência recebemos somas inferiores ou iguais a a? mean(S &lt;= a) Essa será uma aproximação muito boa de \\(F(a)\\) e podemos facilmente responder à pergunta do cassino: qual a probabilidade de perdermos dinheiro? Podemos ver que a probabilidade é bastante baixa: mean(S&lt;0) #&gt; [1] 0.0456 Podemos visualizar a distribuição de \\(S\\) criando um histograma mostrando a probabilidade \\(F(b)-F(a)\\) em vários intervalos \\((a,b]\\): Vemos que a distribuição parece ser aproximadamente normal. Um gráfico QQ confirmará que a aproximação normal está próxima de uma aproximação perfeita para esta distribuição. De fato, se a distribuição é normal, tudo o que precisamos para definir a distribuição é a média e o desvio padrão. Como temos os valores originais dos quais a distribuição é criada, podemos calculá-los facilmente com mean(S) e sd(S). A curva azul adicionada ao histograma acima é uma densidade normal com essa média e desvio padrão. Essa média e esse desvio padrão têm nomes especiais. Eles são conhecidos como expected value_e_standard error da variável aleatória \\(S\\) e será discutido mais adiante na próxima seção. A teoria estatística fornece uma maneira de derivar a distribuição de variáveis aleatórias definidas como extrações aleatórias independentes de uma urna. Especificamente, em nosso exemplo anterior, podemos mostrar que \\((S+n)/2\\) segue uma distribuição binomial. Portanto, não precisamos executar simulações de Monte Carlo para conhecer a distribuição de probabilidade de \\(S\\). Fizemos isso para fins ilustrativos. Nós podemos usar as funções dbinom e pbinom para calcular exatamente as probabilidades. Por exemplo, para calcular \\(\\mbox{Pr}(S &lt; 0)\\) nós notamos que: \\[\\mbox{Pr}(S &lt; 0) = \\mbox{Pr}((S+n)/2 &lt; (0+n)/2)\\] e podemos usar pbinom calcular: \\[\\mbox {Pr}(S \\leq 0) \\] n &lt;- 1000 pbinom(n/2, size = n, prob = 10/19) #&gt; [1] 0.0511 Por se tratar de uma função de probabilidade discreta, obter \\(\\mbox{Pr}(S &lt; 0)\\) ao invés de \\(\\mbox{Pr}(S \\leq 0)\\), nós escrevemos: pbinom(n/2-1, size = n, prob = 10/19) #&gt; [1] 0.0448 Para mais detalhes sobre a distribuição binomial, você pode consultar qualquer livro de probabilidades básico ou mesmo Wikipedia48. Nós não cobrimos esses detalhes aqui. Em vez disso, discutiremos uma abordagem incrivelmente útil que nos fornece a teoria matemática que geralmente se aplica a somas e calcula as médias de qualquer caixa de votação: o Teorema do Limite Central, ou CLT. 14.4 Distribuições versus distribuições de probabilidade Antes de continuar, vamos fazer uma distinção e uma conexão importante entre a distribuição de uma lista de números e uma distribuição de probabilidade. No capítulo de visualização, descrevemos como qualquer lista de números \\(x_1,\\dots,x_n\\) tem uma distribuição. A definição é bastante simples. Nós definimos \\(F(a)\\) como a função que nos diz que proporção da lista é menor ou igual a \\(a\\). Como são resumos úteis quando a distribuição é aproximadamente normal, definimos a média e o desvio padrão. Eles são definidos com uma operação simples do vetor que contém a lista de números x: m &lt;- sum(x)/length(x) s &lt;- sqrt(sum((x - m)^2)/ length(x)) Uma variável aleatória \\(X\\) tem uma função de distribuição. Para definir isso, não precisamos de uma lista de números. É um conceito teórico. Nesse caso, definimos a distribuição como o \\(F(a)\\) respondendo à pergunta: qual é a probabilidade de \\(X\\) é menor ou igual a \\(a\\)? Não há lista de números. No entanto, se \\(X\\) é definido como uma seleção de uma urna com números, portanto, há uma lista: a lista de números dentro da urna. Nesse caso, a distribuição dessa lista é a distribuição de probabilidade de \\(X\\) e a média e o desvio padrão dessa lista são o valor esperado e o erro padrão da variável aleatória. Outra maneira de pensar sobre isso que não envolve uma urna é executar uma simulação de Monte Carlo e gerar uma lista muito grande de resultados de \\(X\\). Estes resultados são uma lista de números. A distribuição desta lista será uma aproximação muito boa da distribuição de probabilidade de \\(X\\). Quanto maior a lista, melhor a aproximação. A média e o desvio padrão desta lista aproximam o valor esperado e o erro padrão da variável aleatória. Notação ## para variáveis aleatórias Nos livros estatísticos, as letras maiúsculas são usadas para denotar variáveis aleatórias e seguimos esta convenção aqui. Letras minúsculas são usadas para os valores observados. Você verá alguma notação que inclui ambos. Por exemplo, eles verão eventos definidos como \\(X \\leq x\\). Aqui \\(X\\) é uma variável aleatória, portanto, é um evento aleatório e \\(x\\) é um valor arbitrário e não aleatório. Por exemplo, \\(X\\) poderia representar o número em um dado e \\(x\\) representará um valor real que vemos: 1, 2, 3, 4, 5 ou 6. Portanto, neste caso, a probabilidade de \\(X=x\\) é 1/6 independentemente do valor observado \\(x\\). Essa notação é um pouco estranha porque, quando fazemos perguntas de probabilidade, \\(X\\) não é uma quantidade observada, mas uma quantidade aleatória que veremos no futuro. Podemos descrever o que esperamos ver, quais valores são prováveis, mas não o que é. Mas assim que temos dados, vemos uma realização de \\(X\\). Então, os cientistas de dados falam sobre o que poderia ter sido depois de ver o que realmente aconteceu. 14.5 O valor esperado e o erro padrão Nós descrevemos modelos de amostragem para sorteios. Agora, revisaremos a teoria matemática que nos permite aproximar as distribuições de probabilidade para a soma dos empates. Depois de fazer isso, podemos ajudar o cassino a prever quanto dinheiro eles ganharão. A mesma abordagem que usamos para a soma dos sorteios será útil para descrever a distribuição das médias e a proporção que precisaremos para entender como as pesquisas funcionam. O primeiro conceito importante a aprender é o valor esperado. Nos livros de estatística, é comum usar a letra \\(\\mbox{E}\\) assim: \\[\\mbox{E}[X]\\] para denotar o valor esperado da variável aleatória \\(X\\). Uma variável aleatória variará em torno do valor esperado de uma maneira que, se eles tiverem a média de muitos, muitos empates, a média dos empates se aproximará do valor esperado, aproximando-se cada vez mais à medida que os empates aumentam. A estatística teórica oferece técnicas que facilitam o cálculo dos valores esperados em diferentes circunstâncias. Por exemplo, uma fórmula útil nos diz que o valor esperado de uma variável aleatória definida por um empate é a média dos números na urna. Na urna que usamos para modelar apostas de roleta, temos $ 20 e $ 18 negativos. O valor esperado é então: \\[ \\mbox{E}[X] = (20 + -18)/38 \\] que é como 5 centavos. É um pouco contraditório dizer que \\(X\\) varia em torno de 0,05, quando os únicos valores necessários são 1 e -1. Uma maneira de entender o valor esperado nesse contexto é perceber que, se jogarmos o jogo repetidamente, o cassino ganha, em média, 5 centavos por jogo. Uma simulação de Monte Carlo confirma isso: B &lt;- 10^6 x &lt;- sample(c(-1,1), B, replace = TRUE, prob=c(9/19, 10/19)) mean(x) #&gt; [1] 0.0517 Em geral, se a urna tem dois resultados possíveis, digamos \\(a\\) e \\(b\\) com proporções \\(p\\) e \\(1-p\\) respectivamente, a média é: \\[\\mbox{E}[X] = ap + b(1-p)\\] Para ver isso, observe que, se houver \\(n\\) contas nas urnas, então temos \\(np\\) \\(a\\) sy \\(n(1-p)\\) \\(b\\) s, e como a média é a soma, \\(n\\times a \\times p + n\\times b \\times (1-p)\\), dividido pelo total \\(n\\), obtemos que a média é \\(ap + b(1-p)\\). Agora, a razão pela qual definimos o valor esperado é porque essa definição matemática é útil para aproximar as distribuições de probabilidade da soma, que é útil para descrever a distribuição de médias e proporções. O primeiro fato útil é que o valor esperado da soma dos draws é: \\[ \\mbox{}\\mbox{number of draws } \\times \\mbox{ average of the numbers in the urn} \\] Portanto, se 1.000 pessoas jogam roleta, o cassino espera ganhar, em média, cerca de 1.000 \\(\\times\\) $0.05 = $ 50. Mas esse é um valor esperado. Quão diferente pode ser uma observação do valor esperado? O cassino realmente precisa saber disso. Qual é o intervalo de probabilidades? Se números negativos forem muito prováveis, eles não instalarão roletas. A teoria estatística mais uma vez responde a essa pergunta. O padrão padrão, ou SE, nos dá uma ideia do tamanho da variação em torno do valor esperado. Nos livros de estatística, é comum usar: \\[\\mbox{SE}[X]\\] para denotar o erro padrão de uma variável aleatória. Se nossos sorteios são independentes, o standard sum error é dado pela equação: \\[ \\sqrt{\\mbox{number of draws }} \\times \\mbox{ standard deviation of the numbers in the urn} \\] Usando a definição de desvio padrão, podemos derivar, com um pouco de matemática, que se uma urna contiver dois valores \\(a\\) e \\(b\\) com proporções \\(p\\) e \\((1-p)\\), respectivamente, o desvio padrão é: \\[\\mid b - a \\mid \\sqrt{p(1-p)}.\\] Portanto, no nosso exemplo de roleta, o desvio padrão dos valores dentro da urna é: \\(\\mid 1 - (-1) \\mid \\sqrt{10/19 \\times 9/19}\\) ou: 2 * sqrt(90)/19 #&gt; [1] 0.999 O erro padrão nos diz a diferença típica entre uma variável aleatória e sua expectativa. Como um empate é obviamente a soma de um único empate, podemos usar a fórmula acima para calcular que a variável aleatória definida por um empate tem um valor esperado de 0,05 e um erro padrão de cerca de 1. Isso faz sentido, pois obtemos 1 ou -1, com 1 ligeiramente favorecido sobre -1. Usando a fórmula acima, a soma de 1.000 pessoas jogando tem um erro padrão de aproximadamente $ 32: n &lt;- 1000 sqrt(n) * 2 * sqrt(90)/19 #&gt; [1] 31.6 Como resultado, quando 1.000 pessoas apostarem no vermelho, o cassino deverá ganhar $50 con un error estándar de $ 32. Parece uma aposta segura. Mas ainda não respondemos à pergunta: qual a probabilidade de perder dinheiro? Aqui a CLT nos ajudará. Nota Avançada: Antes de continuar, devemos observar que os cálculos exatos de probabilidade de vitória no cassino podem ser feitos com a distribuição binomial. No entanto, aqui nos concentramos no CLT, que geralmente pode ser aplicado a somas de variáveis aleatórias, algo que não pode ser feito com a distribuição binomial. 14.5.1 População SD versus amostra SD O desvio padrão, ou SD, de uma lista x (usamos as alturas abaixo como exemplo) é definida como a raiz quadrada da média das diferenças quadradas: library(dslabs) x &lt;- heights$height m &lt;- mean(x) s &lt;- sqrt(mean((x-m)^2)) Usando notação matemática, escrevemos: \\[ \\mu = \\frac{1}{n} \\sum_{i=1}^n x_i \\\\ \\sigma = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2} \\] No entanto, observe que a função sd retorna um resultado ligeiramente diferente: identical(s, sd(x)) #&gt; [1] FALSE s-sd(x) #&gt; [1] -0.00194 Isso ocorre porque a função sd em R não retorna o sd da lista, mas usa uma fórmula que estima desvios padrão da população usando uma amostra aleatória \\(X_1, \\dots, X_N\\) que, por razões não discutidas aqui, divide a soma dos quadrados por \\(N-1\\). \\[ \\bar{X} = \\frac{1}{N} \\sum_{i=1}^N X_i, \\,\\,\\,\\, s = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (X_i - \\bar{X})^2} \\] Você pode ver que esse é o caso escrevendo: n &lt;- length(x) s-sd(x)*sqrt((n-1)/ n) #&gt; [1] 0 Para toda a teoria discutida aqui, eles devem calcular o desvio padrão real, conforme definido: sqrt(mean((x-m)^2)) Portanto, tenha cuidado ao usar a função sd em R. No entanto, lembre-se de que, ao longo do livro, às vezes usamos a função sd quando realmente queremos o verdadeiro SD. Isso ocorre porque quando o tamanho da lista é grande, esses dois são praticamente equivalentes, pois \\(\\sqrt{(N-1)/N} \\approx 1\\). 14.6 Teorema do limite central O Teorema do Limite Central, ou CLT, diz-nos que quando o número de empates, também chamado tamanho da amostra, é grande, a distribuição de probabilidade da soma dos empates independentes é aproximadamente normal. Como os modelos de amostragem são usados para muitos processos de geração de dados, o CLT é considerado uma das idéias matemáticas mais importantes da história. Anteriormente, discutimos que, se soubermos que a distribuição de uma lista de números se aproxima da distribuição normal, tudo o que precisamos para descrever a lista é a média e o desvio padrão. Também sabemos que o mesmo se aplica às distribuições de probabilidade. Se uma variável aleatória tem uma distribuição de probabilidade que se aproxima da distribuição normal, tudo o que precisamos para descrever a distribuição de probabilidade é a média e o desvio padrão, conhecido como valor esperado e erro padrão. Anteriormente, executamos esta simulação de Monte Carlo: n &lt;- 1000 B &lt;- 10000 roulette_winnings &lt;- function(n){ X &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19)) sum(X) } S &lt;- replicate(B, roulette_winnings(n)) A CLT nos diz que a soma \\(S\\) é aproximado por uma distribuição normal. Usando as fórmulas acima, sabemos que o valor esperado e o erro padrão são: n * (20-18)/38 #&gt; [1] 52.6 sqrt(n) * 2 * sqrt(90)/19 #&gt; [1] 31.6 Os valores teóricos anteriores coincidem com os obtidos com a simulação de Monte Carlo: mean(S) #&gt; [1] 52.2 sd(S) #&gt; [1] 31.7 Usando o CLT, podemos pular a simulação de Monte Carlo e, em vez disso, calcular a probabilidade de o cassino perder dinheiro usando esta aproximação: mu &lt;- n * (20-18)/38 se &lt;- sqrt(n) * 2 * sqrt(90)/19 pnorm(0, mu, se) #&gt; [1] 0.0478 o que também concorda muito bem com o resultado da simulação de Monte Carlo: mean(S &lt; 0) #&gt; [1] 0.0458 Qual é o tamanho do teorema do limite central? O CLT funciona quando o número de empates é grande. Mas ótimo é um termo relativo. Em muitas circunstâncias, apenas 30 empates são suficientes para que o CLT seja útil. Em alguns casos específicos, apenas 10 são suficientes. No entanto, estas não devem ser consideradas regras gerais. Por exemplo, quando a probabilidade de sucesso é muito pequena, precisamos de tamanhos de amostra muito maiores. Como ilustração, considere a loteria. Na loteria, a probabilidade de ganhar é menor que 1 em um milhão. Milhares de pessoas jogam, então o número de empates é muito grande. No entanto, o número de vencedores, a soma dos empates, varia de 0 a 4. A distribuição normal não é uma boa aproximação da soma, portanto o CLT não se aplica, mesmo quando o tamanho da amostra é muito grande. . Isso geralmente é verdade quando a probabilidade de sucesso é muito baixa. Nestes casos, a distribuição de Poisson é mais apropriada. Você pode examinar as propriedades da distribuição Poisson usando dpois e ppois. Eles podem gerar variáveis aleatórias seguindo esta distribuição com rpois. No entanto, não discutimos a teoria aqui. Para saber mais sobre a distribuição de Poisson, você pode consultar qualquer livro de probabilidade e até Wikipedia49 14.7 Propriedades estatísticas das médias Existem vários resultados matemáticos úteis que usamos anteriormente e que costumamos empregar ao trabalhar com dados. Listamos-os abaixo. 1. O valor esperado da soma das variáveis aleatórias é a soma do valor esperado de cada variável aleatória. Podemos escrever assim: \\[ \\mbox{E}[X_1+X_2+\\dots+X_n] = \\mbox{E}[X_1] + \\mbox{E}[X_2]+\\dots+\\mbox{E}[X_n] \\] Se ele \\(X\\) são empates independentes das urnas, portanto todos têm o mesmo valor esperado. Vamos chamá-lo \\(\\mu\\) e, por conseguinte: \\[ \\mbox{E}[X_1+X_2+\\dots+X_n]= n\\mu \\] que é outra maneira de escrever o resultado mostrado acima para a soma dos sorteios. 2. O valor esperado de uma constante não aleatória multiplicada por uma variável aleatória é a constante não aleatória multiplicada pelo valor esperado de uma variável aleatória. É mais fácil explicar com símbolos: \\[ \\mbox{E}[aX] = a\\times\\mbox{E}[X] \\] Para ver por que isso é intuitivo, considere alterar as unidades. Se mudarmos as unidades de uma variável aleatória, digamos de dólares para centavos, a expectativa deve mudar da mesma maneira. Uma conseqüência dos dois fatos anteriores é que o valor esperado da média de extrações independentes da mesma urna é o valor esperado da urna. \\(\\mu\\) de novo: \\[ \\mbox{E}[(X_1+X_2+\\dots+X_n)/ n]= \\mbox{E}[X_1+X_2+\\dots+X_n]/ n = n\\mu/n = \\mu \\] 3. O quadrado do erro padrão da soma das variáveis aleatórias independentes é a soma do quadrado do erro padrão de cada variável aleatória. É mais fácil entender matematicamente: \\[ \\mbox{SE}[X_1+X_2+\\dots+X_n] = \\sqrt{\\mbox{SE}[X_1]^2 + \\mbox{SE}[X_2]^2+\\dots+\\mbox{SE}[X_n]^2 } \\] O quadrado do erro padrão é chamado variance nos livros de estatística. Observe que essa propriedade em particular não é tão intuitiva quanto as duas anteriores e que explicações detalhadas podem ser encontradas nos livros de estatística. 4. O erro padrão de uma constante não aleatória multiplicada por uma variável aleatória é a constante não aleatória multiplicada pelo erro padrão da variável aleatória. O mesmo que para o valor esperado: \\[ \\mbox{SE}[aX] = a \\times \\mbox{SE}[X] \\] Para ver por que isso é intuitivo, pense novamente nas unidades. Uma consequência de 3 e 4 é que o erro padrão da média de empates independentes para a mesma urna é o desvio padrão da urna dividido pela raiz quadrada de \\(n\\) (o número de empates), chame-o \\(\\sigma\\): \\[ \\begin{aligned} \\mbox{SE}[(X_1+X_2+\\dots+X_n)/ n] &amp;= \\mbox{SE}[X_1+X_2+\\dots+X_n]/n \\\\ &amp;= \\sqrt{\\mbox{SE}[X_1]^2+\\mbox{SE}[X_2]^2+\\dots+\\mbox{SE}[X_n]^2}/n \\\\ &amp;= \\sqrt{\\sigma^2+\\sigma^2+\\dots+\\sigma^2}/n\\\\ &amp;= \\sqrt{n\\sigma^2}/n\\\\ &amp;= \\sigma/ \\sqrt{n} \\end{aligned} \\] 5. Sim \\(X\\) é uma variável aleatória distribuída normalmente, portanto, se \\(a\\) e \\(b\\) são constantes não aleatórias, \\(aX + b\\) também é uma variável aleatória distribuída normalmente. Tudo o que estamos fazendo é alterar as unidades da variável aleatória multiplicando por \\(a\\) e, em seguida, movendo o centro \\(b\\). Lembre-se de que os estatísticos usam livros gregos \\(\\mu\\) e \\(\\sigma\\) para indicar o valor esperado e o erro padrão, respectivamente. Isto é porque \\(\\mu\\) é a letra grega para \\(m\\), a primeira letra de mean, que é outro termo usado para o valor esperado. Da mesma forma, \\(\\sigma\\) é a letra grega para \\(s\\), a primeira letra do erro padrão. 14.8 Lei dos grandes números Uma implicação importante do resultado final é que o erro padrão da média se torna cada vez menor conforme \\(n\\) torna-se maior. Quando \\(n\\) é muito grande, portanto o erro padrão é praticamente 0 e a média dos empates converge para a média das urnas. Isso é conhecido nos livros estatísticos como a lei dos grandes números ou a lei das médias. 14.8.1 Interpretação incorreta da lei das médias A lei das médias às vezes é mal interpretada. Por exemplo, se eles jogam uma moeda 5 vezes e ela joga cara a cada vez, alguém pode argumentar que o próximo sorteio é provavelmente coroa devido à lei das médias: em média, devemos ver 50 % cara e 50 % coroa. Um argumento semelhante seria dizer que “toca” o vermelho na roleta depois de ver que o preto aparece cinco vezes seguidas. Como esses eventos são independentes, a probabilidade de uma moeda rolar é de 50 %, independentemente dos 5 resultados anteriores. Este também é o caso do resultado da roleta. A lei das médias se aplica somente quando o número de empates é muito grande e não em amostras pequenas. Após um milhão de lançamentos, você definitivamente verá cerca de 50 % de cabeças, independentemente do resultado dos cinco primeiros lançamentos. Outro uso incorreto interessante da lei das médias está nos esportes, quando os apresentadores de televisão prevêem que um jogador está prestes a ter sucesso porque falhou várias vezes seguidas. 14.9 Exercícios 1. Na roleta americana, você também pode apostar no verde. Existem 18 vermelhos, 18 pretos e 2 verdes (0 e 00). Quais são as chances de um green sair? 2. O pagamento verde é $17 dollars. Esto significa que si apuesta un dólar y cae en verde, obtiene $ 17. Crie um modelo de amostragem usando uma amostra para simular a variável aleatória \\(X\\) dos seus ganhos. Dica: Veja o exemplo abaixo para o código de apostas vermelho. x &lt;- sample(c(1,-1), 1, prob = c(9/19, 10/19)) 3. Qual é o valor esperado de \\(X\\)? 4. Qual é o erro padrão de \\(X\\)? 5. Agora crie uma variável aleatória \\(S\\) que é a soma dos seus ganhos depois de apostar verde 1.000 vezes. Dica: altere os argumentos size e replace na sua resposta à pergunta 2. Inicie seu código configurando a semente como 1 com set.seed(1). 6. Qual é o valor esperado de \\(S\\)? 7. Qual é o erro padrão de \\(S\\)? 8. Qual é a probabilidade de você acabar ganhando dinheiro? Dica: use o CLT. 9. Crie uma simulação de Monte Carlo que gere 1.000 resultados a partir de \\(S\\). Calcule a média e o desvio padrão da lista resultante para confirmar os resultados de 6 e 7. Inicie seu código configurando a semente como 1 com set.seed(1). 10. Agora verifique sua resposta à pergunta 8 usando o resultado da simulação de Monte Carlo. Onze. O resultado da simulação de Monte Carlo e a aproximação CLT estão próximos, mas não tão próximos. O que poderia explicar isso? para. 1.000 simulações não são suficientes. Se fizermos mais, eles coincidem. b. O CLT não funciona tão bem quando a probabilidade de sucesso é pequena. Nesse caso, era 1/19. Se aumentarmos o número de jogos de roleta, eles coincidirão melhor. c. A diferença está dentro do erro de arredondamento. d. O CLT funciona apenas para médias. 12. Agora crie uma variável aleatória \\(Y\\) faça o seu pagamento médio por aposta depois de apostar 1.000 vezes no green. 13. Qual é o valor esperado de \\(Y\\)? 14. Qual é o erro padrão de \\(Y\\)? Quinze. Qual é a probabilidade de que, quando você terminar de jogar, os ganhos por jogo sejam positivos? Dica: use o CLT. 16. Crie uma simulação de Monte Carlo que gere 2.500 resultados a partir de \\(Y\\). Calcule a média e o desvio padrão da lista resultante para confirmar os resultados de 6 e 7. Inicie seu código configurando a semente como 1 com set.seed(1). 17. Agora verifique sua resposta para 8 usando o resultado da simulação de Monte Carlo. 18. O resultado da simulação de Monte Carlo e a aproximação CLT estão agora muito mais próximos. O que poderia explicar isso? para. Agora estamos calculando médias em vez de somas. b. 2.500 simulações de Monte Carlo não são melhores que 1.000. c. O CLT funciona melhor quando o tamanho da amostra é maior. Aumentado de 1.000 para 2.500. d. Não está mais perto. A diferença está dentro do erro de arredondamento. 14.10 Estudo de caso: o grande short 14.10.1 Taxas de juros explicadas com um modelo de oportunidade Os bancos também usam versões mais complexas dos modelos de amostragem que discutimos para determinar suas taxas de juros. Suponha que você compre um banco pequeno que tenha um histórico de identificação de possíveis proprietários que podem confiar em seus pagamentos. De fato, historicamente, em um determinado ano, apenas 2% de seus clientes não pagam o dinheiro que lhes foi emprestado. No entanto, o banco sabe que se você simplesmente emprestar dinheiro a todos os seus clientes sem juros, você acabará perdendo dinheiro por causa desses 2%. Embora o banco saiba que provavelmente 2% de seus clientes não pagarão, não sabe quais são. No entanto, cobrando a todos um pouco mais de juros, eles podem compensar as perdas incorridas devido a esses 2% e também cobrir seus custos operacionais. Eles também podem obter lucro, mas se definirem taxas de juros muito altas, os clientes irão para outro banco. Usaremos todos esses fatos e um pouco da teoria das probabilidades para determinar qual taxa de juros cobrar. Suponha que seu banco faça 1.000 empréstimos de $180,000 este año. Además, tras sumar todos los costos, supongamos que su banco pierde $ 200.000 para execução duma hipoteca. Para simplificar, assumimos que isso inclui todos os custos operacionais. Um modelo de amostragem para esse cenário pode ser codificado da seguinte maneira: n &lt;- 1000 loss_per_foreclosure &lt;- -200000 p &lt;- 0.02 defaults &lt;- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE) sum(defaults * loss_per_foreclosure) #&gt; [1] -3800000 Observe que a perda total definida pela soma final é uma variável aleatória. Sempre que eles executam o código acima, eles recebem uma resposta diferente. Podemos facilmente construir uma simulação de Monte Carlo para ter uma idéia da distribuição dessa variável aleatória. B &lt;- 10000 losses &lt;- replicate(B, { defaults &lt;- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE) sum(defaults * loss_per_foreclosure) }) Nós realmente não precisamos de uma simulação de Monte Carlo. Usando o que aprendemos, o CLT nos diz que, como nossas perdas são uma soma de empates independentes, sua distribuição é aproximadamente normal com o valor esperado e os erros padrão dados por: n*(p*loss_per_foreclosure + (1-p)*0) #&gt; [1] -4e+06 sqrt(n)*abs(loss_per_foreclosure)*sqrt(p*(1-p)) #&gt; [1] 885438 Agora podemos definir uma taxa de juros para garantir que, em média, alcancemos um ponto de equilíbrio. Basicamente, precisamos adicionar uma quantidade \\(x\\) para cada empréstimo, que neste caso é representado por empates, para que o valor esperado seja 0. Se definirmos \\(l\\) para ser a perda de execução duma hipoteca, precisamos: \\[ lp + x(1-p) = 0 \\] implicando \\(x\\) é: - loss_per_foreclosure*p/(1-p) #&gt; [1] 4082 ou uma taxa de juros de 0.023. No entanto, ainda temos um problema. Embora essa taxa de juros garanta que, em média, eles atinjam um ponto de equilíbrio, há 50% de chance de perder dinheiro. Se o seu banco perder dinheiro, eles terão que fechá-lo. Portanto, eles devem escolher uma taxa de juros que os proteja disso. Ao mesmo tempo, se a taxa de juros for muito alta, seus clientes irão para outro banco; portanto, devem estar dispostos a correr alguns riscos. Então, digamos que você queira que suas chances de perder dinheiro sejam de 1 em 100, então, quanto deve ser \\(x\\) agora? Isso é um pouco mais difícil. Nós queremos a soma \\(S\\) ter: \\[\\mbox{Pr}(S&lt;0) = 0.01\\] Sabemos que \\(S\\) é aproximadamente normal. O valor esperado de \\(S\\) é: \\[\\mbox{E}[S] = \\{ lp + x(1-p)\\}n\\] com \\(n\\) o número de seleções, que neste caso representa empréstimos. O erro padrão é: \\[\\mbox{SD}[S] = |x-l| \\sqrt{np(1-p)}.\\] Por quê \\(x\\) é positivo e \\(l\\) negativo \\(|x-l|=x-l\\). Observe que essas são apenas uma aplicação das fórmulas mostradas acima, mas usam símbolos mais compactos. Agora vamos usar um “truque” matemático que é muito comum em estatística. Adicionamos e subtraímos os mesmos valores aos dois lados do evento \\(S&lt;0\\) para que a probabilidade não mude e acabemos com uma variável aleatória normal padrão à esquerda, o que nos permitirá escrever uma equação com apenas \\(x\\) como um estranho. Esse “truque” é o seguinte: Sim \\(\\mbox{Pr}(S&lt;0) = 0.01\\), tão: \\[ \\mbox{Pr}\\left(\\frac{S - \\mbox{E}[S]}{\\mbox{SE}[S]} &lt; \\frac{ - \\mbox{E}[S]}{\\mbox{SE}[S]}\\right) \\] E lembre-se disso \\(\\mbox{E}[S]\\) e \\(\\mbox{SE}[S]\\) são o valor esperado e o erro padrão de \\(S\\), respectivamente. Tudo o que fizemos no andar de cima foi somar e dividir pela mesma quantia em ambos os lados. Fizemos isso porque agora o termo à esquerda é uma variável aleatória normal padrão, à qual iremos renomear \\(Z\\). Agora preenchemos os espaços em branco com a fórmula atual para o valor esperado e o erro padrão: \\[ \\mbox{Pr}\\left(Z &lt; \\frac{- \\{ lp + x(1-p)\\}n}{(x-l) \\sqrt{np(1-p)}}\\right) = 0.01 \\] Pode parecer complicado, mas lembre-se de que \\(l\\), \\(p\\) e \\(n\\) todas são quantidades conhecidas, então, eventualmente, as substituiremos por números. Agora como \\(Z\\) é uma variável aleatória normal com valor esperado 0 e erro padrão 1, significa que a quantidade no lado direito do sinal &lt;deve ser igual a: qnorm(0.01) #&gt; [1] -2.33 para que a equação seja verdadeira. Lembre-se disso \\(z=\\) qnorm(0.01) nos dá o valor de \\(z\\) para qual: \\[ \\mbox{Pr}(Z \\leq z) = 0.01 \\] Isso significa que o lado direito da equação complicada deve ser \\(z\\)=qnorm(0.01): \\[ \\frac{- \\{ lp + x(1-p)\\}n} {(x-l) \\sqrt{n p (1-p)}} = z \\] O truque funciona porque acabamos com uma expressão que contém \\(x\\), que sabemos que deve ser igual a uma quantidade conhecida \\(z\\). Agora resolva para \\(x\\) é apenas álgebra: \\[ x = - l \\frac{ np - z \\sqrt{np(1-p)}}{n(1-p) + z \\sqrt{np(1-p)}}\\] o que é: l &lt;- loss_per_foreclosure z &lt;- qnorm(0.01) x &lt;- -l*( n*p - z*sqrt(n*p*(1-p)))/ ( n*(1-p) + z*sqrt(n*p*(1-p))) x #&gt; [1] 6249 Sua taxa de juros agora sobe para 0.035. Ainda é uma taxa de juros muito competitiva. Ao escolher essa taxa de juros, agora você terá um retorno esperado do empréstimo de: loss_per_foreclosure*p + x*(1-p) #&gt; [1] 2124 que é um benefício total esperado de aproximadamente: n*(loss_per_foreclosure*p + x*(1-p)) #&gt; [1] 2124198 dólares! Podemos executar uma simulação de Monte Carlo para verificar nossas aproximações teóricas: B &lt;- 100000 profit &lt;- replicate(B, { draws &lt;- sample( c(x, loss_per_foreclosure), n, prob=c(1-p, p), replace = TRUE) sum(draws) }) mean(profit) #&gt; [1] 2126479 mean(profit&lt;0) #&gt; [1] 0.0124 14.10.2 O Grande Curto Um de seus funcionários ressalta que, como o banco está ganhando 2,124 dólares por empréstimo, o banco deve fazer mais empréstimos! Porque sozinho \\(n\\)? Você explica que encontrar aqueles \\(n\\) clientes foi difícil. Eles precisam de um grupo que seja previsível e que mantenha as chances de inadimplência baixas. Seu funcionário então ressalta que, mesmo que a probabilidade de inadimplência seja maior, desde que o valor esperado seja positivo, o banco pode minimizar sua probabilidade de perda aumentando \\(n\\) e confie na lei de grandes números. Seu funcionário afirma ainda que, mesmo que a taxa padrão seja o dobro, digamos 4%, se definirem a taxa um pouco mais alta que esse valor: p &lt;- 0.04 r &lt;- (- loss_per_foreclosure*p/(1-p))/ 180000 r #&gt; [1] 0.0463 o banco será beneficiado. Em 5%, um valor positivo esperado de: r &lt;- 0.05 x &lt;- r*180000 loss_per_foreclosure*p + x * (1-p) #&gt; [1] 640 e eles podem minimizar suas chances de perder dinheiro simplesmente aumentando \\(n\\) já que: \\[ \\mbox{Pr}(S &lt; 0) = \\mbox{Pr}\\left(Z &lt; - \\frac{\\mbox{E}[S]}{\\mbox{SE}[S]}\\right) \\] com \\(Z\\) uma variável aleatória normal padrão, como mostrado acima. Se definirmos \\(\\mu\\) e \\(\\sigma\\) como o valor esperado e o desvio padrão, respectivamente, da urna (ou seja, de um único empréstimo), usando as fórmulas anteriores, temos: \\(\\mbox{E}[S]= n\\mu\\) e \\(\\mbox{SE}[S]= \\sqrt{n}\\sigma\\). Então, se definirmos \\(z\\)=qnorm(0.01), temos: \\[ - \\frac{n\\mu}{\\sqrt{n}\\sigma} = - \\frac{\\sqrt{n}\\mu}{\\sigma} = z \\] o que implica que se sairmos: \\[ n \\geq z^2 \\sigma^2/ \\mu^2 \\] temos uma probabilidade garantida inferior a 0,01. A implicação é que, desde que \\(\\mu\\) seja positivo, podemos encontrar um \\(n\\) isso minimiza a probabilidade de uma perda. Esta é uma versão da lei dos grandes números: quando \\(n\\) é grande, nossos ganhos médios em empréstimos convergem para o lucro esperado \\(\\mu\\). Com \\(x\\) fixo, agora podemos perguntar o que \\(n\\) precisamos que a probabilidade seja 0,01? No nosso exemplo, se distribuirmos: z &lt;- qnorm(0.01) n &lt;- ceiling((z^2*(x-l)^2*p*(1-p))/(l*p + x*(1-p))^2) n #&gt; [1] 22163 empréstimos, a probabilidade de perda é de aproximadamente 0,01 e esperamos ganhar um total de: n*(loss_per_foreclosure*p + x * (1-p)) #&gt; [1] 14184320 dólares! Podemos confirmar isso com uma simulação de Monte Carlo: p &lt;- 0.04 x &lt;- 0.05*180000 profit &lt;- replicate(B, { draws &lt;- sample( c(x, loss_per_foreclosure), n, prob=c(1-p, p), replace = TRUE) sum(draws) }) mean(profit) #&gt; [1] 14177753 Portanto, essa parece ser uma decisão óbvia. Como resultado, seu funcionário decide deixar o banco e abrir sua própria empresa de hipotecas subprime. Alguns meses depois, o banco de seu ex-funcionário declara falência. Um livro é escrito e, eventualmente, um filme é feito contando o erro cometido por seu funcionário e muitos outros. O que aconteceu? O esquema de seu ex-funcionário foi baseado principalmente nesta fórmula matemática: \\[ \\mbox{SE}[(X_1+X_2+\\dots+X_n)/ n] = \\sigma/ \\sqrt{n} \\] Ao fazer \\(n\\) grandes, eles minimizam o erro padrão do seu benefício de empréstimo. No entanto, para que esta regra seja seguida, o \\(X\\) s devem ser eventos independentes: a falha de uma pessoa deve ser independente da falha de outras. Observe que, no caso de calcular a média do mesmo repetidamente, um exemplo extremo de eventos que não são independentes, obtemos um erro padrão que é \\(\\sqrt{n}\\) vezes maior: \\[ \\mbox{SE}[(X_1+X_1+\\dots+X_1)/ n] = \\mbox{SE}[n X_1/ n] = \\sigma &gt; \\sigma/ \\sqrt{n} \\] Para criar uma simulação mais realista do que a simulação original executada por seu ex-funcionário, suponha que haja um evento global que afete todas as pessoas com hipotecas subprime e mude sua probabilidade. Assumiremos que, com uma probabilidade de 50 a 50, todas as probabilidades aumentam ou diminuem levemente para algo entre 0,03 e 0,05. Mas isso acontece com todos de uma vez, não apenas com uma pessoa. Esses eventos não são mais independentes. p &lt;- 0.04 x &lt;- 0.05*180000 profit &lt;- replicate(B, { new_p &lt;- 0.04 + sample(seq(-0.01, 0.01, length = 100), 1) draws &lt;- sample( c(x, loss_per_foreclosure), n, prob=c(1-new_p, new_p), replace = TRUE) sum(draws) }) Observe que o benefício esperado ainda é ótimo: mean(profit) #&gt; [1] 14137500 No entanto, a probabilidade de o banco ter lucros negativos dispara: mean(profit&lt;0) #&gt; [1] 0.348 Ainda mais preocupante é que a probabilidade de perder mais de US $ 10 milhões é: mean(profit &lt; -10000000) #&gt; [1] 0.24 Para entender como isso acontece, veja a distribuição: data.frame(profit_in_millions=profit/10^6) %&gt;% ggplot(aes(profit_in_millions)) + geom_histogram(color=&quot;black&quot;, binwidth = 5) A teoria quebra completamente e a variável aleatória tem muito mais variabilidade do que o esperado. O colapso financeiro de 2007 deveu-se, entre outras coisas, a “especialistas” financeiros que assumiram a independência quando não era esse o caso. 14.11 Exercícios 1. Crie uma variável aleatória \\(S\\) com os ganhos do seu banco, se você fizer 10.000 empréstimos, a taxa padrão é 0,3 e você perde $ 200.000 em cada execução duma hipoteca. Dica: Use o código que mostramos na seção anterior, mas altere os parâmetros. 2. Execute uma simulação de Monte Carlo com 10.000 resultados para \\(S\\). Faça um histograma dos resultados. 3. Qual é o valor esperado de \\(S\\)? 4. Qual é o erro padrão de \\(S\\)? 5. Suponha que façamos empréstimos de $ 180.000. Qual deve ser a taxa de juros para o nosso valor esperado ser 0? 6. (Mais difícil) Qual deve ser a taxa de juros para que a probabilidade de perda de dinheiro seja de 1 em 20? Em notação matemática, qual deve ser a taxa de juros para \\(\\mbox{Pr}(S&lt;0) = 0.05\\)? 7. Se o banco deseja minimizar as chances de perder dinheiro, qual das seguintes opções não faz com que as taxas de juros subam? para. Um grupo menor de empréstimos. b. Uma maior probabilidade de inadimplência. c. Uma menor probabilidade necessária de perda de dinheiro. d. O número de simulações de Monte Carlo. https://en.wikipedia.org/w/index.php?title=Financial_crisis_of_2007%E2%80%932008↩ https://en.wikipedia.org/w/index.php?title=Binomial_distribution↩ https://en.wikipedia.org/w/index.php?title=Poisson_distribution↩ "]
]
